<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title> Safety Signal Detection and Evaluation</title>

<script src="site_libs/header-attrs-2.29/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<link href="site_libs/font-awesome-6.5.2/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Zehui Bai</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
    Home
  </a>
</li>
<li>
  <a href="02-Clinical_Experience.html">
    <span class="fa fa-user"></span>
     
    Clinical Experience
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-sliders"></span>
     
    Sample Size
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="03-SSC-Everything-to-Know-About-Sample-Size-Determination.html">Everything to Know About Sample Size Determination</a>
    </li>
    <li>
      <a href="03-SSC-Choosing-the-Effect-Size-for-Sample-Size-Calculations.html">Choosing the Effect Size</a>
    </li>
    <li>
      <a href="03-SSC-Biosimilar-Trials.html">Statistical Considerations for the Design and Analysis of Biosimilar Trials</a>
    </li>
    <li>
      <a href="03-SSC-Sample-Size-and-Power-for-Non-Parametric-Analysis.html">Sample Size and Power for Non-Parametric Analysis</a>
    </li>
    <li>
      <a href="03-SSC-Power-for-Complex-Hypotheses.html">Power for Complex Hypotheses</a>
    </li>
    <li>
      <a href="03-SSC-Alternatives-to-Power.html">Bayesian methods - Alternatives to Power</a>
    </li>
    <li>
      <a href="03-SSC-Sample-Size-for-Pilot-Studies.html">Sample Size for Pilot Studies</a>
    </li>
    <li>
      <a href="03-SSC-Case-Continuous-Endpoint.html">Sample Size Determination for Continuous Endpoint</a>
    </li>
    <li>
      <a href="03-SSC-Case-Categorical-Endpoint.html">Sample Size Determination for Categorical Endpoint</a>
    </li>
    <li>
      <a href="03-SSC-Sample-Size-Determination-for-Counts-and-Rates.html">Sample Size Determination for Counts and Rates</a>
    </li>
    <li>
      <a href="03-SSC-Case-Survival-Endpoint.html">Sample Size Determination for Survival Endpoint</a>
    </li>
    <li>
      <a href="03-SSC-Case-Repeated-Measures.html">Sample Size Determination for Repeated Measures</a>
    </li>
    <li>
      <a href="03-SSC-IA-Sequential-Design.html">Statistical Considerations for Group Sequential Design</a>
    </li>
    <li>
      <a href="03-SSC-IA-Adaptive-Design.html">Statistical Considerations for Adaptive Design</a>
    </li>
    <li>
      <a href="03-SSC-Multiple-Test.html">Sample Size for Multiple Test</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-pencil-square-o"></span>
     
    Study Design
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="04-Design-Estimands-Framework.html">Estimands Framework</a>
    </li>
    <li>
      <a href="04-Design-Estimands-Practice.html">Estimands and Sensitivity Analyses</a>
    </li>
    <li>
      <a href="04-Design-Phase-I-Trials---Design-Considerations.html">Phase I Trials - Design Considerations</a>
    </li>
    <li>
      <a href="04-Design-Phase-II-Trials---Design-Considerations.html">Phase II Trials - Design Considerations</a>
    </li>
    <li>
      <a href="04-Design-Phase-III-Trials---Design-Considerations.html">Phase III Trials - Design Considerations</a>
    </li>
    <li>
      <a href="04-Design-Phase-IV-Trials---Design-Considerations.html">Phase IV Trials - Design Considerations for Post Marketing Surveillance</a>
    </li>
    <li>
      <a href="04-Design-Complex-Sequential-Trials.html">Complex Sequential Analysis Trials</a>
    </li>
    <li>
      <a href="04-Design-Adaptive-Clinical-Trials.html">Adaptive Clinical Trials</a>
    </li>
    <li>
      <a href="04-Design-Bayesian-Clinical-Trials.html">Bayesian Clinical Trials</a>
    </li>
    <li>
      <a href="04-Design-Dose-Escalation-and-Stratification-Designs.html">Dose Escalation and Stratification Designs in Early Oncology Development</a>
    </li>
    <li>
      <a href="04-Design-Single-Arm-Clinical-Trials.html">Single Arm Clinical Trials</a>
    </li>
    <li>
      <a href="04-Design-Diagnostic-Study-Design-and-Evaluation.html">Diagnostic Study-Design and Evaluation</a>
    </li>
    <li>
      <a href="04-Design-Diagnostic-Study-MRMC.html">Diagnostic Study-Multireader Multicase (MRMC)</a>
    </li>
    <li>
      <a href="04-Design-Hierarchical-composite-endpoints.html">Hierarchical Composite Endpoints</a>
    </li>
    <li>
      <a href="04-Design-Externally-Controlled-Trials.html">Considerations for the Design and Conduct of Externally Controlled Trials</a>
    </li>
    <li>
      <a href="04-Design-Noninferiority-Trials.html">Noninferiority Trials</a>
    </li>
    <li>
      <a href="04-Design-Bioequivalence-and-Biosimilar-Trials.html">Bioequivalence and Biosimilar Trials</a>
    </li>
    <li>
      <a href="04-Design-Exploring-Survival-Analysis-Designs-for-Clinical-Trials.html">Exploring Survival Analysis Designs for Clinical Trials</a>
    </li>
    <li>
      <a href="04-Design-Projecting-How-Long-Your-Trial-Will-Take.html">Projecting How Long Your Trial Will Take</a>
    </li>
    <li>
      <a href="04-Design-Regulatory-Submission.html">Regulatory Submission from Stats Perspective</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-line-chart"></span>
     
    Data Visualization
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="05-Plot-Adverse-Event.html">Adverse Event Visualization</a>
    </li>
    <li>
      <a href="05-Plot-Laboratory-Data.html">Laboratory Data Visualization</a>
    </li>
    <li>
      <a href="05-Plot-Time-To-Event.html">Time to Event Data Visualization</a>
    </li>
    <li>
      <a href="05-Plot-PRO-Data.html">Patient Reported Outcome Data Visualization</a>
    </li>
    <li>
      <a href="05-Plot-SSC-and-Power-Calculation.html">Sample Size and Power Calculations Visualization</a>
    </li>
    <li>
      <a href="05-Plot-Correlation.html">Correlation Visualization</a>
    </li>
    <li>
      <a href="05-Plot-Model-Table.html">Clinical Data and Model visualization</a>
    </li>
    <li>
      <a href="05-Plot-ScatterPlot.html">Scatter and Line Plot</a>
    </li>
    <li>
      <a href="05-Plot-BarPlot.html">Bar Chart</a>
    </li>
    <li>
      <a href="05-Plot-PieChart.html">Pie Chart</a>
    </li>
    <li>
      <a href="05-Plot-BoxPlot.html">Box Plot</a>
    </li>
    <li>
      <a href="05-Plot-Histogram.html">Histogram</a>
    </li>
    <li>
      <a href="05-Plot-Forest-Plot.html">Forest Plot</a>
    </li>
    <li>
      <a href="05-Plot-Flow-Chart.html">Flow Chart</a>
    </li>
    <li>
      <a href="05-Plot-Some-Interesting.html">Some Interesting Plots</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-calculator"></span>
     
    Statistical Analysis
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="06-Analysis-Linear-Regression.html">Linear Regression</a>
    </li>
    <li>
      <a href="06-Analysis-Logistic-Regression.html">Logistic Regression</a>
    </li>
    <li>
      <a href="06-Analysis-Mixed-Model.html">Mixed Model</a>
    </li>
    <li>
      <a href="06-Analysis-MMRM.html">Mixed Model Repeated Measures</a>
    </li>
    <li>
      <a href="06-Analysis-GEE.html">Generalized Estimating Equation</a>
    </li>
    <li>
      <a href="06-Analysis-ANOVA.html">Analysis of Variance</a>
    </li>
    <li>
      <a href="06-Analysis-Survival-Analysis.html">Survival Analysis</a>
    </li>
    <li>
      <a href="06-Analysis-Survival-Competing-Risk.html">Survival Analysis - Competing Risk</a>
    </li>
    <li>
      <a href="06-Analysis-Missing-Data.html">Missing Data Analysis</a>
    </li>
    <li>
      <a href="06-Analysis-MI-Implementation.html">Multiple Imputation Analysis</a>
    </li>
    <li>
      <a href="06-Analysis-SSD.html">Safety Signal Detection and Evaluation</a>
    </li>
    <li>
      <a href="06-Analysis-Meta-Analysis.html">Meta Analysis</a>
    </li>
    <li>
      <a href="06-Analysis-PK-and-PD.html">PK and PD Analysis</a>
    </li>
    <li>
      <a href="06-Analysis-Time-Series-Analysis.html">Time Series Analysis</a>
    </li>
    <li>
      <a href="06-Analysis-SEM.html">Structural Equation Modeling</a>
    </li>
    <li>
      <a href="06-Analysis-Factor-Analysis.html">Factor Analysis</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-bar-chart"></span>
     
    Machine Learning
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="07-ML-Bayesian-Theory.html">Bayesian Theory</a>
    </li>
    <li>
      <a href="07-ML-Bayesian-Analysis.html">Bayesian Analysis</a>
    </li>
    <li>
      <a href="07-ML-Regularization-Penalized-Regression.html">Regularization Penalized Regression</a>
    </li>
    <li>
      <a href="07-ML-Loss-Regression.html">Loss Functions in Machine Learning</a>
    </li>
    <li>
      <a href="07-ML-PCA.html">Principal Component Analysis</a>
    </li>
    <li>
      <a href="07-ML-KNN.html">K-Nearest Neighbors</a>
    </li>
    <li>
      <a href="07-ML-SVM.html">Support Vector Machine</a>
    </li>
    <li>
      <a href="07-ML-Tree-Models.html">Tree Models</a>
    </li>
    <li>
      <a href="07-ML-LDA.html">Linear Discriminant Analysis</a>
    </li>
    <li>
      <a href="07-ML-Cluster-Analysis.html">Cluster Analysis</a>
    </li>
    <li>
      <a href="07-ML-Neural-Networks.html">Neural Network</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="08-CV.html">
    <span class="fa fa-file-pdf-o"></span>
     
    CV
  </a>
</li>
<li>
  <a href="mailto:zehuibai@outlook.com">
    <span class="fa fa-envelope-o"></span>
     
    Contact me
  </a>
</li>
<li>
  <a href="https://github.com/Zehuibai">
    <span class="fa fa-github"></span>
     
    GitHub
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore"><p><img src="logo.png"
style="width:3in" /><br />
Safety Signal Detection and Evaluation</p></h1>

</div>


<div id="safety-signal-detection" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Safety Signal
Detection</h1>
<div id="introduction" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Introduction</h2>
<p>Safety Signal Detection (SSD) is a critical component of
pharmacovigilance and drug safety monitoring. Its primary aim is to
detect, assess, and manage potential safety risks associated with
pharmaceutical products, ensuring patient safety and supporting public
health.</p>
<p><strong>What is Safety Signal Detection?</strong></p>
<p>SSD involves the routine evaluation of safety signals through
periodic reviews of aggregated data from various sources, including
clinical trials, post-marketing surveillance, and real-world data. A
safety signal refers to evidence of a potentially new adverse event or a
new aspect of a known adverse event that is caused by a medicinal
product and that warrants further investigation.</p>
<p><strong>SSD Process</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Strategy and Scope Setting:</strong>
<ul>
<li>At the early phases of drug development, the clinical team, along
with the benefit-risk lead (subject to confirmation), discusses and
determines the SSD strategy. This includes deciding the frequency of
reviews and the scope of included studies.</li>
<li>The strategy is tailored to ensure that all potential safety issues
are promptly identified and addressed. This involves periodic
assessments that could be aligned with other regulatory requirements
like Periodic Safety Update Reports (PSURs) or Development Safety Update
Reports (DSURs).</li>
</ul></li>
<li><strong>Data Collection and Aggregation:</strong>
<ul>
<li>Relevant data from clinical trials and other sources are collected
and aggregated.</li>
</ul></li>
<li><strong>Analysis and Review:</strong>
<ul>
<li>The data undergoes statistical analysis to identify trends or
patterns that could indicate potential safety issues.</li>
</ul></li>
<li><strong>Signal Evaluation:</strong>
<ul>
<li>Identified signals are then evaluated to confirm their validity and
potential impact on patient safety. This evaluation includes a detailed
investigation into whether the signal represents a true risk or is due
to other factors like underlying diseases or concurrent
medications.</li>
</ul></li>
<li><strong>Risk Management and Mitigation:</strong>
<ul>
<li>If a safety signal is confirmed, risk management strategies are
developed and implemented. These may include changes to the product
labeling, restrictions on use, or in some cases, drug withdrawal.</li>
</ul></li>
<li><strong>Documentation and Reporting:</strong>
<ul>
<li>All findings and actions are thoroughly documented and reported to
regulatory authorities as required.</li>
</ul></li>
</ol>
</div>
<div id="biostatistics-and-data-science-bds-role" class="section level2"
number="1.2">
<h2><span class="header-section-number">1.2</span> Biostatistics and
Data Science (BDS) Role</h2>
<ul>
<li>BDS is primarily responsible for authoring the Program Statistical
Analysis Plan (PSAP).</li>
<li>BDS provides clinical study data summaries, typically in the form of
Tables, Figures, and Listings (TFLs), which are essential for supporting
the detection of safety signals.</li>
<li>Additionally, BDS ensures that the clinical teams have access to
advanced visualization tools for data exploration. These tools help in
the intuitive understanding of complex datasets and trends, facilitating
a more robust safety signal detection process.</li>
</ul>
</div>
</div>
<div id="planning-for-ind-safety-reporting" class="section level1"
number="2">
<h1><span class="header-section-number">2</span> Planning for IND Safety
Reporting</h1>
<div id="safety-reports-ser-ssar" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Safety Reports (SER
&amp; SSAR)</h2>
<p>Safety Evaluation Report: A flexible approach for reviewing safety
topics which are not triggered from the signal detection process. An SER
can be upgraded to an SSAR if the safety topic becomes a valid signal
during the process. This report is led by the Safety Writer</p>
<p>Safety Signal Assessment Report: Further evaluated signal considering
all available evidence, to determine whether there are ne wrisk
causually assoiciated with active substance or medicinal product, or if
known risk have changed. his report is led by the Safety Writer</p>
</div>
<div id="expected-and-anticipated-serious-adverse-events-saes"
class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Expected and
Anticipated Serious Adverse Events (SAEs)</h2>
<ul>
<li><p>Expected SAEs refer to serious adverse events that can reasonably
be predicted based on the known pharmacological properties, previous
clinical trial outcomes, or typical characteristics of the drug class.
These events are usually documented in the drug’s label or other
professional literature. Therefore, when these events occur in new
clinical trials, they are considered “expected” because their potential
has already been identified and acknowledged based on existing data.
Expected SAEs are important for risk management and informed consent
processes, as they help set realistic expectations for both clinicians
and participants regarding the known risks associated with a
drug.</p></li>
<li><p>Anticipated SAEs, while similar to expected SAEs, generally refer
to events whose occurrence is foreseen based on less definitive evidence
than that for expected SAEs. These could be based on preliminary data,
such as early clinical trials, animal studies, or even theoretical
considerations linked to the drug’s mechanism of action. Anticipated
SAEs are not as firmly established as expected SAEs but are considered
likely enough that they should be monitored for in the context of
ongoing clinical research. They may or may not be included in the
product label but are anticipated from a safety surveillance
perspective.</p></li>
</ul>
<p><strong>Key Differences</strong></p>
<ul>
<li><strong>Basis of Prediction</strong>: Expected SAEs are based on
more solid, often clinically verified evidence, while anticipated SAEs
may rely on preliminary or less conclusive evidence.</li>
<li><strong>Documentation</strong>: Expected SAEs are typically
documented in official product materials like labeling, whereas
anticipated SAEs might not be, depending on their level of evidence and
regulatory requirements.</li>
<li><strong>Regulatory Impact</strong>: Expected SAEs have a direct
impact on the drug’s labeling and are crucial for regulatory compliance
and patient safety communications. Anticipated SAEs, while also
important, might influence ongoing monitoring strategies and potential
label updates as more data become available.</li>
</ul>
</div>
<div id="aggregate-analysis-planning" class="section level2"
number="2.3">
<h2><span class="header-section-number">2.3</span> Aggregate Analysis
Planning</h2>
<p>Planning for FDA IND (Investigational New Drug) safety reporting is a
critical component of clinical trial management, ensuring that serious
adverse events (SAEs) are properly identified, analyzed, and reported.
This process is especially vital during the transition from Phase 1 to
Phase 2 of clinical trials, where a clear understanding of the safety
profile of the investigational medicinal product (IMP) is essential for
further development.</p>
<p><strong>Aggregate Analysis Planning:</strong> Early in the product
development lifecycle, planning for the aggregate analysis of aSAEs and
expected SARs should commence. This is crucial as it sets the foundation
for ongoing safety monitoring and regulatory compliance. The planning
should start as the studies transition from Phase 1 to Phase 2, which is
typically when the target population for these studies has been clearly
identified and the safety data from initial human exposure is
available.</p>
<p><strong>Possible Approaches for Aggregate Analysis:</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Analysis of All Serious Adverse Events (SAEs) by Treatment
Group:</strong>
<ul>
<li>This approach involves periodic reviews of all SAEs sorted by
treatment groups within the clinical trial. The primary focus is to
identify whether aSAEs or expected SARs are occurring at a significantly
higher incidence in the group receiving the IMP compared to a concurrent
control (either placebo or an active comparator) or a historical control
group. This method helps in understanding the direct impact of the drug
under study relative to other treatments or known data.</li>
</ul></li>
<li><strong>Unblinding Trigger Approach:</strong>
<ul>
<li>In this method, a blinded quantitative analysis is conducted. The
Unblinding Trigger Approach focuses on the analysis of anticipated
serious adverse events (aSAEs) and expected serious adverse reactions
(SARs). This approach uses a pre-specified threshold to determine
whether the overall blinded incidence rate of these events is higher
than either the estimated background incidence rate in the target
population or the incidence of expected SARs as listed in the Reference
Safety Information (RSI).</li>
<li>If these thresholds are exceeded, the SSDT may conduct a further
blinded review and, if necessary, escalate the safety issue to the
Benefit Risk Team (BRT) for consideration. If the BRT deems it
necessary, an unblinded review may be initiated.</li>
</ul></li>
</ol>
<p><strong>Selection and Initiation of Aggregate Analysis:</strong></p>
<p>The Safety Surveillance and Data Team (SSDT) is responsible for
selecting the appropriate methodology for aggregate analysis. The choice
between analyzing all events by treatment group or applying the
unblinding trigger approach depends on several factors including the
study population, the characteristics of the product, and the size and
duration of the clinical studies involved. Aggregate analysis is
typically initiated during Phase 2 of clinical studies, assuming there
are enough participants and observed SAEs to conduct a meaningful
analysis.</p>
<p><strong>Documentation in Safety Surveillance Plan (SSP):</strong> The
specific methodologies chosen for the aggregate analysis are detailed in
the product-specific FDA IND Safety Surveillance Plan (SSP), which is a
dedicated section of the Safety Signal Detection Strategy. The SSP is
crafted and reviewed by the SSDT and should include:</p>
<ul>
<li>The chosen methodology (Analysis of All Events by Treatment Group
and/or Unblinding Trigger Approach).</li>
<li>Criteria for further assessment if using the Unblinding Trigger
Approach, including thresholds that might trigger an IND safety
report.</li>
<li>A list of aSAEs with MedDRA search criteria used for identifying
these events in the clinical database.</li>
<li>Estimations of background incidence rates for aSAEs, if
possible.</li>
<li>Protocols for when unblinding is necessary to evaluate potential
causal relationships with the IMP.</li>
</ul>
<p><strong>Defining anticipated serious adverse events
(aSAEs)</strong></p>
<ul>
<li>The process begins when the Development Physician initiates the
definition of aSAEs as soon as the target patient population(s) have
been identified and the first study concepts in these populations have
been approved. This early initiation ensures that the safety monitoring
is tailored to the specific needs of the population from the
outset.</li>
<li>The Development Physician is responsible for defining the
characteristics of the target population and checking for any existing
lists of aSAEs relevant to this group.</li>
<li>They lead efforts to update or create new lists of aSAEs, involving
contributions from other functions such as Medical Affairs to identify
relevant clinical studies for evaluating aSAEs and background incidence
rates.</li>
<li>The RWE Representative uses Real World Data, starting with a focused
literature review, and if necessary, conducting studies using
fit-for-use Real World Databases to identify potential aSAEs.</li>
<li>If the initial literature review does not yield sufficient data,
further studies are conducted to estimate background incidence rates for
each aSAE. This step is critical for understanding the typical
occurrence rates of these events outside of clinical trials.</li>
<li>Since Real World Evidence might use non-MedDRA terms, the list of
aSAEs identified needs to be reviewed by the Medical Coding Oversight
Lead to define these events in standardized MedDRA terms.</li>
<li>The Development Physician consolidates a preliminary list of aSAEs
using MedDRA search terms based on the RWE and statistical
analyses.</li>
<li>The Statistical Representative then estimates the background
incidence rates for each aSAE using historical clinical data.</li>
</ul>
</div>
<div id="xsur---standard-required-safety-reporting-documents"
class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> XSUR - Standard
Required Safety Reporting Documents</h2>
<p><strong>DSUR/PSUR/IB/J-NUPR/RMP</strong></p>
<p>These documents are <strong>regulatory safety reporting
requirements</strong> that help ensure the ongoing evaluation of the
safety profile of investigational and marketed drugs.</p>
<div id="dsur-development-safety-update-report"
class="section level3 unnumbered">
<h3 class="unnumbered">1. DSUR (Development Safety Update Report)</h3>
<ul>
<li><p><strong>Purpose</strong>:<br />
The DSUR is a <strong>yearly regulatory document</strong> that provides
a comprehensive safety overview of an investigational product during its
development phase (pre-marketing).</p></li>
<li><p><strong>General Content</strong>:</p>
<ul>
<li>Summary of cumulative safety data across all clinical trials</li>
<li>Adverse event (AE) overview</li>
<li>Exposure data</li>
<li>Ongoing and completed study updates</li>
<li>Benefit-risk considerations</li>
</ul></li>
<li><p><strong>Timing</strong>:<br />
Annually, typically synchronized with the <strong>Investigational
Brochure (IB)</strong> update.</p></li>
<li><p><strong>BST Contribution</strong>:</p>
<ul>
<li>TFLs summarizing:
<ul>
<li><strong>Cumulative subject exposure</strong>, often broken down by
population (e.g., healthy vs. patient, age groups)</li>
<li><strong>Study-specific exposure summaries</strong></li>
<li><strong>Participant withdrawals due to adverse events</strong></li>
<li><strong>SAE/AE listings</strong> if required</li>
</ul></li>
</ul></li>
</ul>
<hr />
</div>
<div id="psur-periodic-safety-update-report"
class="section level3 unnumbered">
<h3 class="unnumbered">2. PSUR (Periodic Safety Update Report)</h3>
<ul>
<li><p><strong>Purpose</strong>:<br />
The PSUR is used to monitor the safety of <strong>marketed (authorized)
products</strong> over time, usually post-approval.</p></li>
<li><p><strong>General Content</strong>:</p>
<ul>
<li>Cumulative safety information since the product was authorized</li>
<li>Global safety data</li>
<li>Benefit-risk evaluation</li>
<li>Regulatory actions taken</li>
<li>Literature and spontaneous report summaries</li>
</ul></li>
<li><p><strong>Timing</strong>:<br />
Varies depending on the product’s time on the market and specific
regulatory agreements (e.g., every 6 months, 1 year, or 3
years).</p></li>
<li><p><strong>BST Contribution</strong>:</p>
<ul>
<li>TFLs supporting:
<ul>
<li><strong>Cumulative subject exposure</strong>, including dose and
duration</li>
<li><strong>Trend summaries</strong> (e.g., AE over time)</li>
<li><strong>Stratified summaries by indication or region</strong>, if
required</li>
</ul></li>
</ul></li>
</ul>
<hr />
</div>
<div id="ib-investigators-brochure" class="section level3 unnumbered">
<h3 class="unnumbered">3. IB (Investigator’s Brochure)</h3>
<ul>
<li><p><strong>Purpose</strong>:<br />
The IB is a reference document for investigators conducting clinical
trials and contains comprehensive data on the <strong>investigational
product</strong>, including safety and efficacy findings.</p></li>
<li><p><strong>General Content</strong>:</p>
<ul>
<li>Clinical and non-clinical safety data</li>
<li>Pharmacokinetics and pharmacodynamics</li>
<li>Investigator guidance</li>
<li>Benefit-risk summary</li>
</ul></li>
<li><p><strong>Timing</strong>:<br />
Updated <strong>annually</strong>, often in parallel with DSUR
preparation.</p></li>
<li><p><strong>BST Contribution</strong>:</p>
<ul>
<li>TFLs used in the <strong>Adverse Drug Reaction</strong> (ADR)
section:
<ul>
<li><strong>Listings or summary tables</strong> of AEs considered
related</li>
<li><strong>Cumulative AE rates</strong></li>
<li><strong>Narrative support</strong> via structured data</li>
</ul></li>
</ul></li>
</ul>
<hr />
</div>
<div id="j-nupr-japanese-non-serious-unlisted-periodic-report"
class="section level3 unnumbered">
<h3 class="unnumbered">4. J-NUPR (Japanese Non-serious Unlisted Periodic
Report)</h3>
<ul>
<li><p><strong>Purpose</strong>:<br />
A <strong>Japan-specific post-marketing requirement</strong> for
periodic reporting of <strong>non-serious and unlisted adverse
events</strong> observed during post-marketing surveillance.</p></li>
<li><p><strong>General Content</strong>:</p>
<ul>
<li>Line listings of applicable non-serious, unlisted AEs</li>
<li>Summary counts stratified by term, SOC, region, etc.</li>
</ul></li>
<li><p><strong>Timing</strong>:<br />
Regular intervals defined by the Japanese Ministry of Health, Labour and
Welfare (MHLW)</p></li>
<li><p><strong>BTS Contribution</strong>:</p>
<ul>
<li>Delivery of:
<ul>
<li><strong>Cumulative frequency tables</strong></li>
<li><strong>Listings of unlisted events</strong></li>
<li><strong>Patient-level datasets</strong>, filtered by local
criteria</li>
</ul></li>
</ul></li>
</ul>
<hr />
</div>
<div id="rmp-risk-management-plan" class="section level3 unnumbered">
<h3 class="unnumbered">5. RMP (Risk Management Plan)</h3>
<ul>
<li><p><strong>Purpose</strong>:<br />
The RMP outlines how the <strong>risks of a medicinal product</strong>
will be <strong>identified, characterized, prevented, or
minimized</strong> once the product is on the market.</p></li>
<li><p><strong>General Content</strong>:</p>
<ul>
<li>Product safety specification</li>
<li>Pharmacovigilance plans</li>
<li>Risk minimization measures (e.g., targeted education,
monitoring)</li>
</ul></li>
<li><p><strong>Timing</strong>:</p>
<ul>
<li>At the time of <strong>marketing authorization application
(MAA)</strong></li>
<li><strong>Updated as needed</strong> post-authorization (e.g., after
new safety signals)</li>
</ul></li>
<li><p><strong>BST Contribution</strong>:</p>
<ul>
<li>Statistical review and validation of:
<ul>
<li><strong>Exposure estimates</strong></li>
<li><strong>Incidence rates of identified and potential
risks</strong></li>
<li><strong>Monitoring metrics (e.g., patient compliance, reporting
rates)</strong></li>
</ul></li>
</ul></li>
<li><p><strong>Reference</strong>:<br />
EMA guidance:</p>
<ul>
<li><em>GVP Module V – Risk Management Systems (Rev 2)</em><br />
</li>
<li><em>RMP Q&amp;A on EMA website</em></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="bssd" class="section level1" number="3">
<h1><span class="header-section-number">3</span> BSSD</h1>
<div id="ongoing-clinical-trials-utilizing-a-bayesian-framework"
class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Ongoing Clinical
Trials, Utilizing a Bayesian Framework</h2>
<p>Safety signals refer to <strong>potential indications of an adverse
effect</strong> caused by a drug or treatment. Detecting these early
during a clinical trial can:</p>
<ul>
<li>Prevent harm to participants</li>
<li>Inform decisions about trial continuation</li>
<li>Support regulatory compliance</li>
</ul>
<p>The purpose of this safety signal detection approach is to improve
how potential risks and adverse effects are identified during ongoing
clinical trials, especially when those trials are still blinded.
Detecting safety signals early is crucial to protect participants and to
make informed decisions about whether a drug or treatment should
continue to be studied. Traditional methods often fall short in this
area because they may not be flexible enough to handle evolving data or
may not work well when treatment assignments are still unknown. That’s
why a new, more adaptive system is needed. Using a Bayesian framework,
which allows for continuous learning from new data and offers a more
intuitive way to assess risk as information accumulates. This approach
is particularly useful in the clinical development phase, where
decisions must be made based on incomplete or uncertain data. By
re-engineering the safety signal detection process to include these
modern statistical techniques, the goal is to create a more reliable and
responsive system that can better safeguard patient health and support
smarter, faster decision-making in drug development.</p>
<p><strong>Bayesian methods</strong> as a core solution:</p>
<ul>
<li><p>Bayesian statistics allow for <strong>updating beliefs with new
evidence</strong>, which is ideal for ongoing trials where new safety
data constantly emerge.</p></li>
<li><p>This framework supports:</p>
<ul>
<li><strong>Assessment of blinded data</strong></li>
<li><strong>Evaluation of adverse events</strong></li>
<li><strong>Estimation of probabilities of safety risks</strong></li>
</ul></li>
</ul>
<p>Bayesian reasoning is often more <strong>flexible and
intuitive</strong> for interpreting risk over time compared to
traditional frequentist approaches.</p>
<hr />
<p><strong>Why a change in safety signal detection (SSD) is needed by
highlighting key drivers across five major areas: regulatory
requirements, scientific definitions, industry trends, patient safety,
and corporate responsibility.</strong></p>
<p>From a <strong>regulatory perspective</strong>, recent FDA guidance
(e.g., CFR 312.32 and other 2012–2015 documents) increasingly emphasizes
the need for sponsors to systematically assess safety data across
ongoing studies. Regulations call for aggregate analyses to identify
adverse event patterns and recommend setting up dedicated Safety
Assessment Committees and Surveillance Plans. Moreover, guidance
encourages quantitative approaches to determine the likelihood of causal
associations between treatments and adverse events.</p>
<p>In terms of <strong>signal detection science</strong>, definitions
from CIOMS (Council for International Organizations of Medical Sciences)
and Good Pharmacovigilance Practices describe a signal as a hypothesis
of a causal link between a treatment and observed events. Detection is
about identifying statistically unusual patterns—those exceeding a
specified threshold—that justify further verification. This highlights
the need for methods that can handle complex data and allow continuous
evaluation, especially in blinded settings.</p>
<ul>
<li><p>CIOMS (Council for International Organizations of Medical
Sciences) Working Group VI recommends a <strong>holistic, program-level
review</strong> of safety data across studies.</p></li>
<li><p>This approach is intended to:</p>
<ul>
<li><strong>Identify safety signals earlier</strong>, ideally before
they become serious or widespread.</li>
<li><strong>Protect patients</strong> by responding quickly to potential
safety issues.</li>
<li><strong>Support medical judgment</strong> with a
<strong>quantitative framework</strong>—i.e., not just subjective
interpretations, but data-supported decisions.</li>
</ul></li>
<li><p><strong>US FDA integrated these ideas into official
regulations</strong></p></li>
<li><p><em>21 CFR Parts 312.32 and 320.31</em> (effective 2010): These
specify the rules for <em>IND (Investigational New Drug)</em> safety
reporting.</p></li>
<li><p><strong>Guidance Documents</strong></p>
<ul>
<li><em>2012 Guidance</em>: Clarifies how sponsors should implement
safety reporting in clinical studies.</li>
<li><em>2015 Draft Guidance</em>: Suggests how sponsors can <em>assess
safety data</em> more rigorously—especially for identifying <em>causal
relationships</em> between the drug and adverse events.</li>
</ul></li>
<li><p><strong>US FDA Final Rule – Key Expectations</strong>, The
<strong>Final Rule</strong> establishes that sponsors must take a
<strong>systematic approach to pharmacovigilance</strong>.</p></li>
<li><p>It emphasizes that <strong>IND safety reports</strong> should
only be submitted when there’s <strong>reasonable evidence of a causal
relationship</strong> between the drug and the adverse event.</p></li>
<li><p>Breaking the Blind: The FDA allows breaking the blind for serious
adverse events not related to clinical endpoints.</p></li>
<li><p>According to the Final Rule, two options exist:</p>
<ol style="list-style-type: decimal">
<li><strong>Routine Aggregate Unblinded Review by a Safety Assessment
Committee (SAC)</strong>
<ul>
<li>Conducted independently.</li>
<li><strong>Preferred</strong> method: Balances patient safety with
trial integrity.</li>
<li>Allows identification of signals <strong>without bias</strong>.</li>
</ul></li>
<li><strong>Conditional Unblinding</strong>
<ul>
<li>Sponsors remain blinded <strong>unless</strong> overall adverse
event rates exceed a <strong>predefined expected
threshold</strong>.</li>
<li>Only then do they unblind data to assess if the drug might be
responsible.</li>
</ul></li>
</ol></li>
</ul>
<p>The <strong>industry trend</strong> is also pushing for more rigorous
SSD, as literature (e.g., Yao et al., 2013) points to the limitations of
traditional monitoring. Bayesian approaches are being promoted for their
ability to offer objective, real-time analysis during trials. Early
detection improves patient protection and can reduce long-term
development costs. Industry best practices also advocate for early and
proactive planning of safety analyses through frameworks like Program
Safety Analysis Plans (PSAPs).</p>
<p><strong>Patient safety</strong> remains the central ethical and
practical driver of SSD improvement. Leading pharmaceutical companies
(e.g., Teva, Pfizer, Eli Lilly) publicly commit to placing patient
safety at the heart of their operations, underscoring the societal and
reputational importance of effective monitoring.</p>
<p>Lastly, <strong>corporate principles</strong> stress that accurate
safety profiles affect not just regulatory compliance and patient
safety, but also the financial valuation of a compound. Failure to
detect and report risks promptly can result in serious harm, legal
consequences, and a loss of public trust and investment.</p>
<hr />
<p><strong>How change should occur in the safety signal
detection</strong></p>
<p>Firstly, the use of <strong>advanced data visualization
tools</strong> is encouraged to enhance the interpretability and
interactivity of safety data. These tools, including forest plots,
threshold plots, time-to-event plots, and hazard plots, offer
interactive and drill-down features that allow for a more dynamic
exploration of potential safety issues. Such visualization supports
faster identification of concerning patterns and facilitates more
informed decision-making.</p>
<p>Secondly, there is a push for greater <strong>scientific and
statistical rigor</strong>, particularly in the analysis of both blinded
and unblinded safety data. This includes applying robust statistical
methodologies to datasets covering adverse events (AEs), laboratory
results, vital signs, and electrocardiograms (ECGs). The goal is to
bring consistency, objectivity, and depth to the SSD process.</p>
<p>Another key element is the <strong>development of global safety
databases</strong> organized by compound. These databases would
centralize safety data from multiple studies, making it easier to detect
patterns, perform aggregate analyses, and assess compound-specific risks
across different trials and populations.</p>
<p>Lastly, organizations are advised to <strong>develop Program Safety
Analysis Plans (PSAPs)</strong>. These formalized plans lay out a
comprehensive strategy for conducting safety analyses throughout a
drug’s development lifecycle. They ensure that safety monitoring is
systematic, pre-planned, and harmonized across studies.</p>
<hr />
<p><strong>What specifically needs to change in managing safety signal
detection</strong></p>
<p>First, companies should <strong>develop pooled, aggregate safety
databases</strong> much earlier in the clinical development
lifecycle—rather than waiting until submission. These databases should
have standardized structures and reporting templates to support not only
SSD but also broader safety reporting needs. By centralizing safety data
early, companies can enable faster and more effective signal detection
and trend analysis across studies.</p>
<p>Second, in the area of <strong>safety signal detection</strong>,
companies need to adopt or enhance <strong>statistical methods</strong>
capable of handling <strong>blinded clinical trial data</strong>,
ensuring valid analysis even when treatment assignments are unknown.
This should be coupled with the <strong>use of data visualization
tools</strong>—both static and interactive—to better identify patterns
and communicate findings. Additionally, optimizing the outputs produced
during SSD (e.g., reports, dashboards, alerts) ensures that results are
both informative and actionable.</p>
<p>Lastly, <strong>IND safety reporting</strong> practices must be
realigned to match FDA guidance and expectations. This includes
improving how serious adverse events (SAEs) are assessed and
reported—focusing on those with a demonstrated causal relationship to
the drug, as judged by a combination of sponsor, medical, and
statistical input. Consistency in the use of terms like “anticipated,”
“predicted,” or “expected adverse event” in Investigator Brochures (IBs)
is also important to avoid ambiguity.</p>
</div>
<div id="objectives-of-bssd-analyses" class="section level2"
number="3.2">
<h2><span class="header-section-number">3.2</span> Objectives of BSSD
Analyses</h2>
<p>Objective of SSD in blinded studies is to use predefined statistical
thresholds, reference rates, and modeling (like Bayesian or frequentist
methods) to detect early signs of safety concerns without breaking the
blind. The approach must balance avoiding unnecessary alarms with
catching true adverse trends early enough for meaningful
intervention.</p>
<p><strong>Core challenge</strong> in blinded SSD: distinguishing
between <strong>“noise” (random variation)</strong> and <strong>true
“signal” (evidence of risk)</strong> while the treatment assignment
remains unknown.</p>
<ul>
<li>A <strong>criterion</strong> (threshold) is set to decide whether an
“alarm” should be triggered, suggesting a potential safety issue.</li>
<li>If the true signal falls below this threshold, it may result in a
<strong>missed alarm</strong>—failing to detect a true risk.</li>
<li>Conversely, if random noise exceeds the threshold, a <strong>false
alarm</strong> may be triggered—suggesting a safety issue when none
exists.</li>
</ul>
<p>The objective is to define and refine this threshold in a way that
balances sensitivity (catching real issues) and specificity (avoiding
false alarms), especially under the constraints of blinded data.</p>
<p>E.g. <strong>practical statistical example</strong> to frame the
objectives of SSD in blinded trials:</p>
<ul>
<li>Historically, from a dataset of 500 subjects with 24 weeks of
follow-up, the <strong>adverse event (AE) incidence</strong> in the
placebo group for a particular event is known to be
<strong>2%</strong>.</li>
<li>In a <strong>new blinded study</strong>, 80 subjects (with a 3:1
treatment-to-placebo randomization ratio) have completed the same
follow-up period, and <strong>Y events</strong> have been observed.</li>
<li>The central question is: <strong>What number of events (Y)</strong>
would suggest that the observed rate is too high to be due to chance,
assuming no true difference exists between the groups?</li>
<li>This analysis seeks to determine whether the current study data
(while still blinded) deviates enough from expected patterns to indicate
a <strong>potential safety signal</strong>.</li>
</ul>
<div id="simple-frequentist-approach-binomial-model"
class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Simple Frequentist
Approach (Binomial Model)</h3>
<ol style="list-style-type: decimal">
<li><strong>Start with a baseline</strong> estimate of adverse event
rate (θ) from robust historical placebo data.</li>
<li>Adjust for <strong>time-at-risk</strong> and censoring using
person-time and hazard models.</li>
<li>Use the adjusted θ to compute expected counts in the ongoing
study.</li>
<li>Compare observed counts (Y) to this expectation via binomial or
Poisson models.</li>
<li><strong>Trigger a signal</strong> if observed Y exceeds what is
reasonably expected under the null hypothesis.</li>
</ol>
<p>Throughout, it emphasizes the <strong>need for careful
adjustment</strong> for follow-up time and differences between
populations to avoid <strong>false alarms or missed
signals</strong>.</p>
<ul>
<li><p><strong>Assumptions:</strong></p>
<ul>
<li>The event of interest is rare (e.g., AE incidence of 2%).</li>
<li>In a new blinded study, 80 subjects have completed follow-up.</li>
</ul></li>
<li><p>The <strong>expected number of AEs</strong> under the null
hypothesis (no increased risk) is:</p>
<p><span class="math display">\[
\mathbb{E}[Y] = n \cdot \theta = 80 \cdot 0.02 = 1.6
\]</span></p></li>
<li><p>A binomial distribution <span class="math inline">\(Y \sim
\text{Binomial}(n=80, \theta=0.02)\)</span> is used to calculate
probabilities of observing different values of Y.</p></li>
<li><p>If 5 or more AEs are observed, the cumulative probability <span
class="math inline">\(P(Y &lt; 5)\)</span> is 0.9776 → suggesting a
<strong>low likelihood under the null</strong> (possible
signal).</p></li>
</ul>
<p>However: underline the <strong>need for careful estimation of
θ</strong>, the true underlying event rate in the placebo group.</p>
<ul>
<li><strong>Where did θ (2%) come from?</strong> Was it from robust,
representative historical data?</li>
<li><strong>Are historical and current populations comparable?</strong>
Differences in demographics, care settings, and follow-up time can bias
results.</li>
<li><strong>Time-at-risk mismatch</strong>: If subjects in the
historical and current study differ in follow-up duration or censoring,
comparing incidence rates directly could be misleading.</li>
</ul>
<p>Therefore, the estimation of θ is approached <strong>indirectly via
hazard rate λ</strong>, using historical data:</p>
<ul>
<li><p>For each historical study:</p>
<p><span class="math display">\[
\text{IR}_j = \frac{r}{\sum_{i=1}^{r} t_i + (n - r)T}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(r\)</span>: number of events</li>
<li><span class="math inline">\(t_i\)</span>: time to event</li>
<li><span class="math inline">\(T\)</span>: censoring time for those
without events</li>
</ul></li>
<li><p>This yields an <strong>incidence rate per unit time (hazard
rate)</strong>, assuming an exponential model.</p></li>
<li><p>IR is treated as the <strong>Maximum Likelihood Estimate
(MLE)</strong> of λ.</p></li>
</ul>
<p>Next, the <strong>pooled hazard rate (λ_w)</strong> across multiple
historical studies is computed using a <strong>weighted
average</strong>:</p>
<p><span class="math display">\[
\lambda_w = \frac{\sum_{j=1}^k w_j \lambda_j}{\sum_{j=1}^k w_j}
\]</span></p>
<ul>
<li><span class="math inline">\(w_j\)</span>: total person-time in study
j</li>
<li><span class="math inline">\(λ_j\)</span>: incidence rate for study
j</li>
</ul>
<p>Then, derive the <strong>distribution of time-at-risk</strong> for
each patient in the blinded trial. This accounts for:</p>
<ul>
<li>Patients who completed the study</li>
<li>Those who withdrew prematurely</li>
<li>Those censored at database cut-off</li>
</ul>
<p>This step ensures <strong>alignment between exposure times</strong>
in the new and historical data.</p>
<p>Finally, the <strong>expected number of AEs</strong> in the ongoing
blinded study is calculated:</p>
<p><span class="math display">\[
\mathbb{E}[Y] = \sum_{i=1}^{n} \left(1 - e^{-\lambda_w t_i}\right)
\]</span></p>
<p><span class="math display">\[
θ = \frac{\mathbb{E}[Y]}{n}
\]</span></p>
<ul>
<li>This gives a <strong>time-adjusted estimate of the incidence
proportion</strong> <span class="math inline">\(θ\)</span>, accounting
for censoring and variable follow-up times.</li>
<li>However, the distribution of this <span
class="math inline">\(θ\)</span> is not straightforward and
<strong>requires further consideration</strong>, especially for
constructing confidence intervals or hypothesis testing.</li>
</ul>
</div>
<div id="hybrid-frequentistbayesian-approach" class="section level3"
number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Hybrid
Frequentist/Bayesian approach</h3>
<p>This approach enhances traditional frequentist models by
incorporating uncertainty through <strong>probability
distributions</strong>,</p>
<ul>
<li>The <strong>Beta distribution</strong> allows uncertainty in θ to be
modeled directly from historical data.</li>
<li>The <strong>Beta-binomial distribution</strong> extends the
frequentist framework, producing a richer model for event counts by
integrating uncertainty.</li>
<li>This approach gives a <strong>probabilistically coherent</strong>
and <strong>statistically robust</strong> method for detecting safety
signals in blinded trials, improving decision-making under
uncertainty.</li>
</ul>
<p>In real-world scenarios, <strong>θ (the AE probability)</strong> is
often not known with certainty—it varies across populations, time
points, or study settings. This variation can be <strong>modeled using a
Beta distribution</strong>, which provides a flexible way to represent a
distribution of probabilities for θ.</p>
<ul>
<li><p>The <strong>Beta distribution</strong>, parameterized by α (event
counts) and β (non-event counts), is suitable for modeling probabilities
between 0 and 1.</p></li>
<li><p>The <strong>mean (expected value)</strong> is:</p>
<p><span class="math display">\[
\mu = \frac{\alpha}{\alpha + \beta}
\]</span></p></li>
<li><p>The <strong>variance</strong> is:</p>
<p><span class="math display">\[
\text{Var}(\theta) = \mu(1 - \mu)\phi \quad \text{where} \quad \phi =
\frac{1}{\alpha + \beta}
\]</span></p></li>
<li><p>Example: Historical data from 500 patients show 10 with AEs and
490 without → <span class="math inline">\(\text{Beta}(10,
490)\)</span></p>
<ul>
<li>Mean = 0.02</li>
<li>Standard deviation ≈ 0.006</li>
</ul></li>
</ul>
<p>While the most likely value of θ is 0.02, there’s <strong>inherent
variability</strong>, captured by the <strong>spread of the
distribution</strong>.</p>
<p>If θ follows a <strong>Beta(α, β)</strong> distribution and Y ~
Binomial(n, θ), then the <strong>marginal distribution of Y</strong>
(i.e., accounting for uncertainty in θ) is the <strong>Beta-Binomial
distribution</strong>.</p>
<ul>
<li><p>Probability mass function:</p>
<p><span class="math display">\[
f(y; \alpha, \beta) = \binom{n}{y} \cdot \frac{B(\alpha + y, \beta + n -
y)}{B(\alpha, \beta)}
\]</span></p></li>
<li><p>Expected value and variance:</p>
<p><span class="math display">\[
\mathbb{E}[Y] = n\mu = \frac{n\alpha}{\alpha + \beta}
\]</span></p>
<p><span class="math display">\[
\text{Var}(Y) = n\mu(1 - \mu)\left[\frac{1 + (n - 1)\phi}{1 +
\phi}\right]
\]</span></p></li>
</ul>
<p>This variance is <strong>higher than in the standard binomial
model</strong>, reflecting the extra uncertainty from estimating θ
rather than treating it as fixed.</p>
<p>This hybrid method enhances the frequentist binomial model by
accounting for <strong>uncertainty in historical estimates</strong>.
Instead of assuming a fixed θ, it treats θ as a <strong>random
variable</strong> based on real-world data.</p>
<ul>
<li><strong>More realistic modeling</strong>: Reflects natural variation
in AE rates between studies or populations.</li>
<li><strong>Improved inference</strong>: Wider distributions reduce
overconfidence and better balance false/missed alarm risks.</li>
<li><strong>Consistency</strong>: When α and β are large (i.e., precise
historical data), the Beta distribution approaches a point estimate, and
the beta-binomial converges to the standard binomial.</li>
</ul>
</div>
<div id="bayesian-methods" class="section level3" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Bayesian
methods</h3>
<p>Bayesian approaches provide a <strong>rational, structured way to
update beliefs</strong> as new data become available. In the context of
SSD for blinded trials, where uncertainty is high and full treatment
assignments are not yet revealed, Bayesian methods allow sponsors
to:</p>
<ul>
<li>Incorporate <strong>historical safety data</strong> (e.g., placebo
AE rates),</li>
<li>Reflect <strong>uncertainty in model parameters</strong> (such as
event probabilities),</li>
<li>Continuously <strong>update risk assessments</strong> without
breaking the blind.</li>
</ul>
<p>This is especially useful for detecting early signals without making
premature, binary conclusions from sparse data.</p>
<p>At its core, Bayesian analysis relies on <strong>Bayes’
Theorem</strong>:</p>
<p><span class="math display">\[
P(\theta | y) = \frac{P(y | \theta) P(\theta)}{P(y)}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(P(\theta)\)</span> = prior belief about
the AE rate (from historical data),</li>
<li><span class="math inline">\(P(y | \theta)\)</span> = likelihood (how
likely observed data is under a given AE rate),</li>
<li><span class="math inline">\(P(\theta | y)\)</span> = posterior
belief after observing data <span class="math inline">\(y\)</span>.</li>
</ul>
<p>This framework allows for <strong>inductive learning</strong>, making
it well-suited for ongoing SSD as trial data accumulate.</p>
<p>As Example:</p>
<ul>
<li><strong>Prior Distribution <span
class="math inline">\(P(\theta)\)</span>:</strong> Represents the belief
about AE rates before observing new data. For example, if historical
data show 10 AEs in 500 placebo patients, a <strong>Beta(10,
490)</strong> distribution is a natural prior.</li>
<li><strong>Likelihood <span
class="math inline">\(P(y|\theta)\)</span>:</strong> Based on the
binomial model, this reflects the chance of seeing Y AEs out of n
patients given a particular value of θ.</li>
<li><strong>Posterior Distribution <span class="math inline">\(P(\theta
| y)\)</span>:</strong> Updated belief about θ after seeing the blinded
data, used to judge if a safety signal is emerging.</li>
</ul>
<p>In blinded trials, the treatment group (placebo vs. active drug) is
unknown, introducing <strong>latent structure</strong>. A <strong>finite
mixture model</strong> addresses this by modeling:</p>
<ul>
<li>Multiple possible distributions (e.g., AE rates for placebo and
active groups),</li>
<li>With weights <span class="math inline">\(\pi_j\)</span> representing
the <strong>probability of each group assignment</strong>.</li>
</ul>
<p>This allows a comprehensive analysis without needing to unblind,
maintaining trial integrity.</p>
</div>
</div>
<div id="bayesian-application-to-ssd" class="section level2"
number="3.3">
<h2><span class="header-section-number">3.3</span> Bayesian application
to SSD</h2>
<div id="core-bayesian-framework" class="section level3 unnumbered">
<h3 class="unnumbered">Core Bayesian Framework</h3>
<p>At the heart of the Bayesian SSD approach is the idea of computing
the <strong>posterior probability</strong> that a clinical parameter
(e.g., AE rate θ) <strong>exceeds a critical safety threshold
(θc)</strong>:</p>
<p><span class="math display">\[
\Pr(\theta &gt; \theta_c \mid \text{blinded data}) &gt;
P_{\text{cutoff}}
\]</span></p>
<ul>
<li><strong>θ</strong> = estimated AE rate or other safety metric (e.g.,
risk difference).</li>
<li><strong>θc</strong> = comparator value, typically based on
historical placebo rates or clinical benchmarks.</li>
<li><strong>P_cutoff</strong> = predefined probability threshold (e.g.,
90%, 95%) used to flag potential safety signals.</li>
</ul>
<p>This decision rule is model-agnostic and applies to any Bayesian
setup, making it a <strong>flexible universal framework</strong> for
SSD.</p>
<p>To implement a <strong>Bayesian SSD approach in practice</strong>,
follow these key principles:</p>
<ol style="list-style-type: decimal">
<li><strong>Define a clinical parameter</strong> of interest (θ), such
as AE rate.</li>
<li><strong>Specify a threshold value (θc)</strong> representing a point
of clinical concern.</li>
<li><strong>Choose an appropriate prior</strong> based on historical
data, model context, and desired neutrality.</li>
<li><strong>Calculate the posterior probability</strong> that θ &gt; θc
given the blinded data.</li>
<li><strong>Compare it to a P_cutoff</strong> value, selected based on
event rarity and desired balance of sensitivity vs. specificity.</li>
</ol>
<p>This framework enables <strong>dynamic, data-driven safety
monitoring</strong> under uncertainty—ideal for the complex, evolving
nature of clinical trials.</p>
<hr />
<p>The <strong>choice of prior distribution</strong> for θ has a major
influence on the posterior, especially when event counts are small. A
“non-informative” prior might seem neutral but can actually
<strong>bias</strong> the result if not chosen carefully.</p>
<ul>
<li><p><strong>Beta(1/3, 1/3)</strong> (Kerman’s prior) is
<strong>centered on the sample mean</strong> and is often used as a
neutral or default prior because of its balanced shape.</p></li>
<li><p>For <strong>mixture models</strong> (where θ comes from both
placebo and treatment arms), a generalized version is used:</p>
<p><span class="math display">\[
\theta \sim \text{Beta}(1/3 + mp, \ 1/3 + m(1-p))
\]</span></p>
<p>where <code>p</code> is the historical incidence and <code>m</code>
is the prior’s effective sample size.</p></li>
<li><p>For <strong>overall pooled models</strong>, priors can also be
derived from historical data but should be
<strong>down-weighted</strong> to reflect uncertainty (e.g., treating
historical data as if it contributes one subject’s worth of
information).</p></li>
</ul>
<p>In all cases, priors should reflect <strong>real-world clinical
understanding</strong> while guarding against overconfidence or undue
influence.</p>
<hr />
<p><strong>📈 Setting Thresholds: The Role of P_cutoff</strong></p>
<p>The <strong>P_cutoff</strong> is the posterior probability required
to declare a potential safety signal. Choosing this value impacts the
balance between <strong>sensitivity</strong> (detecting true signals)
and <strong>specificity</strong> (avoiding false alarms).</p>
<ul>
<li>For <strong>rare events (≤5%)</strong>, lower cutoffs like
<strong>0.90 to 0.925</strong> are suitable, because the goal is to be
sensitive to early potential risks.</li>
<li>For <strong>more frequent events (≥10%)</strong>, stricter cutoffs
like <strong>0.975</strong> are used, to reduce the chance of
overreacting to normal variation.</li>
</ul>
<p>This mirrors traditional hypothesis testing logic—more common
outcomes require more convincing evidence to flag as abnormal.</p>
</div>
<div id="general-considerations" class="section level3 unnumbered">
<h3 class="unnumbered">General Considerations</h3>
<p><strong>Patient Population</strong></p>
<p>The objective is to clearly define and select the appropriate patient
population for the study, based on:</p>
<ul>
<li>The specific medical indication being studied (e.g., a certain
cancer type, diabetes, etc.)</li>
<li>Any other relevant classifications or criteria (e.g., disease
severity, prior treatments). Ensuring that the enrolled population
reflects the target population for which the treatment is intended helps
maintain relevance and generalizability of the results.</li>
</ul>
<hr />
<p><strong>TEAEs of Special Focus</strong></p>
<p>TEAEs of special focus are pre-identified adverse events of
particular interest in a trial, typically outlined in the Program
Statistical Analysis Plan (PSAP). These usually include:</p>
<ul>
<li>Anticipated TEAEs: Events common in the disease population,
regardless of drug exposure (background events).</li>
<li>Expected ADRs (Adverse Drug Reactions): Events known or suspected to
be caused by the investigational drug.</li>
</ul>
<p>Key Considerations:</p>
<ul>
<li>Observing a single or small number of anticipated events does not
automatically signal causality.</li>
<li>To assess causality, an unblinded aggregate analysis is
used—comparing event rates between the treatment and control groups to
identify meaningful differences.</li>
<li>These analyses should only be initiated when statistical thresholds
(described later) indicate a potential safety signal.</li>
</ul>
<hr />
<p><strong>Data Sources on Historical Controls</strong></p>
<p>When monitoring safety in blinded studies, researchers compare
ongoing event rates to historical background rates to detect abnormal
patterns. The goal is to identify if observed rates of anticipated AEs
are higher than expected, potentially indicating a safety signal.</p>
<p>Data Source Considerations:</p>
<ul>
<li><p>Use a meta-analytic approach (combining results from multiple
studies) to estimate background rates, preferably from placebo/control
arms.</p></li>
<li><p>Preferred sources (ranked by data quality and bias control):</p>
<ol style="list-style-type: decimal">
<li>Double-blind RCTs – best for avoiding bias.</li>
<li>Open-label RCTs</li>
<li>Single-arm multi-center trials</li>
<li>Retrospective cohort studies</li>
<li>Electronic Health Records (RWE)</li>
</ol></li>
<li><p>Only sources 1–3 are typically used directly in meta-analyses.
Others (4–5) may offer supporting evidence but require caution due to
potential biases.</p></li>
<li><p>Relevance and comparability to the ongoing study population and
design is critical when selecting historical data.</p></li>
</ul>
<hr />
<p><strong>Probability Thresholds for Flagging Safety
Signals</strong></p>
<p>To avoid unblinding prematurely, a trigger-based approach is
used:</p>
<ul>
<li>A posterior probability is calculated indicating the likelihood that
the AE rate in the treatment group exceeds that of the control group by
more than expected (background rate).</li>
<li>This probability threshold is pre-specified and agreed upon by the
project team.</li>
</ul>
<p>Trigger System Example:</p>
<ul>
<li>Yellow Flag (warning): Posterior probability &gt; 80%</li>
<li>Red Flag (alert): Posterior probability &gt; 82%</li>
<li>This system allows ongoing blinded monitoring while maintaining
trial integrity.</li>
</ul>
<p>Considerations for Threshold Setting:</p>
<ul>
<li><p>Thresholds should not be overly conservative (e.g., &gt;90%) to
avoid missing real signals due to low power.</p></li>
<li><p>Several factors influence these probabilities:</p>
<ul>
<li>Event rarity (common vs. rare AEs)</li>
<li>Sample size of current and historical data</li>
<li>Degree of heterogeneity across studies</li>
</ul></li>
</ul>
<p>Regulatory Context:</p>
<ul>
<li><p>The FDA recommends:</p>
<ul>
<li>Clearly documenting why specific events were selected for
monitoring.</li>
<li>Justifying the choice of thresholds used to trigger
evaluation/unblinding.</li>
</ul></li>
<li><p>Any decision to unblind is made by the Blinded Review Team (BRT),
which includes both clinical and statistical experts.</p></li>
<li><p>Whether an imbalance suggests a reasonable possibility of
causality (and therefore needs IND reporting) is a Sponsor-level
judgment.</p></li>
</ul>
</div>
</div>
<div id="binomial-beta-model" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Binomial-Beta
Model</h2>
<p>In this document, we present 2 methods for monitoring safety data
from blinded ongoing clinical trials with the aim of detecting potential
safety signals. 1) Bayesian Markov Chain Monte Carlo (MCMC) method:
Subject level data with blinded treatment are modeled using a mixture of
binomial distribution with an indicator variable and the MCMC algorithm
is used to estimate the parameters accounting for variable follow-up
times across subjects. 2) Simpler Monte Carlo (MC) approximation method:
The method is based on study level data for planning purposes and result
comparison with the primary method. Because the simpler approximation
method is based on study-level data, the model assumes a fixed follow-up
time for all subjects and an approximation has to be made to account for
this.</p>
<div id="prior-distributions-chi-square-approach"
class="section level3 unnumbered">
<h3 class="unnumbered">Prior Distributions Chi-square Approach</h3>
<p>The Chi-square approach is a practical method to construct an
informative prior from multiple historical studies by first assessing
their consistency and then pooling their data. It is termed “Chi-square”
because it typically relies on <strong>Cochran’s Q (chi-square)
test</strong> to check <strong>heterogeneity</strong> (i.e. whether
observed differences in event rates across studies are beyond chance
variability). In essence, this approach asks: <em>Can we treat all
historical studies as having a common underlying event rate?</em> If
yes, the data are combined (pooled) to form a single prior distribution.
If not (significant heterogeneity), adjustments are made – for example,
using a <em>random-effects</em> meta-analysis to allow each study to
have its own rate and account for between-study variance. This approach
is appropriate when you have a few historical studies and want a
straightforward summary of the historical event rate. It works best if
the studies are fairly homogeneous (similar populations, endpoints,
etc.), or if you plan to widen the prior’s uncertainty when they
differ.</p>
<p>In practice, the Chi-square approach often mirrors a
<strong>classical meta-analysis</strong> of proportions. It might be
used in trial planning to set a prior on a control event rate by
combining past control group rates. It is conceptually simpler than MAP:
it does not fully model hierarchical variation, but rather uses
hypothesis testing and summary statistics to decide how much information
to borrow.</p>
<hr />
<p><strong>Mathematical Formulation</strong></p>
<p>Suppose we have historical studies <span class="math inline">\(i =
1,2,\dots,k\)</span>, each with <span
class="math inline">\(x\_i\)</span> events out of <span
class="math inline">\(n\_i\)</span> participants (event rate estimate
<span class="math inline">\(\hat p\_i = x\_i/n\_i\)</span>). The
Chi-square approach involves the following key elements and
equations:</p>
<ul>
<li><p><strong>Fixed-effect (pooled) model:</strong> This assumes a
common true event rate <span class="math inline">\(p\)</span> across all
studies. The weighted pooled estimate can be obtained by a weighted
average of <span class="math inline">\(\hat p\_i\)</span>. A simple
weighting is by sample size (for event rates, this is reasonable if
rates are not extremely low/high). The <em>pooled event rate</em> would
be: <span class="math inline">\(\bar p = \frac{\sum_{i=1}^k w_i \hat
p_i}{\sum_{i=1}^k w_i},\)</span> where <span
class="math inline">\(w\_i\)</span> are weights for study <span
class="math inline">\(i\)</span>. For example, one may choose <span
class="math inline">\(w\_i = n\_i\)</span> for simple pooling or <span
class="math inline">\(w\_i = 1/\text{Var}(\hat p\_i)\)</span> if using
inverse-variance. Under a fixed-effect assumption (no between-study
heterogeneity), <span class="math inline">\(\bar p\)</span> estimates
the common event rate.</p></li>
<li><p><strong>Cochran’s Q (Chi-square test for heterogeneity):</strong>
To assess if the <span class="math inline">\(k\)</span> study results
are consistent with a common rate, we compute <span
class="math inline">\(Q = \sum_{i=1}^k w_i (\hat p_i - \bar
p)^2.\)</span> Under the null hypothesis of homogeneity (all studies
share the same true <span class="math inline">\(p\)</span>), <span
class="math inline">\(Q\)</span> follows a chi-square distribution with
<span class="math inline">\(k-1\)</span> degrees of freedom. A
<em>large</em> <span class="math inline">\(Q\)</span> (or small <span
class="math inline">\(p\)</span>-value) indicates significant
heterogeneity – meaning the event rates likely differ beyond chance. The
<strong>I² statistic</strong> is often used alongside <span
class="math inline">\(Q\)</span> to quantify heterogeneity: $I^2 = %, $
which is the percentage of total variance across studies due to true
heterogeneity rather than chance. (For example, <span
class="math inline">\(I^2=0\)</span> if all variation is consistent with
chance, and higher values imply more between-study
variability.)</p></li>
<li><p><strong>Random-effects model (if needed):</strong> If
heterogeneity is detected, a random-effects meta-analysis is used to
incorporate between-study variance. This assumes each study’s true event
rate <span class="math inline">\(\theta\_i\)</span> varies around an
overall mean <span class="math inline">\(\mu\)</span>. A common
assumption is <span class="math inline">\(\theta\_i \sim
\mathcal{N}(\mu,\tau^2)\)</span> on some scale (often the log-odds scale
for proportions, or directly on the proportion scale for small
heterogeneity). The <strong>DerSimonian-Laird (DL)</strong> method
provides an estimate for the between-study variance <span
class="math inline">\(\tau^2\)</span> using the <span
class="math inline">\(Q\)</span> statistic. In simplified form: <span
class="math inline">\(\hat\tau^2 = \frac{Q - (k-1)}{\sum_{i}w_i - \sum_i
w_i^2/\sum_i w_i},\)</span> if <span
class="math inline">\(Q&gt;(k-1)\)</span> (and <span
class="math inline">\(\hat\tau^2=0\)</span> if <span
class="math inline">\(Q\le k-1\)</span>). Updated weights become <span
class="math inline">\(w\_i^\* = 1/(\text{Var}(\hat
p\_i)+\hat\tau^2)\)</span>, and the <em>random-effects pooled
estimate</em> is <span class="math inline">\(\hat p_{\text{RE}} =
\frac{\sum_i w_i^* \hat p_i}{\sum_i w_i^*}.\)</span> The variance of
<span class="math inline">\(\hat p\_{\text{RE}}\)</span> is <span
class="math inline">\(1/\sum\_i w\_i^\*\)</span>. This model essentially
treats the true event rate in each study as a draw from a distribution
with mean <span class="math inline">\(\mu\)</span> and variance <span
class="math inline">\(\tau^2\)</span>. It acknowledges heterogeneity by
<strong>inflating the uncertainty</strong> of the combined
estimate.</p></li>
<li><p><strong>Deriving a prior distribution:</strong> Once an overall
event rate and variance are determined (from either fixed or
random-effects), we translate that into a prior distribution for the new
trial’s event rate. A convenient choice for an event rate (a
probability) is a <strong>Beta distribution</strong> prior. The Beta
distribution is defined on [0,1] and is conjugate to the Binomial
likelihood. We can think of the Beta prior parameters as “pseudo-counts”
of events and non-events. For instance, a Beta(<span
class="math inline">\(\alpha,\beta\)</span>) prior corresponds to <span
class="math inline">\(\alpha-1\)</span> prior “successes” and <span
class="math inline">\(\beta-1\)</span> “failures” observed
historically.</p>
<p>– <strong>If using fixed-effect pooling (homogeneous case):</strong>
We might set <span class="math inline">\(\alpha = x\_{\text{total}} +
1\)</span> and <span class="math inline">\(\beta = n\_{\text{total}} -
x\_{\text{total}} + 1\)</span> (assuming a flat base prior Beta(1,1)).
Here <span
class="math inline">\(x\_{\text{total}}=\sum\_{i}x\_i\)</span> and <span
class="math inline">\(n\_{\text{total}}=\sum\_{i}n\_i\)</span>. This
yields a Beta prior with mean <span
class="math inline">\(\frac{\alpha}{\alpha+\beta} \approx
\frac{x\_{\text{total}}}{n\_{\text{total}}} = \bar p\)</span> and a
variance that reflects the binomial variation from the aggregated data.
Essentially, we treat all historical data as one big trial. For example,
if across 3 studies there were 50 events out of 200 patients, the prior
could be Beta(51,151), which has mean ~0.25 (25% event rate). We might
also incorporate a <em>slight</em> increase in <span
class="math inline">\(\beta\)</span> (or decrease in <span
class="math inline">\(\alpha\)</span>) to deliberately <strong>widen the
prior</strong> if we suspect any unmodeled heterogeneity, ensuring we’re
not overconfident.</p>
<p>– <strong>If using random-effects (heterogeneous case):</strong>
There isn’t a single obvious closed-form prior, because the true rates
vary. One simple approach is to choose a <strong>conservative
(over-dispersed) Beta prior</strong> that has the same mean as <span
class="math inline">\(\hat p\_{\text{RE}}\)</span> but larger variance
to account for between-study differences. For instance, one can
<strong>match moments</strong>: set the Beta’s mean <span
class="math inline">\(m = \hat p\_{\text{RE}}\)</span> and variance
<span class="math inline">\(v = \hat p\_{\text{RE}}(1-\hat
p\_{\text{RE}})/N\_{\text{eff}}\)</span>, where <span
class="math inline">\(N\_{\text{eff}}\)</span> is an “effective sample
size” smaller than <span
class="math inline">\(n\_{\text{total}}\)</span>. <span
class="math inline">\(N\_{\text{eff}}\)</span> can be chosen such that
the Beta’s variance <span class="math inline">\(v\)</span> is roughly
equal to the <strong>total uncertainty</strong> (within-study binomial
error plus between-study variance <span
class="math inline">\(\tau^2\)</span>). Another approach is to use the
<strong>predictive interval</strong> from the random-effects
meta-analysis: for a new study of similar size, the event rate is
expected (with 95% probability) to lie in, say, [L, U]. One can then
choose a Beta prior whose 95% credible interval is [L, U], thus
reflecting the heterogeneity. There is some art to this, but the
principle is to <em>down-weight the historical information</em> when
heterogeneity exists. Essentially, the prior will be broader (less
informative) as heterogeneity increases, reflecting greater uncertainty
about the event rate in a new setting.</p></li>
</ul>
<p>In summary, the Chi-square approach uses classical meta-analytic
formulas (like <span class="math inline">\(Q\)</span> and possibly DL
random effects) to derive a Beta (or similar) prior for the event rate.
Each term in these equations corresponds to either a measure of
variability (<span class="math inline">\(Q\)</span>, <span
class="math inline">\(\tau^2\)</span>) or a summary of data (pooled
<span class="math inline">\(\bar p\)</span>). The Beta prior’s
parameters <span class="math inline">\((\alpha,\beta)\)</span> are
interpreted in plain language as prior evidence equivalent to <span
class="math inline">\(\alpha-1\)</span> events and <span
class="math inline">\(\beta-1\)</span> non-events.</p>
<hr />
<p><strong>Step-by-Step Application</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Collect and summarize historical data:</strong> List each
prior study’s sample size (<span class="math inline">\(n\_i\)</span>)
and number of events (<span class="math inline">\(x\_i\)</span>).
Compute the observed event rates <span class="math inline">\(\hat p\_i =
x\_i/n\_i\)</span> for each study.</p></li>
<li><p><strong>Assess heterogeneity:</strong> Calculate Cochran’s <span
class="math inline">\(Q\)</span> statistic and its <span
class="math inline">\(p\)</span>-value (and/or <span
class="math inline">\(I^2\)</span>).</p>
<ul>
<li>If <span class="math inline">\(Q\)</span> is not significant (or
<span class="math inline">\(I^2\)</span> is very low), assume the
studies are consistent with a common event rate.</li>
<li>If <span class="math inline">\(Q\)</span> is significant (large
heterogeneity), acknowledge that event rates differ across studies.</li>
</ul></li>
<li><p><strong>Choose pooling model:</strong></p>
<ul>
<li><strong>Low heterogeneity (homogeneous)</strong>: Use a
<em>fixed-effect</em> pooling. Combine all events and all patients:
<span class="math inline">\(x\_{\text{total}}=\sum x\_i\)</span>, <span
class="math inline">\(n\_{\text{total}}=\sum n\_i\)</span>. The pooled
estimate is <span class="math inline">\(\bar p =
x\_{\text{total}}/n\_{\text{total}}\)</span>. Compute the binomial
variance <span class="math inline">\(\mathrm{Var}(\bar p) = \bar
p(1-\bar p)/n\_{\text{total}}\)</span> (or get a confidence interval for
<span class="math inline">\(\bar p\)</span>).</li>
<li><strong>Notable heterogeneity</strong>: Use a
<em>random-effects</em> meta-analysis. Estimate <span
class="math inline">\(\tau^2\)</span> (between-study variance) via a
formula (e.g., DerSimonian-Laird). Compute a weighted average <span
class="math inline">\(\hat p\_{\text{RE}}\)</span> with weights <span
class="math inline">\(w\_i^\* = 1/(\text{Var}(\hat
p\_i)+\tau^2)\)</span>. Also calculate an approximate 95%
<strong>prediction interval</strong> for a new study’s true rate: this
is often <span class="math inline">\(\hat p\_{\text{RE}} \pm t\_{(k-1)}
\sqrt{\tau^2 + \mathrm{AvgVar}}\)</span>, where <span
class="math inline">\(\mathrm{AvgVar}\)</span> is the typical
within-study variance and <span
class="math inline">\(t\_{(k-1)}\)</span> is a quantile from a <span
class="math inline">\(t\)</span> distribution with <span
class="math inline">\(k!-!1\)</span> degrees of freedom (to reflect
uncertainty in $p_{}`). This interval gives a range where we expect the
new trial’s event rate to fall, considering between-study
variability.</li>
</ul></li>
<li><p><strong>Derive the prior distribution:</strong> Translate the
meta-analytic result into a prior for the event rate <span
class="math inline">\(p\)</span> in the new trial.</p>
<ul>
<li>If pooling fixed-effect: set a <strong>Beta prior</strong> <span
class="math inline">\(\pi(p) = \mathrm{Beta}(\alpha,\beta)\)</span> with
<span class="math inline">\(\alpha = x\_{\text{total}} + c\)</span> and
<span class="math inline">\(\beta = n\_{\text{total}} -
x\_{\text{total}} + c\)</span>, where <span
class="math inline">\(c\)</span> is a small constant reflecting any
initial prior (for example <span class="math inline">\(c=1\)</span> if
using Beta(1,1) as non-informative base). This Beta has mean <span
class="math inline">\(\approx \bar p\)</span> and a 95% credible
interval roughly <span class="math inline">\(\bar p \pm 1.96\sqrt{\bar
p(1-\bar p)/(n\_{\text{total}}+c)}\)</span>.</li>
<li>If random-effects: choose a <strong>more diffuse prior</strong> to
account for heterogeneity. One way is to reduce the effective sample
size: e.g. <span class="math inline">\(\pi(p) =
\mathrm{Beta}(\tilde\alpha,\tilde\beta)\)</span> such that <span
class="math inline">\(\tilde\alpha/(\tilde\alpha+\tilde\beta) = \hat
p\_{\text{RE}}\)</span> but <span class="math inline">\(\tilde\alpha +
\tilde\beta &lt; n\_{\text{total}}\)</span> to widen the variance. You
can solve <span class="math inline">\(\tilde\alpha = m \cdot
N\_{\text{eff}}\)</span> and <span class="math inline">\(\tilde\beta =
(1-m)\cdot N\_{\text{eff}}\)</span> with <span
class="math inline">\(m=\hat p\_{\text{RE}}\)</span> and choose <span
class="math inline">\(N\_{\text{eff}}\)</span> to achieve a desired
variance (for instance, set <span class="math inline">\(N\_{\text{eff}}
= n\_{\text{total}}/(1+\text{extra variance inflation})\)</span> or use
the predictive interval width to guide it). Another approach is to adopt
a <strong>mixture prior</strong>: for example, a mixture of Beta
distributions each reflecting different historical studies or clusters,
though this gets more complex. In most cases, a single Beta with
inflated variance suffices for simplicity.</li>
</ul></li>
<li><p><strong>Validate or adjust (if necessary):</strong> It’s good
practice to double-check that the chosen prior makes sense. Plot the
Beta prior density to see if it reasonably covers the range of
historical study estimates. If one historical study was an outlier,
ensure the prior’s spread covers that or consider excluding that study
if it’s not deemed relevant. Essentially, verify that the prior is
neither too narrow (overconfident) nor shifted in a way that ignores
important differences. If the prior seems too informative given
heterogeneity, you can scale back <span
class="math inline">\(\alpha,\beta\)</span> (reducing <span
class="math inline">\(N\_{\text{eff}}\)</span>). Conversely, if
heterogeneity was low and we might be underutilizing information, one
might confidently use <span
class="math inline">\(n\_{\text{total}}\)</span>.</p></li>
</ol>
<hr />
<p><strong>Strengths:</strong></p>
<ul>
<li><em>Simplicity:</em> The Chi-square approach is relatively easy to
execute and explain. It uses familiar meta-analysis techniques
(proportions, chi-square test) that many clinical trialists know from
frequentist analysis. For example, summarizing “in 5 previous studies
the event rate ranged 20–25%, with no significant heterogeneity, so we
use a Beta prior centered around 23%” is intuitively
understandable.</li>
<li><em>Transparency:</em> Each step (testing heterogeneity, pooling or
not) is explicit. Investigators retain control to include or exclude
certain studies based on clinical judgment (e.g. exclude a study that
appears clinically different if heterogeneity is high).</li>
<li><em>Speed:</em> It requires minimal specialized software – one can
do it with basic stats tools or even by hand for simple cases. This
makes it practical in early trial planning when quick estimates are
needed.</li>
<li><em>Reasonable with homogeneous data:</em> If historical studies
truly have a common rate, this approach will produce a tight prior
reflecting all the data. If heterogeneity is truly absent or very low,
the Chi-square approach and the more complex MAP approach will give
similar results (since modeling heterogeneity isn’t needed in that
case).</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li><em>All-or-nothing heterogeneity handling:</em> The reliance on a
heterogeneity test means the approach can be somewhat dichotomous –
pooling fully versus ad-hoc adjustments. Cochran’s Q has low power with
few studies, so one might miss heterogeneity and pool inappropriately
(overly narrow prior), or if many studies, Q can be overly sensitive,
leading one to declare heterogeneity and perhaps over-broaden or even
discard data. There’s no continuous “partial pooling” built-in; any
down-weighting of data due to heterogeneity is manual and subjective
(e.g., deciding an effective sample size).</li>
<li><em>Less formal uncertainty modeling:</em> Unlike the MAP approach,
the Chi-square method does not explicitly model the distribution of true
event rates across studies. As a result, the uncertainty due to
between-study variability might be handled in a ad-hoc manner. For
instance, choosing how much to inflate the Beta prior variance when
<span class="math inline">\(I^2\)</span> is 50% can be tricky and might
under- or over-estimate the true uncertainty.</li>
<li><em>Potential bias if data differ:</em> If one historical study’s
population is slightly different (say, older patients with higher event
rate), a simple pooled prior might be biased for the new trial
population. The Chi-square approach doesn’t inherently account for
covariates or differences in study design that could explain
heterogeneity. It treats heterogeneity as noise rather than something to
model. This can lead to a prior that is centered at a value not truly
applicable to the new trial.</li>
<li><em>No automatic conflict resolution:</em> In Bayesian analysis, a
<em>prior-data conflict</em> refers to the prior strongly disagreeing
with new data. The Chi-square approach doesn’t have a mechanism to
identify or mitigate this conflict beyond the initial heterogeneity
test. If the new trial’s early data look very different from the prior,
one must recognize this and perhaps discount the prior in an ad-hoc
interim adjustment. In contrast, some Bayesian methods (like robust MAP)
include heavy-tailed priors that automatically reduce influence in such
conflicts.</li>
<li><em>Not a fully Bayesian updating of evidence:</em> One could argue
the Chi-square approach is a hybrid – you use frequentist meta-analysis,
then plug that into a Bayesian prior. It doesn’t use the Bayesian
formalism to combine historical data and new data in one model; instead
it’s a two-step approach. While this is not inherently bad, it lacks the
coherence of a single model and may be suboptimal in fully leveraging
information.</li>
</ul>
<p>In short, the Chi-square approach is <strong>straightforward but can
be rigid</strong>. It works well for quick summaries when data are
consistent, but its ad-hoc nature in handling heterogeneity is both a
strength (simple rule) and a weakness (potentially inadequate
modeling).</p>
<hr />
<p><strong>When use the Chi-square approach when:</strong></p>
<ul>
<li><strong>Historical studies are few and similar:</strong> If you only
have 1–3 prior studies and they appear clinically and statistically
consistent (e.g., similar patient populations, same treatment and
endpoint, and no obvious discrepancies in event rates), the Chi-square
approach is attractive. With minimal heterogeneity, a simple pooled Beta
prior is almost as good as a more complex model, and it’s easier to
justify to stakeholders.</li>
<li><strong>A quick, rough prior is needed:</strong> Early in trial
planning, you might need a ballpark prior for calculations (like sample
size or Bayesian power). The Chi-square method can rapidly provide an
estimate. For instance, if two past trials had 30% and 35% event rates
with similar sample sizes, you might immediately propose a Beta prior
around 0.33 with a certain pseudo-count, without doing a full
hierarchical model analysis.</li>
<li><strong>Computational or expertise limitations:</strong> Not all
teams have Bayesian experts or the software set up to run hierarchical
models. The Chi-square approach can be done with basic stats knowledge.
Regulators or team members might also be more familiar with seeing a
pooled estimate and a chi-square test for heterogeneity (common in
meta-analysis) than with a Bayesian hierarchical model. If you need to
communicate the prior derivation in a simpler way, Chi-square approach
has that interpretability (though note: regulatory agencies are becoming
conversant with MAP for historical borrowing too).</li>
</ul>
<p>However, be cautious using the Chi-square method if there is
<strong>substantial heterogeneity or many historical studies</strong>.
In those cases, the MAP approach is usually preferred:</p>
</div>
<div id="meta-analytic-predictive-map-approach"
class="section level3 unnumbered">
<h3 class="unnumbered">Meta-Analytic Predictive (MAP) Approach</h3>
<p>The Meta-Analytic Predictive (MAP) approach is a Bayesian method that
formally <strong>models the historical data in a hierarchical
framework</strong>, capturing both the common trend and the
between-study heterogeneity. Instead of pooling or making yes/no
decisions about heterogeneity, MAP treats the true event rate in each
historical study as a random draw from a population distribution. This
yields a <strong>posterior predictive distribution</strong> for the
event rate in a new trial – that predictive distribution <em>is</em> the
MAP prior for the new trial’s event rate. In other words, MAP uses all
historical evidence to “predict” what the new study’s event rate is
likely to be, with uncertainty bands naturally widened if the historical
results disagree with each other.</p>
<p>Situations appropriate for MAP include having <strong>multiple
historical studies, especially with some heterogeneity</strong>. MAP
shines in complexity: it can borrow strength from many studies while
appropriately down-weighting those that don’t agree. It is a fully
Bayesian approach – the historical data and the new data are linked
through a hierarchical model. Conceptually, it’s like saying: “We have a
distribution of possible event rates (learned from past trials). Let’s
use that distribution as our prior for the new trial’s event rate.” The
approach is grounded in Bayesian meta-analysis techniques and often
implemented with MCMC simulations due to its complexity.</p>
<p>One big advantage is that the MAP prior inherently accounts for
<strong>uncertainty at multiple levels</strong>: it recognizes both
<strong>within-study uncertainty</strong> (each study’s finite sample)
and <strong>between-study uncertainty</strong> (variation in true rates
across studies). The resulting prior is “predictive” in the sense that
it represents our uncertainty about the new trial’s parameter after
seeing the old trials. This method is particularly useful when
historical data are available but not identical to the new trial’s
setting – MAP will downweight the influence of historical data if they
are inconsistent (via a larger between-study variance).</p>
<hr />
<p><strong>Mathematical Formulation</strong></p>
<p>At the heart of MAP is a <strong>Bayesian hierarchical
model</strong>. Let’s define parameters and distributions for a binary
outcome (event rate):</p>
<ul>
<li><p><strong>Data level (likelihood):</strong> For each historical
study <span class="math inline">\(i\)</span> ( <span
class="math inline">\(i=1,\dots,k\)</span> ), denote the true event rate
by <span class="math inline">\(\theta\_i\)</span>. We observe <span
class="math inline">\(x\_i\)</span> events out of <span
class="math inline">\(n\_i\)</span> in that study. We model this as
<span class="math inline">\(x_i \sim \text{Binomial}(n_i,\,
\theta_i),\)</span> meaning given the true event rate <span
class="math inline">\(\theta\_i\)</span>, the number of events follows a
binomial distribution. This is just the standard likelihood for each
study’s data.</p></li>
<li><p><strong>Parameter level (between-trial distribution):</strong>
Now we impose a model on the <em>vector</em> of true rates <span
class="math inline">\((\theta\_1,\theta\_2,\dots,\theta\_k)\)</span> to
capture heterogeneity. A common choice is to assume the <span
class="math inline">\(\theta\_i\)</span> are distributed around some
overall mean. There are a couple of ways to specify this:</p>
<ul>
<li><strong>Beta-Binomial hierarchical model:</strong> Assume each <span
class="math inline">\(\theta\_i\)</span> is a draw from a Beta
distribution: <span class="math inline">\(\theta_i \sim
\text{Beta}(\alpha,\; \beta),\)</span> with hyperparameters <span
class="math inline">\(\alpha,\beta\)</span> (which are unknown to be
estimated). This implies a prior belief that across different studies
the event rate varies according to a Beta distribution. The Beta’s mean
<span class="math inline">\(\frac{\alpha}{\alpha+\beta}\)</span> can be
thought of as the “typical” event rate and its variance <span
class="math inline">\(\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}\)</span>
captures the between-study variability. If <span
class="math inline">\(\alpha\)</span> and <span
class="math inline">\(\beta\)</span> are closer to each other (and
large), the Beta is tight (low heterogeneity); if <span
class="math inline">\(\alpha\)</span> and <span
class="math inline">\(\beta\)</span> are small or very different, the
Beta is broad (high heterogeneity).</li>
<li><strong>Logistic-normal hierarchical model:</strong> Alternatively,
assume <span class="math display">\[\text{logit}(\theta_i) =
\ln\frac{\theta_i}{1-\theta_i} \sim \mathcal{N}(\mu,\; \tau^2).\]</span>
Here <span class="math inline">\(\mu\)</span> is the overall average
log-odds of the event and <span class="math inline">\(\tau^2\)</span> is
the between-study variance on the log-odds scale. This is analogous to
the classical random-effects meta-analysis assumption (normal
distribution of effects), but fully in a Bayesian context. The pair
<span class="math inline">\((\mu,\tau^2)\)</span> are hyperparameters to
be estimated. Logistic-normal is quite flexible and can accommodate
<span class="math inline">\(\theta\_i\)</span> near 0 or 1 without Beta
distribution’s constraints.</li>
</ul>
<p>Both approaches serve the same purpose: introduce
<strong>hyperparameters</strong> (like <span
class="math inline">\(\alpha,\beta\)</span> or <span
class="math inline">\(\mu,\tau\)</span>) that govern the
<em>distribution of true event rates across studies</em>. Let’s use a
generic notation <span class="math inline">\(\psi\)</span> for the set
of hyperparameters (either <span
class="math inline">\({\alpha,\beta}\)</span> or <span
class="math inline">\({\mu,\tau}\)</span>, etc.). The hierarchical model
can be written abstractly as: <span class="math display">\[
p(\theta_1,\dots,\theta_k,\theta_{\text{new}}\mid \psi) =
\Big[\prod_{i=1}^k p(\theta_i \mid \psi)\Big]\; p(\theta_{\text{new}}
\mid \psi),\]</span> where <span
class="math inline">\(p(\theta\_i|\psi)\)</span> is the distribution of
a true rate given hyperparameters (Beta or logistic-normal), and we’ve
included <span class="math inline">\(\theta\_{\text{new}}\)</span> (the
new study’s true event rate) as another draw from the same distribution
(since we consider the new trial exchangeable with the historical ones
<em>a priori</em>). We will use the historical data to learn about <span
class="math inline">\(\psi\)</span>, and then infer <span
class="math inline">\(p(\theta\_{\text{new}})\)</span> from
that.</p></li>
<li><p><strong>Hyperprior level:</strong> We must specify priors for the
hyperparameters <span class="math inline">\(\psi\)</span>. These are
chosen to be relatively non-informative or weakly informative, because
we want the historical data to drive the estimates. For example, if
using <span class="math inline">\((\alpha,\beta)\)</span>, one might put
independent vague priors on them (like <span
class="math inline">\(\alpha,\beta \sim \text{Gamma}(0.01,0.01)\)</span>
or something that is nearly flat over plausible range). If using <span
class="math inline">\((\mu,\tau)\)</span>, one might choose <span
class="math inline">\(\mu \sim \mathcal{N}(0, 10^2)\)</span> (a wide
normal for log-odds, implying a broad guess for event rate around 50%
with large uncertainty) and <span class="math inline">\(\tau \sim
\text{Half-Normal}(0, 2^2)\)</span> or a Half-Cauchy – a prior that
allows substantial heterogeneity but is not overly informative. These
choices should be made carefully, often guided by previous knowledge or
defaults from literature (for instance, half-normal with scale 1 on
<span class="math inline">\(\tau\)</span> is a common weak prior for
heterogeneity on log-odds).</p></li>
</ul>
<p>Given this three-level model (data, parameters, hyperparameters), we
use <strong>Bayesian inference</strong> to combine the information:</p>
<ul>
<li><p><strong>Posterior with historical data:</strong> We apply Bayes’
rule to update our beliefs about <span
class="math inline">\(\psi\)</span> (and the <span
class="math inline">\(\theta\_i\)</span> for <span
class="math inline">\(i=1\ldots k\)</span>) after seeing the historical
outcomes <span class="math inline">\(x\_1,\dots,x\_k\)</span>. The joint
posterior is: <span class="math display">\[ p(\theta_1,\dots,\theta_k,
\psi \mid x_1,\dots,x_k) \propto \left[\prod_{i=1}^k \underbrace{
\binom{n_i}{x_i} \theta_i^{x_i}(1-\theta_i)^{n_i-x_i} }_{\text{Binomial
likelihood for study $i$}} \underbrace{p(\theta_i \mid
\psi)}_{\text{Beta or logit-normal}} \right] \;
\underbrace{p(\psi)}_{\text{hyperprior}}.\]</span> We typically do not
need to write this out explicitly in practice – we use MCMC software to
sample from this posterior. The key point is that the posterior captures
what we have learned about <span class="math inline">\(\psi\)</span>
(the overall rate and heterogeneity) from the data.</p></li>
<li><p><strong>Posterior predictive for new study’s rate (MAP
prior):</strong> The <strong>MAP prior</strong> for the new trial’s
event rate <span class="math inline">\(\theta\_{\text{new}}\)</span> is
the posterior predictive distribution given the historical data. In
formula form: <span
class="math display">\[\pi_{\text{MAP}}(\theta_{\text{new}} \mid
\text{historical data}) = \int p(\theta_{\text{new}} \mid \psi)\; p(\psi
\mid x_1,\dots,x_k)\, d\psi.\]</span> This integral means: we average
over the uncertainty in the hyperparameters <span
class="math inline">\(\psi\)</span> (as described by their posterior) to
predict <span class="math inline">\(\theta\_{\text{new}}\)</span>. In
plainer language, we’ve learned a distribution of possible event rates
(by seeing the past studies), now we derive the implied distribution for
a new study’s rate by <em>integrating out</em> our uncertainty in the
overall mean and heterogeneity. The result <span
class="math inline">\(\pi\_{\text{MAP}}(\theta\_{\text{new}} \mid
\text{data})\)</span> is the <strong>informative prior</strong> to use
for <span class="math inline">\(\theta\_{\text{new}}\)</span> in the new
trial’s analysis. In the context of actual trial analysis, once new data
<span class="math inline">\(y\_{\text{new}}\)</span> is observed, the
posterior for <span class="math inline">\(\theta\_{\text{new}}\)</span>
would be proportional to <span class="math inline">\(p(y\_{\text{new}}
\mid \theta\_{\text{new}}),
\pi\_{\text{MAP}}(\theta\_{\text{new}})\)</span>, as usual.</p>
<p>It’s rare to get a closed-form expression for <span
class="math inline">\(\pi\_{\text{MAP}}(\theta\_{\text{new}})\)</span>.
However, we can characterize it. For example, if we used the
Beta-Binomial model and had a conjugate Beta hyperprior for <span
class="math inline">\(\alpha,\beta\)</span>, one could in theory
integrate to get a Beta mixture. More generally, one uses MCMC samples:
each MCMC draw of <span class="math inline">\(\psi\)</span> produces a
draw of <span class="math inline">\(\theta\_{\text{new}}\)</span> from
<span class="math inline">\(p(\theta\_{\text{new}}|\psi)\)</span>, and
aggregating those yields a Monte Carlo representation of <span
class="math inline">\(\pi\_{\text{MAP}}(\theta\_{\text{new}})\)</span>.
Often this distribution is then <strong>approximated</strong> by a
convenient form (e.g., a Beta distribution or a mixture of Betas) for
easier communication. For instance, one might find that <span
class="math inline">\(\pi\_{\text{MAP}}\)</span> is roughly Beta(20,80)
(just as an example) or perhaps a mixture like 0.7·Beta(15,60) +
0.3·Beta(3,12) if there were bi-modality or excess variance. Tools like
the R package <strong>RBesT</strong> use algorithms to fit a parametric
mixture to the MCMC output.</p></li>
</ul>
<p>To describe each term in plain language:</p>
<ul>
<li><span class="math inline">\(\theta\_i\)</span> = the true event rate
in historical study <span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(\psi\)</span> = hyperparameters (like
overall mean rate and heterogeneity) describing how <span
class="math inline">\(\theta\_i\)</span> values are distributed across
studies.</li>
<li><span class="math inline">\(p(\theta\_i|\psi)\)</span> = the model
for variability in event rates between studies (e.g., “<span
class="math inline">\(\theta\_i\)</span> are around 0.3 with a
between-study standard deviation of 0.05”).</li>
<li><span class="math inline">\(p(\psi)\)</span> = prior belief about
the overall rate and heterogeneity before seeing historical data
(usually vague).</li>
<li><span class="math inline">\(p(\psi|x\_{1..k})\)</span> = updated
belief about those hyperparameters after seeing historical results
(e.g., after 5 studies, we might infer the typical event rate is ~30%
and there’s moderate heterogeneity with <span class="math inline">\(\tau
\approx 0.4\)</span> on log-odds).</li>
<li><span class="math inline">\(\pi\_{\text{MAP}}(\theta\_{\text{new}}|
\text{data})\)</span> = a weighted blend of possible <span
class="math inline">\(\theta\_{\text{new}}\)</span> values, weighted by
how plausible each scenario is given historical data. If historical
studies were very consistent, <span
class="math inline">\(p(\psi|data)\)</span> will be tight (low <span
class="math inline">\(\tau\)</span>) and <span
class="math inline">\(\pi\_{\text{MAP}}(\theta\_{\text{new}})\)</span>
will be concentrated around the common rate. If they were inconsistent,
<span class="math inline">\(p(\psi|data)\)</span> will favor larger
<span class="math inline">\(\tau\)</span> (heterogeneity) and <span
class="math inline">\(\pi\_{\text{MAP}}(\theta\_{\text{new}})\)</span>
will be broader, reflecting our uncertainty what the new rate will
be.</li>
</ul>
<p>To make this concrete, imagine 3 historical studies had event rates
of 10%, 20%, and 30% in similar settings. A MAP model would treat those
as random draws. It might infer an overall average ~20% and substantial
heterogeneity. The MAP prior for a new study’s rate would then be a
distribution perhaps centered near 20% but quite wide (e.g., 95%
interval maybe 5% to 40%). Compare that to a chi-square pooling:
chi-square might have flagged heterogeneity and if one still pooled
naively one might pick 20% ± some fudge. The MAP gives a principled way
to get that wide interval. If instead all 3 studies had ~20%,
heterogeneity would be inferred as low, and MAP prior would be tight
around 20% (small variance).</p>
<hr />
<p><strong>Step-by-Step Application</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Assemble historical data:</strong> Just like before,
gather the outcomes <span class="math inline">\(x\_i\)</span> and sample
sizes <span class="math inline">\(n\_i\)</span> of all relevant
historical trials on the event of interest. Also, carefully consider
inclusion criteria for historical data – ensure the studies are
sufficiently similar to your new trial’s context (patient population,
definitions, etc.), because the MAP will faithfully combine whatever
data you feed it. If one study is markedly different but included, the
MAP model will try to accommodate it via larger heterogeneity, which
might dilute the influence of all data (sometimes that’s warranted,
other times you might exclude that study). In short, garbage in, garbage
out applies: select historical data that you truly consider exchangeable
with the new trial <strong>aside from</strong> random
heterogeneity.</p></li>
<li><p><strong>Specify the hierarchical model:</strong> Choose a
parametric form for between-trial variability and assign priors to its
parameters. For example, decide between a Beta vs logistic-normal model
for <span class="math inline">\(\theta\_i\)</span>. Suppose we choose
the logistic-normal random-effects model (commonly used for
meta-analysis of proportions). We then specify priors like:</p>
<ul>
<li><span class="math inline">\(\mu \sim N(0, 2^2)\)</span> (implies a
prior guess that the overall event rate is around 50% but could be
anywhere from a few% to ~98% within 2 SDs; we could shift this if we
have prior expert belief on the general rate).</li>
<li><span class="math inline">\(\tau \sim \text{Half-Normal}(0,
1^2)\)</span> or perhaps <span class="math inline">\(\tau \sim
\text{Half-Cauchy}(0, 1)\)</span> (a weakly informative prior that
allows heterogeneity but doesn’t put too much mass on extreme values;
e.g., <span class="math inline">\(\tau\)</span> on log-odds around 1
means quite large heterogeneity). If using Beta-Binomial: we might say
<span class="math inline">\(\alpha,\beta \sim
\text{Uniform}(0,10)\)</span> or something moderate. There’s also a
concept of putting a prior on the between-study variance directly (e.g.,
<span class="math inline">\(\tau^2 \sim \text{Inverse-Gamma}\)</span>),
but modern Bayesian practice often prefers more interpretable priors on
<span class="math inline">\(\tau\)</span>.</li>
</ul>
<p>It’s important here to do <em>sensitivity checking</em>: since the
MAP will be used as prior, you might examine how different reasonable
hyperpriors affect the result, especially if data are sparse. However,
with moderate amount of historical data, the influence of the hyperprior
will be minor.</p></li>
<li><p><strong>Fit the model to historical data:</strong> Using a
Bayesian software (Stan, JAGS, BUGS, or specialized R packages like
<strong>RBesT</strong> or <strong>bayesmeta</strong>), perform posterior
sampling or approximation. Essentially, you feed in <span
class="math inline">\({x\_i, n\_i}\)</span> and get posterior draws of
<span class="math inline">\((\mu,\tau)\)</span> (or <span
class="math inline">\((\alpha,\beta)\)</span>) and possibly of each
<span class="math inline">\(\theta\_i\)</span>. Verify the model fit by
checking if the posterior predicts the observed data well (e.g.,
posterior predictive checks for each study’s event count can be done).
If one study is extremely improbable under the model, that might
indicate model mis-specification or that the study is an outlier (you
might consider a more robust model or excluding that study). Usually,
though, this step is straightforward with modern tools, and you obtain
MCMC samples from <span class="math inline">\(p(\psi |
\text{data})\)</span>.</p></li>
<li><p><strong>Obtain the MAP prior (posterior predictive for
new):</strong> Extract the predictive distribution for <span
class="math inline">\(\theta\_{\text{new}}\)</span>. If using MCMC, for
each saved draw of <span class="math inline">\((\mu,\tau)\)</span>, draw
a sample <span class="math inline">\(\theta\_{\text{new}}^{(s)} \sim
p(\theta\_{\text{new}}|\mu^{(s)},\tau^{(s)})\)</span>. This gives a
large sample of <span
class="math inline">\(\theta\_{\text{new}}\)</span> values from the MAP
prior. Now summarize this distribution:</p>
<ul>
<li>Compute its mean (this is the MAP prior’s expected value for the
event rate, a kind of weighted average of historical rates).</li>
<li>Compute credible intervals (e.g., 95% interval from the 2.5th to
97.5th percentile of the draws). This interval is essentially the
Bayesian <em>prediction interval</em> for a new study’s true rate,
incorporating uncertainty in both <span
class="math inline">\(\mu\)</span> and <span
class="math inline">\(\tau\)</span>.</li>
<li>Plot the density or histogram to see its shape. It might be
approximately bell-shaped on probability scale, or skewed, or even
multi-modal if data had clusters. If it’s multi-modal (say, two groups
of studies had different rates), the MAP prior might show two bumps.
That’s okay – it reflects ambiguity (maybe two subpopulations). One can
either embrace that (use a mixture prior explicitly) or refine model
(sometimes adding covariates or splitting the meta-analysis by known
differences).</li>
</ul></li>
<li><p><strong>Approximate with convenient distribution (optional but
recommended):</strong> For practical use in trial design or analysis,
it’s handy to express <span
class="math inline">\(\pi\_{\text{MAP}}(\theta\_{\text{new}})\)</span>
in a closed form. Often a <strong>mixture of Beta distributions</strong>
is used for binary endpoints. For instance, using an EM algorithm to fit
a 2- or 3-component Beta mixture to the MCMC sample. The result could be
something like <span class="math inline">\(\pi\_{\text{MAP}}(\theta)
\approx 0.6\mathrm{Beta}(a\_1,b\_1) +
0.4\mathrm{Beta}(a\_2,b\_2)\)</span>. This mixture can then be used in
standard Bayesian calculations without needing MCMC each time. If the
MAP prior is roughly unimodal, a single Beta might even suffice (by
matching the mean and variance of the sample draws). The approximation
step doesn’t change the inference; it’s a technical convenience. When
reporting the prior, you might say, <em>“Based on the MAP analysis of 5
historical studies, the prior for the event rate in the new trial is
well-approximated by a Beta(18, 42) distribution,”</em> for
example.</p></li>
<li><p><strong>Incorporate a robustness component (if needed):</strong>
A practical extension is to make the prior <strong>robust against
potential conflict</strong> by mixing it with a vague component. For
example, define <span class="math inline">\(\pi_{\text{robust}}(\theta)
= (1-w)\,\pi_{\text{MAP}}(\theta) + w\, \pi_{\text{vague}}(\theta),
\qquad 0 &lt; w &lt; 1.\)</span> Here <span
class="math inline">\(\pi\_{\text{vague}}(\theta)\)</span> could be a
very flat prior (like Beta(1,1) or a wide Beta perhaps Beta(2,2) just to
keep it proper). A typical choice might be <span class="math inline">\(w
= 0.1\)</span> or 0.2, meaning a 10–20% weight on a non-informative
prior and 80–90% weight on the MAP prior. The idea is that if the new
trial’s data <strong>strongly conflict</strong> with the MAP prior, the
vague part ensures the prior has heavier tails and won’t overly pull the
posterior. This is a safeguard; it slightly sacrifices information to
gain robustness. Whether to do this depends on how confident you are in
the applicability of the historical data. Many practitioners include a
small robust component by default, since the cost (a minor increase in
needed sample size) is usually worth the protection against prior-data
conflict.</p></li>
<li><p><strong>Use the MAP prior in trial planning:</strong> With <span
class="math inline">\(\pi\_{\text{MAP}}(\theta)\)</span> determined, you
can now do things like: simulate the operating characteristics of the
planned trial (e.g., probability of success given certain true rate,
since you have an informative prior), or calculate the <strong>effective
sample size (ESS)</strong> of the prior. ESS is a concept that
translates the information content of the prior into an equivalent
number of patients worth of data. For a Beta prior, ESS = <span
class="math inline">\(\alpha + \beta\)</span> (for mixture, it’s more
complicated but there are methods to compute it). Knowing ESS helps
communicate how much historical info we’re using. If ESS is very high
relative to new trial N, regulators might be wary; if ESS is modest, it
seems more reasonable. You can adjust the prior (e.g., adding robustness
or broadening it) to get an ESS that you feel is appropriate (some teams
target an ESS equal to a fraction of the new trial’s N, to not let prior
dominate completely).</p></li>
</ol>
<p>By following these steps, the MAP approach yields a <strong>fully
Bayesian prior</strong> for the new study’s event rate, rooted in a
rigorous meta-analytic model of the historical data.</p>
<hr />
<p><strong>Strengths:</strong></p>
<ul>
<li><em>Accounts for heterogeneity explicitly:</em> The MAP approach
doesn’t just test for heterogeneity; it <strong>quantifies</strong> it.
If between-study variability is large, the MAP prior naturally becomes
wide (less informative), whereas if variability is low, the prior is
tighter. This is a continuous adjustment governed by the data, rather
than a binary decision. It thus uses partial information from somewhat
inconsistent studies, instead of all-or-nothing. As one publication put
it, <em>“The MAP approach accounts [for] a hierarchical model for
between-trial heterogeneity in order to derive an informative prior from
historical data.”</em>. This leads to more principled borrowing of
information.</li>
<li><em>Greater <strong>borrowing power</strong></em> (when warranted):
If the historical data are consistent, MAP will yield a very informative
prior, often more so than a cautious chi-square pooling might. This can
substantially reduce the required sample size of the new trial or
increase its power. Conversely, if data conflict, MAP down-weights them.
In other words, MAP <strong>optimally uses the information</strong> in
historical studies according to their signal-to-noise ratio and
consistency.</li>
<li><em>Predictive interpretation:</em> The prior you get is literally a
predictive distribution for a new study’s parameter, which is a concept
clinicians and decision-makers can find intuitive. For instance, one can
say “Based on prior trials, before seeing new data we believe the new
treatment’s success rate is most likely around 30%, but could reasonably
be as low as 15% or as high as 45% (95% interval)”. This is a
probabilistic statement about the new context that accounts for what
happened before and the uncertainty in generalizing it.</li>
<li><em>Cohesive Bayesian updating:</em> MAP is one coherent Bayesian
model. If one wanted, one could actually incorporate the new trial data
into the same hierarchical model and get the combined posterior (this is
sometimes called “Meta-Analytic-Combined”, which yields the same result
as using the MAP prior then updating with new data). This coherence is
comforting – no disconnect between how prior was derived and how it’s
used.</li>
<li><em>Flexibility and extensions:</em> The hierarchical framework can
be extended in many ways: you can include <strong>covariates</strong>
(e.g., maybe historical studies have slight differences you can adjust
for), or even use <em>power priors</em> or <em>commensurate priors</em>
(variations on hierarchical modeling that allow discounting data). MAP
itself can be “robustified” by mixing with a vague component, as
discussed. These frameworks provide a lot of flexibility to tailor how
historical data is used.</li>
<li><em>Objective tuning of prior strength:</em> Because the model is
probability-based, one can quantify the effective weight of the
historical data. For example, the effective sample size (ESS) of a MAP
prior is often smaller than the sum of actual patients from historical
trials – reflecting discounting due to heterogeneity. This can be
computed and reported. If someone feels the prior is too tight, one can
adjust (for example, add 10% vague component or increase the prior on
<span class="math inline">\(\tau\)</span> to allow more heterogeneity)
and <strong>see the effect on ESS</strong>. Thus, it provides a more
objective way to tune how much information is borrowed, rather than
subjective choices in the chi-square approach.</li>
<li><em>Improved inference and decision-making:</em> Generally, using a
MAP prior leads to better-calibrated Bayesian inferences. For instance,
credible intervals for the new trial’s treatment effect will properly
reflect the uncertainty from historical variation. If historical data
were over-optimistic or not directly applicable, the MAP’s heterogeneity
parameter prevents the new analysis from being falsely confident. This
can be critical in decision-making (e.g., “go/no-go” decisions in drug
development) where incorporating historical data incautiously could
mislead; MAP reduces that risk by incorporating an “uncertainty tax”
when needed.</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li><em>Complexity:</em> The MAP approach is more complex to implement
and explain. It generally requires Bayesian expertise and computational
tools for MCMC or specialized algorithms. Stakeholders not familiar with
Bayesian hierarchical models might find it a “black box.” Explaining the
prior derivation might involve describing hyperparameters and MCMC,
which can be challenging for non-statisticians. That said, summarizing
the end result in terms of ESS and a mixture-of-Betas can help bridge
this communication gap.</li>
<li><em>Computational demand:</em> Although modern computers make this
manageable, fitting a hierarchical model via MCMC can take time
(especially if the data are large or the model is complex). One must
also perform diagnostic checks (ensure convergence, etc.). In contrast,
the chi-square method can be done in seconds analytically. However, with
packages like RBesT, the workflow of MAP is becoming more
automated.</li>
<li><em>Sensitivity to model assumptions:</em> The results can depend on
the chosen model for heterogeneity and the priors on hyperparameters.
For example, using a logistic-normal vs. a Beta distribution for <span
class="math inline">\(\theta\_i\)</span> could yield slightly different
priors, especially with sparse data. Similarly, an informative
hyperprior could unintentionally sway the result. One needs to do
sensitivity analyses (e.g., try different reasonable priors on <span
class="math inline">\(\tau\)</span>) to ensure conclusions aren’t an
artifact of modeling choices. The Chi-square approach, while less
sophisticated, at least doesn’t introduce many modeling assumptions
beyond “either common or different” – with MAP, you assume a particular
form of distribution of true rates. If the true heterogeneity
distribution is very different (say, bimodal when you assumed unimodal
normal), the model might not fit perfectly or might overestimate <span
class="math inline">\(\tau\)</span> to accommodate it.</li>
<li><em>Potential overconfidence if model underestimates
heterogeneity:</em> This is the flip side of strength – if the model
mistakenly assumes less heterogeneity than truly exists (perhaps due to
coincidentally similar historical results or a prior on <span
class="math inline">\(\tau\)</span> that’s too restrictive), the MAP
prior could be too narrow. One must be careful to allow for sufficient
heterogeneity in the model. It’s wise to use fairly non-informative
priors on heterogeneity and to consider model fit. If there’s doubt,
adding the robust component (mixture with vague prior) is a good
safeguard.</li>
<li><em>Data selection impact:</em> MAP will faithfully reflect the data
it’s given. If you include a historical study that is irrelevant or very
different, the MAP prior might become overly broad (since heterogeneity
blows up) or even bi-modal, which could complicate the interpretation
and use of the prior. Thus, the onus is on the analyst to carefully vet
historical data. In practice this is a necessary step for any method,
but the chi-square approach might have simply rejected a very
heterogenous study (not pooling it), whereas MAP might include it and
just increase <span class="math inline">\(\tau\)</span>. Sometimes
excluding outlying data could give a more useful prior; but one has to
justify that scientifically.</li>
<li><em>Regulatory acceptance:</em> While Bayesian methods are
increasingly accepted, a regulatory agency might scrutinize an
informative prior, especially one derived from a complex model. The
trial team must thoroughly justify the prior. The MAP approach’s
complexity means the documentation needs to include a clear explanation
of the model, diagnostics, and how robust the conclusions are. However,
it’s worth noting that several regulatory submissions (especially in
single-arm trials borrowing control rates, or rare disease trials) have
successfully used MAP priors, often with FDA or EMA consulting on the
approach. It’s no longer an exotic approach, but it does require
transparency and sometimes additional simulations to convince others
it’s reliable.</li>
</ul>
<p>In summary, MAP’s limitations are mostly about <strong>practical
implementation and the need for care in modeling</strong>, whereas its
strengths lie in <strong>rigor and adaptability</strong>. It is a
powerful approach that, when used properly, can extract maximum
information from historical data while appropriately reflecting
uncertainty.</p>
<hr />
<p><strong>When to choose MAP approach:</strong> If you have
<strong>multiple historical studies</strong> (more than just a couple)
or even a few that show <strong>meaningful heterogeneity</strong>, the
MAP approach is usually recommended. It will allow you to use all the
information without overconfidence. For example, if you have 5 prior
studies with varying results, a MAP prior will include all five in a
balanced way. A chi-square approach might struggle: either it pools and
underestimates variance or it gives up on pooling and maybe throws away
some info. MAP is also preferable if the consequence of prior
mis-specification is serious – say you’re reducing a control group in a
trial based on historical data; you want a robust yet information-rich
prior, which MAP provides via the hierarchical variance. Additionally,
if you anticipate <strong>scrutiny or need a thorough analysis</strong>,
MAP’s formal framework can be reassuring, since you can demonstrate via
Bayesian analysis how the prior was derived and even update it
continuously as new data come (e.g., in an ongoing platform trial that
adds new cohorts, MAP can sequentially update the prior for each new
arm).</p>
<p><strong>When to stick with Chi-square approach:</strong> If the
historical data is <strong>very limited or uniform</strong> and the
context is straightforward, the added complexity of MAP might not be
worth it. For instance, if you only have one prior study that is
directly applicable (say, a pilot study with 100 patients giving an
event rate of 40%), one could simply use a Beta prior based on that
(with perhaps a slight wider variance to be safe) rather than a full MAP
analysis (which in this case would just yield essentially the same Beta
since with one study heterogeneity is unidentifiable). Similarly, if two
or three studies all show ~15% event rate with no hints of differences,
a quick pooling will be fine. In early-phase trials or when the prior is
not going to dramatically affect the decision, simplicity might be
preferred. Also, if your team lacks Bayesian modeling experience, the
chi-square method can be a reasonable starting point – one can always
later refine to MAP if needed.</p>
<p><strong>Data heterogeneity as a deciding factor:</strong> A
rule-of-thumb: if <strong><span class="math inline">\(I^2\)</span> is
moderate to high (say &gt;25%) or <span class="math inline">\(Q\)</span>
test <span class="math inline">\(p&lt;0.1\)</span></strong>, lean
towards MAP. If <span class="math inline">\(I^2 \approx 0\)</span> and
studies are essentially identical in results, chi-square pooling is
acceptable. MAP will <strong>never perform worse</strong> in terms of
validity – it will just revert to an approximately pooled result if
heterogeneity is low. The only “cost” is effort. On the other hand, if
heterogeneity is present, the chi-square approach could either
over-shrink (if one pools anyway) or waste information (if one decides
not to pool). MAP finds the middle ground by partially pooling.</p>
<p><strong>Number of studies and size considerations:</strong> With only
1 historical study, MAP is basically just using that study’s likelihood
as prior (with perhaps a tiny bit of extra variance if you put a prior
on heterogeneity). With 2 studies, MAP can handle it but the
heterogeneity estimate will be very uncertain (still, a Bayesian will
integrate that uncertainty). With <span
class="math inline">\(\ge3\)</span> studies, MAP can start to truly
estimate heterogeneity. The more studies, the more one should use MAP
because it can utilize subtle patterns (maybe there’s mild heterogeneity
– MAP will catch that; chi-square might not). If each study is very
small or event counts are very low, the chi-square test might be useless
(low power) but MAP can still combine evidence, borrowing strength
through the hierarchical structure (this is common in rare events or
rare diseases – MAP is used to combine a bunch of tiny studies or case
series).</p>
<p><strong>Regulatory and clinical context:</strong> If you plan to
present the results to a broad audience, consider how they view
evidence:</p>
<ul>
<li><strong>Chi-square approach</strong> might be easier for a clinician
to grasp: “We pooled historical data to get a prior of Beta(51,151)” can
be explained as “prior ~ 25% event rate with ~200 patient worth of
confidence.”</li>
<li><strong>MAP approach</strong> might impress a statistically savvy
regulator who knows you did a full analysis, but a clinician might need
the results translated (“our prior is equivalent to about 150 patients
worth of data, centered at 25%, because past studies varied a bit”). In
critical trials (e.g., registration trials using external controls),
regulators will likely expect a MAP or similar rigorous approach – FDA’s
guidance on Bayesian trials (2010) explicitly mentions using
hierarchical models for borrowing to avoid overconfidence.</li>
</ul>
<p>In practice, one might use <strong>both</strong> methods in a
complementary way: perform a quick chi-square pooling as an initial
sanity check and to have an easy reference point, but use MAP for the
final prior. The chi-square pooling could inform your hyperprior choices
for MAP (e.g., it gives you an idea of the range of rates and whether
heterogeneity exists). It’s also useful to report both: <em>“The crude
pooled rate is X% (95% CI [A, B]) and a Bayesian random-effects MAP
analysis yields a predictive prior ~ Beta(a,b).”</em> If they align,
great. If not, you can explain why (usually heterogeneity).</p>
</div>
</div>
<div
id="poisson-gamma-model---schnell-2016-bayesian-exposure-time-method"
class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Poisson-Gamma Model -
Schnell (2016) Bayesian Exposure-Time Method</h2>
<hr />
<p><strong>Model and Approach:</strong></p>
<ul>
<li><p>The Schnell (2016) method is designed specifically for blinded
safety monitoring using <strong>Poisson-Gamma models</strong> to account
for adverse event counts. The approach is rooted in the concept of
<strong>exposure-time</strong>, where adverse event counts are modeled
in proportion to patient exposure time.</p></li>
<li><p>Each patient is assumed to contribute a count of adverse events
over their follow-up period, and this count is modeled as a
<strong>Poisson random variable</strong>, with the rate of events
dependent on the patient’s exposure time:</p>
<p><span class="math display">\[
Y_i \sim \text{Poisson}(\lambda_{a_i} \times t_i)
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(Y_i\)</span> is the count of adverse
events for patient <span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(t_i\)</span> is the exposure time for
patient <span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(\lambda_{a_i}\)</span> is the event rate
for the arm the patient is in (treatment or control).</li>
</ul></li>
<li><p>The method uses <strong>Gamma priors</strong> for the event rates
(<span class="math inline">\(\lambda_0\)</span> for control and <span
class="math inline">\(\lambda_1\)</span> for treatment):</p>
<p><span class="math display">\[
\lambda_0 \sim \text{Gamma}(\alpha_0, \beta_0), \quad \lambda_1 \sim
\text{Gamma}(\alpha_1, \beta_1)
\]</span></p></li>
<li><p>These priors are chosen based on historical data (for control) or
clinical judgment (for treatment).</p></li>
<li><p>The Bayesian framework then updates these priors using the
blinded data to produce posterior distributions for the rates:</p>
<p><span class="math display">\[
p(\lambda_0, \lambda_1 \mid \text{data}) \propto (\lambda_0 T_0 +
\lambda_1 T_1)^y \, e^{-(\lambda_0 T_0 + \lambda_1 T_1)}
\lambda_0^{\alpha_0-1} \lambda_1^{\alpha_1-1}
\]</span></p>
<p>where <span class="math inline">\(T_0\)</span> and <span
class="math inline">\(T_1\)</span> are the expected exposure times for
the control and treatment groups.</p></li>
</ul>
<p><strong>Trigger and Safety Decision:</strong></p>
<ul>
<li>A posterior probability is computed to determine the probability
that the treatment event rate <span
class="math inline">\(\lambda_1\)</span> exceeds a pre-defined critical
rate (e.g., <span class="math inline">\(\lambda_1 &gt; 1.2 \times
\lambda_0\)</span>). If this probability exceeds a threshold (e.g.,
62%), a safety signal is raised, and further investigation is
recommended.</li>
</ul>
<hr />
<p>In modern clinical trials, sponsors must monitor patient safety
continuously without compromising the blind. Regulatory guidance (e.g.,
the FDA’s “Final Rule” on safety reporting) mandates rapid reporting if
an aggregate analysis shows that an adverse event is occurring more
often in the drug arm than in control. However, sponsors remain blinded
to treatment assignments to preserve trial integrity, which makes direct
safety comparisons challenging. The <strong>Bayesian exposure-time
method</strong> is a statistical framework that addresses this challenge
by using <strong>blinded data</strong> (pooled across arms) together
with prior knowledge of expected event rates. This method enables
continuous safety monitoring and can trigger alerts when there is
evidence of unacceptable risk in the treatment group – all
<strong>without unblinding the trial</strong>.</p>
<p><strong>How the Method Works (Plain-Language Overview):</strong> This
approach treats adverse events as outcomes of a Poisson process that
depends on how long patients are exposed to treatment (their “exposure
time”). It uses a Bayesian model that combines <strong>prior
information</strong> (from past trials or epidemiological data) with the
<strong>current blinded event data</strong> to infer the likely adverse
event rate in the treatment arm. In simple terms, we start with what we
already know about how often the adverse event of interest typically
occurs (especially in patients not receiving the new drug), then update
that knowledge as events accumulate in the ongoing trial. Crucially, we
assume we know the total number of events and how many patients (or
person-time) have been treated, but <strong>not</strong> which
individual events came from which arm (since the data are blinded). By
leveraging the known <strong>randomization ratio</strong> (e.g., 1:1 or
2:1) and a solid prior on the <strong>control-arm event rate</strong>,
the method can statistically deduce how high the treatment-arm rate
would have to be to explain the observed total events. If the combined
data show more events than expected under safe conditions, the model
will shift its belief toward a higher event rate in the treatment
arm.</p>
<p>A key component is the use of <strong>Bayesian priors</strong>.
Before seeing current trial data, experts define priors for the event
rates in the control arm and treatment arm. Typically, the control-arm
prior is <strong>informative (strong)</strong>, grounded in historical
evidence (for example, prior trials or published rates in similar
populations). The treatment-arm prior may be <strong>weaker</strong>,
reflecting greater uncertainty — perhaps using any available Phase 2
data or simply a conservative guess that the treatment’s rate is similar
to control unless proven otherwise. The stronger the prior knowledge on
the control rate, the easier it is to detect an anomaly in the blinded
data: since we “know” what to expect from control, any excess in total
events is more likely attributed to the drug. Conversely, if we had
little idea about the control rate, distinguishing a true drug effect
from natural variation would be harder. Proper prior specification is
therefore crucial, and the method involves a <strong>collaborative
elicitation process</strong> where clinicians, safety experts, and
statisticians translate medical knowledge into the parameters of these
prior distributions.</p>
<p>Once the priors are set, the trial data are monitored as they come
in. The model is updated with each new adverse event, producing a
<strong>posterior distribution</strong> for the treatment’s event rate.
This posterior reflects our updated belief about how frequent the
adverse event is in the treatment group, given both the prior and the
observed data. From this posterior, the safety team can compute the
probability that the treatment’s true event rate exceeds some
pre-defined <strong>“critical” rate</strong> (the maximum acceptable
rate). For example, the team might decide that a 20% higher event rate
than control would be clinically unacceptable. The Bayesian method can
continually answer the question: “Given all the data so far, what is the
probability that the treatment’s event rate is above that unacceptable
threshold?” If that probability rises above a certain cutoff (call it
<em>p</em>), the method flags a <strong>safety signal</strong>.</p>
<p><strong>Safety Alerts and Decision Threshold:</strong> The choice of
the alert threshold <em>p</em> is a balance between sensitivity and
false alarms. It is usually determined by simulation before the trial
starts. The goal is to calibrate <em>p</em> so that if the drug truly
has a problem (event rate above the threshold), the method will alert
with high probability, but if the drug is actually safe (event rate at
or below acceptable), the chance of a false alert is controlled. For
instance, Schnell and Ball (2016) describe choosing <em>p</em> such that
there would be at most a 50% chance of an alert if the treatment’s true
event rate equals the maximum acceptable rate. In their case study, this
calibration led to <em>p ≈ 62%</em> (i.e., an alert triggers if there is
≥62% posterior probability that the treatment rate is too high). The
method’s <strong>operating characteristics</strong> – like the
probability of detecting a safety issue under various true rates, and
the false alarm rate – are evaluated through these simulations to ensure
the monitoring plan is well-tuned. Importantly, when the algorithm
signals an alert, it doesn’t automatically stop the trial; rather, it
<strong>alerts the Safety Management Team (SMT)</strong>, who would then
use medical judgment and possibly involve the Data Monitoring Committee
(DMC) to review unblinded data if needed. The Bayesian safety monitoring
tool is thus a <strong>trigger for action</strong>, not a standalone
decision rule.</p>
<p><strong>Implementing the Method (Expert Input and
Process):</strong></p>
<ul>
<li><strong>Identify Adverse Events of Special Interest (AESI):</strong>
Select which specific adverse event types will be monitored closely for
safety signals. These are typically events that are suspected based on
the drug’s class or previous studies (for instance, liver toxicity or
cardiac events known to occur with similar drugs).</li>
<li><strong>Establish Plausible Rates and a Critical Threshold:</strong>
Using available data and expert opinion, determine the range of
plausible incidence rates for these events in both the control and
treatment arms, and decide on the <strong>maximum acceptable
rate</strong> for the treatment arm (the “critical” rate that would
raise concern). For example, if historical placebo data suggest a 1% per
month serious adverse event rate, the team might consider anything above
1.2% per month on the drug as unacceptable.</li>
<li><strong>Translate into Model Priors and Calibrate:</strong> Convert
this information into the parameters of the Bayesian model. In practice,
this means setting the hyperparameters of the Gamma priors for the
control and treatment event rates such that they reflect the believed
rates and uncertainties. The control arm’s prior will often be based on
a large existing dataset (giving a Gamma distribution with a
correspondingly large “prior sample size”), whereas the treatment’s
prior might incorporate any smaller-scale data (like a Phase 2 trial) or
be more diffuse. The team then runs <strong>simulation studies</strong>
using these priors: many hypothetical trials are simulated under various
true rate scenarios to see how often the method would signal an alert.
This is where the alert threshold <em>p</em> is fine-tuned to achieve
desired operating characteristics (for instance, controlling false
alerts to an acceptable level while maximizing power to detect a
problem).</li>
<li><strong>Deploy the Monitoring Procedure:</strong> With priors and
threshold set, the team prepares a <strong>“safety signal
calculator”</strong> – essentially an implementation (often a
statistical script or software tool) that takes the ongoing blinded data
as input and outputs the posterior probability and alert status at any
given time. According to the safety monitoring plan, the SMT will use
this tool at regular intervals (or even continuously) to evaluate the
accumulating data. All parties remain blinded during this process,
except the DMC who would only get involved to review unblinded data if
an alert is triggered.</li>
</ul>
<p><strong>Mathematical details</strong> of the Bayesian exposure-time
model</p>
<p><strong>Statistical Model (Poisson-Gamma Framework):</strong> The
method models adverse event occurrences using a Poisson process, which
is appropriate for counts of events over time under the assumption that
events happen independently and at a roughly constant rate. Suppose we
have a trial with two arms (control and treatment) and a total of
<em>N</em> patients. Let each patient <em>i</em> have an observed
follow-up time <span class="math inline">\(t\_i\)</span> (their exposure
time in the study). Because the trial is blinded, we do not know each
patient’s treatment assignment, but we do know the randomization
probability. Let’s denote by <span class="math inline">\(a\_i\)</span> a
binary indicator of patient <em>i</em>’s arm: <span
class="math inline">\(a\_i = 0\)</span> for control, <span
class="math inline">\(a\_i = 1\)</span> for treatment. We assume <span
class="math inline">\(a\_i \sim \text{Bernoulli}(r)\)</span>
independently, where <em>r</em> is the known probability of being
assigned to the treatment (for example, <span
class="math inline">\(r=0.5\)</span> in a 1:1 randomization).</p>
<p>Under these assumptions, the <strong>count of AESI events for patient
<em>i</em></strong>, denoted <span class="math inline">\(Y\_i\)</span>,
is modeled as:</p>
<ul>
<li><span class="math inline">\(Y\_i \mid (a\_i = 0, \lambda\_0) \sim
\text{Poisson}(\lambda\_0 , t\_i)\)</span> for a control patient,</li>
<li><span class="math inline">\(Y\_i \mid (a\_i = 1, \lambda\_1) \sim
\text{Poisson}(\lambda\_1 , t\_i)\)</span> for a treated patient.</li>
</ul>
<p>Here <span class="math inline">\(\lambda\_0\)</span> is the true
adverse event rate (hazard) in the control arm, and <span
class="math inline">\(\lambda\_1\)</span> is the event rate in the
treatment arm. The model <span class="math inline">\(Y\_i \sim
\text{Poisson}(\lambda\_{a\_i} t\_i)\)</span> encapsulates the
“exposure-time” concept: the expected number of events for a patient is
proportional to how long they are observed, with the proportionality
constant being the rate <span class="math inline">\(\lambda\)</span> for
the arm they’re in. If a patient has zero follow-up (<span
class="math inline">\(t\_i=0\)</span>), they contribute no risk, while
longer follow-up increases the expected count of events linearly.</p>
<p>The <strong>prior distributions</strong> for the rates are chosen to
be Gamma distributions, which are conjugate to the Poisson likelihood.
We write:</p>
<ul>
<li><span class="math inline">\(\lambda\_0 \sim \text{Gamma}(\alpha\_0,;
\beta\_0)\)</span> for the control rate,</li>
<li><span class="math inline">\(\lambda\_1 \sim \text{Gamma}(\alpha\_1,;
\beta\_1)\)</span> for the treatment rate.</li>
</ul>
<p>(<em>Note:</em> Here we use a parameterization where <span
class="math inline">\(\alpha\)</span> is the shape parameter and <span
class="math inline">\(\beta\)</span> is the <strong>exposure</strong> or
scale parameter such that the mean of <span
class="math inline">\(\lambda\)</span> is <span
class="math inline">\(\alpha/\beta\)</span>. Schnell &amp; Ball
interpret <span class="math inline">\(\alpha\)</span> as a notional
“number of prior events” and <span class="math inline">\(\beta\)</span>
as the “total time at risk” associated with those events. For example, a
Gamma(339, 26064) prior for <span
class="math inline">\(\lambda\_0\)</span> means roughly the prior
expectation is 339 events per 26064 person-months, i.e. an expected rate
of 0.013 per person-month.)</p>
<p>With this setup, the <strong>likelihood</strong> of the observed
(blinded) data can be described as follows. If we had full information,
the total number of events in the control arm <span
class="math inline">\(Y\_{\text{control}} = \sum\_{i: a\_i=0}
Y\_i\)</span> would be Poisson<span class="math inline">\((\lambda\_0
T\_0)\)</span> and in the treatment arm <span
class="math inline">\(Y\_{\text{treat}} = \sum\_{i: a\_i=1}
Y\_i\)</span> would be Poisson<span class="math inline">\((\lambda\_1
T\_1)\)</span>, where <span class="math inline">\(T\_0 = \sum\_{i:
a\_i=0} t\_i\)</span> and <span class="math inline">\(T\_1 = \sum\_{i:
a\_i=1} t\_i\)</span> are the total exposure times in each arm. Because
<span class="math inline">\(a\_i\)</span> are i.i.d. Bernoulli, the
expected proportion of patients (and total person-time) in treatment is
<span class="math inline">\(r\)</span> and in control is <span
class="math inline">\(1-r\)</span>, but the actual <span
class="math inline">\(T\_0, T\_1\)</span> are random. In practice, by
the time of an interim analysis, we know how many patients have been
enrolled and their follow-up times, so <span
class="math inline">\(T\_0\)</span> and <span
class="math inline">\(T\_1\)</span> can be approximated (for instance,
<span class="math inline">\(T\_1 \approx r \sum\_{i=1}^N t\_i\)</span>
and <span class="math inline">\(T\_0 \approx (1-r)\sum t\_i\)</span> if
dropouts are minimal). The key <strong>observations</strong> we actually
have from blinded data are the individual <span
class="math inline">\(Y\_i\)</span> (or just the total count <span
class="math inline">\(Y\_{\text{total}} = \sum\_{i=1}^N Y\_i\)</span>)
and the <span class="math inline">\(t\_i\)</span> for each patient, but
not the <span class="math inline">\(a\_i\)</span>. For modeling
convenience, one can aggregate the data as “total events = <span
class="math inline">\(y\)</span> in total exposure <span
class="math inline">\(T\_{\text{total}}\)</span>, with an unknown split
between arms.”</p>
<p>Given the model above, the <strong>joint posterior
distribution</strong> of <span class="math inline">\((\lambda\_0,
\lambda\_1)\)</span> given the observed data can be derived (up to a
normalizing constant) by combining the likelihood of all <span
class="math inline">\(Y\_i\)</span> with the priors. Formally, using the
fact that the sum of Poissons is Poisson, the probability of observing a
particular configuration of events can be expressed in a couple of ways.
One intuitive formulation is to consider the latent allocation of events
to arms. Each observed event could have come from either a control
patient or a treated patient. If we denote by <span
class="math inline">\(Y\_{\text{treat}}\)</span> the (unobserved) number
of events in the treatment arm, then <span
class="math inline">\(Y\_{\text{control}} = Y\_{\text{total}} -
Y\_{\text{treat}}\)</span> is the number in control. The
<strong>full-data likelihood</strong> (if we knew the split) would
be:</p>
<p><span class="math inline">\(P(Y_{\text{control}} = m,\;
Y_{\text{treat}} = n \mid \lambda_0,\lambda_1) = \frac{e^{-\lambda_0
T_0}(\lambda_0 T_0)^m}{m!}\;\frac{e^{-\lambda_1 T_1}(\lambda_1
T_1)^n}{n!},\)</span></p>
<p>with <span class="math inline">\(m+n = y\)</span> (the total events).
Because we are blinded, we only know <span
class="math inline">\(y\)</span>. The <strong>marginal
likelihood</strong> for the total <span class="math inline">\(y\)</span>
(summing over all splits <span class="math inline">\(m,n\)</span> that
sum to <span class="math inline">\(y\)</span>) is:</p>
<p><span class="math inline">\(P(Y_{\text{total}} = y \mid
\lambda_0,\lambda_1) \;=\; \frac{e^{-(\lambda_0 T_0 + \lambda_1 T_1)} \,
(\lambda_0 T_0 + \lambda_1 T_1)^y}{y!},\)</span></p>
<p>since the sum of independent Poisson random variables is Poisson with
mean <span class="math inline">\(\lambda\_0 T\_0 + \lambda\_1
T\_1\)</span>. This coupling term <span
class="math inline">\((\lambda\_0 T\_0 + \lambda\_1 T\_1)^y\)</span> in
the likelihood is what makes the posterior of <span
class="math inline">\(\lambda\_0\)</span> and <span
class="math inline">\(\lambda\_1\)</span> <strong>jointly
dependent</strong> – we cannot factor the posterior into separate parts
for <span class="math inline">\(\lambda\_0\)</span> and <span
class="math inline">\(\lambda\_1\)</span> because the data only inform
the sum <span class="math inline">\(\lambda\_0 T\_0 + \lambda\_1
T\_1\)</span>. The priors, however, are independent. Thus, the
<strong>unnormalized joint posterior</strong> is:</p>
<p><span class="math display">\[
p(\lambda_0,\lambda_1 \mid \text{data}) \;\propto\; (\lambda_0 T_0 +
\lambda_1 T1)^y \, e^{-(\lambda_0 T_0 + \lambda_1 T_1)} \;\times\;
\lambda_0^{\alpha_0-1} e^{-\beta_0 \lambda_0} \; \times\;
\lambda_1^{\alpha_1-1} e^{-\beta_1 \lambda_1}\,.
\]</span></p>
<p>This posterior does not have a simple closed-form solution for, say,
the marginal distribution of <span
class="math inline">\(\lambda\_1\)</span>. Nevertheless, we can
<strong>compute or approximate</strong> anything we need from it via
Bayesian computation methods. One approach is to sample from the joint
posterior using <strong>Gibbs sampling</strong>, leveraging conditional
conjugacy. Gibbs sampling breaks a complex joint sampling problem into
easier conditional sampling steps.</p>
<p>Intuitively, we update each arm’s rate with the events allocated to
that arm, adding the “prior counts” and “prior exposure” to the observed
counts and exposure. For example, if the control prior was equivalent to
339 events in 26064 months and we (hypothetically) allocate 10 new
events to control over 1000 months of control exposure, the new
posterior for <span class="math inline">\(\lambda\_0\)</span> would be
Gamma(339+10, 26064+1000).</p>
<p>By iterating these two steps (sampling a split given the current
<span class="math inline">\(\lambda\)</span>’s, then sampling new <span
class="math inline">\(\lambda\)</span>’s given that split), the Gibbs
sampler generates a sequence of <span class="math inline">\((\lambda\_0,
\lambda\_1)\)</span> draws that converge to samples from the true joint
posterior. From these samples, one can directly estimate the
<strong>posterior probability</strong> that <span
class="math inline">\(\lambda\_1\)</span> exceeds the critical
threshold. For instance, simply compute the fraction of sampled <span
class="math inline">\(\lambda\_1\)</span> values that are greater than
the pre-specified critical rate. If that fraction is above the chosen
cutoff <em>p</em>, an alert is signaled.</p>
<p><strong>Prior Elicitation Details:</strong> A crucial practical
aspect is how <span class="math inline">\(\alpha\_0,\beta\_0\)</span>
and <span class="math inline">\(\alpha\_1,\beta\_1\)</span> are chosen
to encode expert knowledge.</p>
<ul>
<li>As mentioned, one convenient way is to base them on actual data from
previous studies by treating those data as if they were a prior
“experiment.” For example, if a meta-analysis of placebo groups
(historic control data) observed 339 events over 26064 person-months,
one can set the control prior <span class="math inline">\(\lambda\_0
\sim \text{Gamma}(339,; 26064)\)</span> so that its mean equals <span
class="math inline">\(339/26064 \approx 0.013\)</span> events per month,
and it reflects the weight of <em>339 events worth of
information</em>.</li>
<li>Another common approach to elicitation is via
<strong>quantiles</strong>: experts might say “I’m 50% sure the true
rate is below X and 90% sure it’s below Y.” The authors provide R
routines to convert such probability judgments into the <span
class="math inline">\((\alpha,\beta)\)</span> of a Gamma distribution.
This flexibility allows the SMT to incorporate not just hard data but
also subjective expert belief into the model in a principled way.</li>
</ul>
</div>
<div id="poisson-gamma-model---waterhouse-2020-bdribs-method"
class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Poisson-Gamma Model -
Waterhouse (2020) BDRIBS Method</h2>
<hr />
<p><strong>Model and Approach:</strong></p>
<ul>
<li><p>The Waterhouse (2020) BDRIBS method is designed for more general
application in blinded safety monitoring, focusing on <strong>relative
risk (RR)</strong> as the main metric:</p>
<p><span class="math display">\[
RR = \frac{\lambda_1}{\lambda_0}
\]</span></p>
<p>This means the method directly models the <strong>relative risk of
adverse events in the treatment group versus control</strong>, rather
than modeling the absolute event rates.</p></li>
<li><p>The model starts with a <strong>fixed or Gamma-distributed
prior</strong> for the background (control) event rate (<span
class="math inline">\(\lambda_0\)</span>), which is derived from
historical data:</p>
<p><span class="math display">\[
\lambda_0 \sim \text{Gamma}(\alpha_0, \beta_0)
\]</span></p></li>
<li><p>The probability that an observed adverse event is from the
treatment group is modeled using a <strong>Beta prior</strong>:</p>
<p><span class="math display">\[
p \sim \text{Beta}(\alpha_p, \beta_p)
\]</span></p></li>
<li><p>The relative risk <span class="math inline">\(RR\)</span> is
derived using these priors and is updated using Bayesian inference to
produce a posterior distribution for <span
class="math inline">\(RR\)</span>.</p></li>
<li><p>The method is implemented using <strong>Markov Chain Monte Carlo
(MCMC)</strong>, typically via Gibbs sampling, to generate posterior
samples of <span class="math inline">\(RR\)</span>.</p></li>
</ul>
<p><strong>Trigger and Safety Decision:</strong></p>
<ul>
<li>The BDRIBS method directly monitors the <strong>posterior
probability that <span class="math inline">\(RR\)</span> exceeds a
critical threshold</strong> (e.g., <span class="math inline">\(RR &gt;
1.5\)</span>).</li>
<li>This probability is calculated based on the posterior distribution,
and if it exceeds a pre-defined cutoff (e.g., 80%), a safety signal is
raised, prompting an unblinded review.</li>
</ul>
<hr />
<p>The BDRIBS method models adverse event occurrences in a blinded
clinical trial using a <strong>Poisson process</strong> framework. It
assumes that for a given safety event (or category of events), the
incidence rate is <strong>constant over time</strong> and sufficiently
low (a rare event assumption), with a large amount of patient-time at
risk. Under these conditions, the <strong>number of events in each
treatment group</strong> (investigational drug vs. control) can be
regarded as arising from independent Poisson processes with constant
rates. In the absence of any treatment effect, both groups share a
common <strong>baseline event rate</strong> (often denoted λ_0). If the
investigational drug is associated with increased risk, its event rate
is higher by some factor. BDRIBS quantifies this difference via the
<strong>relative risk (RR)</strong> parameter (denoted <em>r</em>),
defined as the ratio of the event rate on investigational drug to the
event rate on control. The model considers one specific event (or
aggregated event category) at a time, and it requires that an estimate
of the <strong>historical background event rate</strong> for the control
population is available (e.g. from epidemiological data or previous
trials). Key assumptions include:</p>
<ul>
<li><ol style="list-style-type: decimal">
<li>event occurrences are independent and Poisson-distributed;</li>
</ol></li>
<li><ol start="2" style="list-style-type: decimal">
<li>event rates remain constant throughout the observation period (no
time trend); and</li>
</ol></li>
<li><ol start="3" style="list-style-type: decimal">
<li>the trial’s blinded aggregate data can be treated as a mixture of
two Poisson processes, given the known randomization ratio and total
exposure.</li>
</ol></li>
</ul>
<p>These assumptions allow the use of Poisson-based inference on blinded
data, treating any excess in overall event frequency as a potential
signal of increased risk in one of the arms.</p>
<p>Applying BDRIBS is not just a statistical exercise; it fits into a
broader <strong>safety monitoring workflow</strong> that involves
planning, data collection, analysis, and decision-making. Waterhouse
<em>et al.</em> outline a cross-functional procedure with <strong>seven
steps</strong>, from before the trial starts through to the point of
unblinding. Here is a summary of each step:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Identification of anticipated events and safety topics of
interest (STIs):</strong> Before or early in the trial, the safety team
(clinical safety scientists) identifies which adverse events will be the
focus of blinded monitoring. These are events that are
<strong>anticipated</strong> in the trial population (for example,
events common in the disease under study or the demographic group) or
specific <strong>safety topics of interest</strong> related to the drug
(for example, adverse events seen in preclinical studies or known class
effects). The events chosen here are typically ones that <em>could</em>
occur in the trial and might signal a safety problem if they occur at
high rates. By defining this set of events/STIs up front, the team
ensures that BDRIBS analyses will be performed for these categories. For
instance, in an osteoporosis trial one might flag femur fractures as an
anticipated event, and in a diabetes drug trial hypoglycemia could be an
STI due to class effects. This step is critical in focusing the
surveillance on the most relevant safety concerns.</p></li>
<li><p><strong>Determination of background reference event
rates:</strong> Once the events of interest are set,
<strong>epidemiology or clinical experts determine the expected
background incidence rates</strong> for each of those events in a
population similar to the trial. Sources for these background rates
include historical clinical trial data (e.g., placebo group rates from
prior studies), observational cohort data, literature, or disease
registries. The output of this step is an estimate like “Event X is
expected to occur at 2 cases per 100 patient-years in this population.”
These estimates will feed into the BDRIBS model as the baseline rate
(λ_0) – either directly if using a fixed rate or as the center of a
prior distribution. In cases where precise rates are unavailable, the
team may have to use approximate rates or ranges. Establishing these
reference rates is essential for BDRIBS because it anchors the Bayesian
prior for the event frequency under normal conditions.</p></li>
<li><p><strong>Choose an appropriate quantitative method and define
probability thresholds:</strong> At this planning stage, the team
selects the statistical method for aggregate safety monitoring and sets
the rules for triggering alerts. In our context, BDRIBS is chosen as the
method (other methods could be used if, for example, no reliable
background rate can be obtained – in which case one might use a simpler
counting rule or another Bayesian model). Along with choosing BDRIBS,
the team defines the <strong>unblinding criteria in terms of posterior
probabilities</strong>. This involves deciding on the threshold
probability that will prompt further action (as discussed earlier,
e.g. “if <span class="math inline">\(P(r&gt;1) &gt; 0.90\)</span>, we
will unblind”). The thresholds may vary by event severity – for
instance, for a very serious event, the team might choose a slightly
lower threshold to be extra sensitive. All these decisions are
documented in a Safety Surveillance Plan. At this step, statisticians,
safety physicians, and epidemiologists collaborate: statisticians
contribute the design of the BDRIBS analysis and simulations to pick a
threshold, safety clinicians provide input on which signals would be
worrying, and epidemiologists ensure thresholds make sense given
background rates. By the end of Step 3, the team has a clear plan: which
events to monitor, what the expected rates are, what method will be used
(BDRIBS), and what probability triggers will be applied.</p></li>
<li><p><strong>Periodic safety data assembly:</strong> Once the trial is
underway and patients are enrolling and being followed, the clinical
data team conducts <strong>periodic aggregation of the relevant safety
data</strong>. This means at regular intervals (e.g. monthly or
quarterly, or after a certain number of patients enrolled), they compile
the current blinded count of events for each monitored category, along
with information on how many patients or patient-years have accumulated.
Data management ensures that the adverse events are properly coded and
entered in the database, and that the <strong>number of patients
enrolled and total exposure time</strong> are up to date, since these
are needed for the analysis. Essentially, Step 4 is about making sure
the <strong>data are clean and ready</strong> for an interim safety
analysis. For example, the team might say “As of June 30, we have 500
patients enrolled with a total of 200 patient-years of follow-up, and 3
cases of Event X have been reported in total.” This step often involves
the clinical study team and pharmacovigilance or safety operations
personnel to verify the case counts. It’s emphasized that having an
accurate estimate of exposure and event counts is critical at this point
before running the analysis【56† lines】, because any errors in the data
will directly affect the statistical inference in BDRIBS.</p></li>
<li><p><strong>Identification of events with rates exceeding the
predesignated probability threshold:</strong> In this step, the
statistical analysis is performed on the assembled data to check for
<strong>potential signals</strong>. The statisticians (with support from
statistical programmers) run the BDRIBS model for each event of interest
using the current blinded data. For each event, the posterior
probability (e.g. <span class="math inline">\(P(r&gt;1)\)</span> or
<span class="math inline">\(P(r&gt;c)\)</span>) is computed. The results
are then compared to the <strong>pre-specified thresholds from Step
3</strong>. If none of the monitored events exceed the threshold, the
conclusion is that no signal has been detected at this time, and the
trial continues without unblinding (the team will repeat Steps 4–5 at
the next data cut). If <strong>any event’s probability exceeds the
threshold</strong>, that event is flagged as a potential safety signal.
For example, perhaps “Event X now has a 96% posterior probability of
being more frequent on drug; threshold was 95%, so this triggers an
alert.” The output of this step is essentially a list of any “triggered”
events. Waterhouse <em>et al.</em> note that such analyses can be done
periodically (e.g. monthly) or after a certain count of events has
occurred – the frequency should be agreed upfront to manage the alpha
spending (though in Bayesian analysis there isn’t a formal alpha, one
still doesn’t want to look too often and cause overreactions to noise).
The programming output might be a summary for the Safety Management Team
showing, for each event, the prior, observed count, posterior
probability of signal, and whether it crossed the threshold.</p></li>
<li><p><strong>Assessment of totality of evidence for an association
with product (investigational drug):</strong> When a threshold is
exceeded for an event, simply getting a statistical alert is not the end
of the process. The <strong>safety management team (SMT)</strong> – a
cross-functional team including clinicians, safety experts,
statisticians, etc. – convenes to perform a <strong>comprehensive review
of the data related to that event</strong>. This step goes beyond the
single-number posterior probability. The team will examine <strong>all
available information</strong> on the cases: for example, the case
narratives, patient medical histories, timing of events, any patterns
such as all events coming from one study site or one region, etc. They
will also consider biologic plausibility (is it plausible that the drug
causes this event?), whether the event could be due to other factors,
and if other safety data (perhaps unblinded from other sources or other
similar drugs) support the finding. <strong>Interactive
graphics</strong> and detailed data listings may be used to facilitate
discussion【25† (Step6 snippet)】. The SMT’s job here is to distinguish
a true safety signal from a statistical false alarm or an explicable
cluster. For instance, if BDRIBS triggered on “heart attacks,” the SMT
might look and realize all patients who had a heart attack had a
particular risk factor, or perhaps some were actually on placebo (if any
info can be gleaned without full unblinding, sometimes one might know a
particular patient’s treatment if it’s unblinded via SAE reporting
channels). The SMT also evaluates how serious the potential risk is.
This step is essentially a medical judgment phase, informed by the
quantitative signal.</p></li>
<li><p><strong>Decision to escalate to unblinded review (SAC) or
continue to monitor:</strong> Finally, the cross-functional team must
decide on an action. If the totality of evidence in Step 6 suggests that
the imbalance is likely real and concerning, the team will
<strong>escalate the issue to an unblinded safety review</strong>.
Typically, this means referring the event to the <strong>Safety
Assessment Committee (SAC)</strong> or equivalent independent body that
can break the blind for that specific event and examine the treatment
allocation of those cases. The SAC is usually a small group of experts
who are unblinded to data in a controlled manner to protect trial
integrity while assessing safety. They will determine, once unblinded,
if the cases predominantly occurred on the investigational drug
(confirming a risk). If yes, regulatory reporting may be triggered
(e.g. an IND safety report to FDA) and potentially stopping rules or
other actions might be considered. If the SMT instead judges that the
signal is weak or not compelling (for example, perhaps the posterior was
just barely over the threshold and clinical review finds confounding
factors), they might choose to <strong>continue blinded
monitoring</strong> without escalation. In that case, they document why
they did not unblind and continue to watch the event closely in future
analyses. Essentially, Step 7 is a <strong>decision point</strong>:
unblind now for a focused evaluation, or maintain blinding and
re-evaluate later. This decision takes into account the
<strong>probability data (from BDRIBS)</strong> <em>and</em> the
<strong>clinical context</strong>. When a decision is made either way,
it is typically documented in meeting minutes and communicated to
relevant stakeholders. If unblinded review is done, appropriate actions
(up to halting the trial or updating consent forms) can be taken
depending on what is found.</p></li>
</ol>
<p>Throughout these steps, it is clear that BDRIBS is a tool supporting
a <strong>structured, risk-based approach</strong> to safety monitoring,
rather than a standalone solution. The workflow ensures that statistical
signals lead to thoughtful medical evaluation before any drastic action,
thus balancing patient safety with trial integrity. It also shows the
multi-disciplinary nature of safety surveillance: for example,
epidemiologists contribute in Step 2, statisticians in Steps 3 and 5,
clinicians and safety experts in Steps 1, 6, 7, and so on. This
integration is important in a thesis or technical appendix to
demonstrate how methodology like BDRIBS is implemented in practice.</p>
<p><strong>Summary of Model Inputs and Outputs</strong></p>
<table>
<colgroup>
<col width="11%" />
<col width="88%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Model Inputs (Parameters &amp; Data)</strong></th>
<th><strong>Description / Role in BDRIBS</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Event of interest definition</strong></td>
<td>The specific adverse event or composite category being monitored
(e.g. serious infection, MACE, etc.). BDRIBS is run separately for each
such event category.</td>
</tr>
<tr class="even">
<td><strong>Historical background rate (λ_0)</strong></td>
<td>Expected incidence rate of the event in the study population (per
unit time), derived from epidemiological or prior trial data. Used to
set the prior for the control arm event rate – either as a fixed value
or as the mean of a Gamma prior. This represents “no treatment effect”
benchmark.</td>
</tr>
<tr class="odd">
<td><strong>Prior on background rate</strong></td>
<td>Choice of either a fixed λ_0 or a Gamma distribution for λ_0.
Parameters (shape, scale) are chosen based on the confidence in
historical data. A non-informative or weak prior (flat or broad Gamma)
can be used if unsure, whereas a tight Gamma reflects strong prior
knowledge.</td>
</tr>
<tr class="even">
<td><strong>Randomization ratio / allocation (p)</strong></td>
<td>The proportion of participants on the investigational drug (e.g. 0.5
for a 1:1 trial). If pooling multiple studies, an exposure-weighted
average allocation is used. This informs the prior probability that an
event is in the drug group.</td>
</tr>
<tr class="odd">
<td><strong>Prior on allocation (Beta prior for p)</strong></td>
<td>A Beta(<span class="math inline">\(\alpha\)</span>,<span
class="math inline">\(\beta\)</span>) prior for the probability an event
is from the drug arm. Often Beta(1,1) (uniform) for non-informative
prior, unless one wants to center it on the known allocation. This
captures prior belief about event split (often we start with no
preference).</td>
</tr>
<tr class="even">
<td><strong>Blinded observed event count (E)</strong></td>
<td>The total number of events observed in the trial <strong>for that
category</strong>, up to the analysis time. This is the key data input
that updates the priors. It is assumed to follow a Poisson mixture of
events from both arms under the hood.</td>
</tr>
<tr class="odd">
<td><strong>Total exposure or trial size</strong></td>
<td>The total patient-years of follow-up (or number of patients and
average follow-up time) accrued at the time of analysis. This, combined
with λ_0, determines the expected number of events under no treatment
effect. It helps scale the Poisson rates in the likelihood. (If using
person-years, expected events = λ_0 * total PY.)</td>
</tr>
<tr class="even">
<td><strong>Probability threshold for signal</strong></td>
<td>The pre-specified cutoff for <span
class="math inline">\(P(r&gt;c)\)</span> that constitutes a trigger
(e.g. 90% for <span class="math inline">\(c=1\)</span>). This is not an
input to the Bayesian model per se, but an input to the decision rule
applied to the model’s output. It is set in advance (Step 3) based on
risk tolerance.</td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col width="11%" />
<col width="88%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Model Outputs (After Posterior Analysis)</strong></th>
<th><strong>Description / Use</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Posterior distribution of λ_0 (control rate)</strong></td>
<td>If a Gamma prior was used, the analysis updates it to a posterior
for the control event rate given the data. Often, if λ_0 is fixed, this
output is trivial (λ_0 remains fixed). If not, one can obtain a
posterior mean and interval for the background rate. This can be used to
see if the overall event rate in the study is higher than expected even
without attribution to treatment.</td>
</tr>
<tr class="even">
<td><strong>Posterior distribution of p (event allocation)</strong></td>
<td>The Bayesian updating yields a posterior Beta distribution for p (or
samples thereof). This reflects the inferred probability that any given
event is in the drug arm after seeing the data. If the posterior of p
shifts away from the prior (e.g. shifts above the prior mean of 0.5), it
suggests more events are likely coming from the drug group than expected
by random chance.</td>
</tr>
<tr class="odd">
<td><strong>Posterior distribution of RR (r)</strong></td>
<td><strong>Primary output:</strong> the distribution of the relative
risk. This can be summarized by its median, mean, and credible interval.
For example, one might report that the posterior median <span
class="math inline">\(r\)</span> is 1.8 with a 95% credible interval
[0.9, 3.5], for instance. This informs how large the risk increase might
be, and the uncertainty around it.</td>
</tr>
<tr class="even">
<td><strong>Posterior probability of risk &gt; 1 (or &gt;
c)</strong></td>
<td>The <strong>trigger metric:</strong> <span
class="math inline">\(P(r&gt;1 \mid \text{data})\)</span> (or a
different threshold c) computed from the posterior. This is often
distilled into a single number used for decision-making. For instance,
“Based on current data, there is an 88% probability that the event rate
is higher on drug than on placebo.” This is compared to the predefined
threshold.</td>
</tr>
<tr class="odd">
<td><strong>Signal flag/alert</strong></td>
<td>A yes/no output indicating whether the threshold criterion was met.
This is the practical result used by the safety team: for example,
“Event X has triggered a signal at this analysis.” It is not a direct
statistical output but derived from the above probability.</td>
</tr>
<tr class="even">
<td><strong>Recommended action</strong></td>
<td>Although not a numerical model output, the analysis leads to an
action recommendation: either “continue blinded monitoring” or “escalate
for unblinded review.” In a technical appendix, one might note that if
the signal flag is TRUE, the recommendation is to refer to SAC (as per
workflow Step 7); if FALSE, no immediate action aside from continued
monitoring.</td>
</tr>
</tbody>
</table>
<hr />
<div id="prior-distributions-and-their-selection"
class="section level3 unnumbered">
<h3 class="unnumbered">Prior Distributions and Their Selection</h3>
<p>Being a Bayesian methodology, BDRIBS incorporates prior beliefs about
the event rates and how events split between treatment arms, then
updates these beliefs with incoming data. Two key priors are specified
in the model:</p>
<ul>
<li><strong>Background event rate (λ_0) prior:</strong> This is usually
given a <strong>Gamma distribution</strong> prior, reflecting
uncertainty in the true baseline rate of the event in the study
population. The gamma prior can be informed by historical data – for
example, using the observed number of events and total patient-years of
exposure from past studies or epidemiological sources to set the shape
and scale parameters. In some applications, if a very reliable estimate
of the background rate is available, the background rate may even be
treated as fixed (a degenerate prior) to simplify the analysis. Using a
fixed background rate reduces variability in the model and can make it
easier to trigger a signal (since all uncertainty then comes from the
data rather than the background rate). On the other hand, a gamma prior
allows the model to account for uncertainty in the assumed background
frequency.</li>
<li><strong>Allocation probability (p) prior:</strong> Since data are
blinded, we do not know how observed events are divided between drug and
control. BDRIBS addresses this by introducing a parameter <em>p</em>,
defined as the <strong>probability that an observed event occurred in
the investigational drug group</strong>. This effectively represents the
allocation of events to the drug arm (as a fraction of all events). The
prior for <em>p</em> is modeled with a <strong>Beta
distribution</strong>. In most cases, this beta prior is chosen to be
<strong>non-informative</strong> (for example, Beta(1,1), a uniform
prior), unless there is a strong reason to assume a particular split of
events. A non-informative prior for <em>p</em> means we initially
<strong>do not bias</strong> the analysis toward assuming either arm has
more events, aside from what the randomization ratio itself would
predict. (If desired, one could center the Beta prior around the known
randomization proportion – for instance, if the trial is 1:1
randomization, a Beta(α,α) symmetric prior centers at 0.5. However,
Waterhouse <em>et al.</em> choose a flat prior in general, letting the
data drive the allocation inference.)</li>
</ul>
<p>These two priors (for λ_0 and p) together induce an implied
<strong>prior distribution on the relative risk r</strong>. Notably,
there are two sources of uncertainty influencing the prior for RR:
<strong>(i)</strong> uncertainty in the baseline event rate (if λ_0 is
not fixed but has a gamma prior), and <strong>(ii)</strong> uncertainty
in the allocation of events between arms (captured by the beta prior on
<em>p</em>). If little prior information is available, both priors can
be set to be diffuse (e.g. Gamma with small shape/scale implying a wide
range of possible rates, and Beta(1,1) for p) so that the analysis is
driven largely by the data. In other cases, one or both priors can be
made more informative if solid external data exist (for example, a
precise background rate from a meta-analysis). <strong>In practice,
BDRIBS often starts with a conservative approach of fixing the
background rate at a plausible value for initial screening, then later
performing sensitivity analysis with a gamma-distributed background to
ensure conclusions are robust.</strong></p>
</div>
<div id="posterior-updating-with-blinded-data-mcmc-simulation"
class="section level3 unnumbered">
<h3 class="unnumbered">Posterior Updating with Blinded Data (MCMC
Simulation)</h3>
<p>Once the trial is ongoing and accumulating safety events, the BDRIBS
model is updated with the <strong>blinded safety data</strong> to obtain
the <strong>posterior distributions</strong> of the parameters,
especially the posterior for the relative risk <em>r</em>. The data
input at a given analysis is essentially the total number of events
observed for the event of interest and the total exposure (or an
equivalent measure of trial progress, such as total patient-years) at
that time. Because the data are blinded, we do not have the breakdown of
events by treatment, but the model treats the unknown allocation of
events between arms as a latent variable governed by the parameter
<em>p</em>. Given the priors described above and the likelihood of
observing the total event count under the Poisson mixture model, Bayes’
theorem is used to derive the <strong>joint posterior of (λ_0,
p)</strong> – or equivalently (λ_0, r) – <strong>given the observed
data</strong>.</p>
<p>In practice, the posterior distribution does not have a closed-form
solution, so <strong>Markov chain Monte Carlo (MCMC)</strong> methods
are employed to <strong>simulate draws from the posterior</strong>.
Waterhouse <em>et al.</em> implement the model in an <a
href="https://abbviescience.shinyapps.io/BDRIBS/">R/Shiny
application</a>, likely using MCMC (e.g. a Gibbs sampler or Hamiltonian
Monte Carlo via Stan/JAGS) to generate a large sample from the joint
posterior of the parameters. Each MCMC iteration might involve sampling
a possible split of events between arms (according to a binomial
distribution conditioned on <em>p</em>), as well as sampling a value of
the underlying event rate λ_0 (if not fixed). From these, one can derive
a sampled relative risk <span class="math inline">\(r =
\frac{\text{(sampled drug-arm rate)}}{\text{(sampled control
rate)}}\)</span>. After many iterations, the result is a
<strong>posterior distribution of the relative risk</strong>. This
posterior combines the prior information and the evidence from the
observed data. The <strong>posterior distribution of r</strong> is the
key output – it represents the updated belief about the true risk
difference given what has been observed so far. For example, if more
events are occurring than expected under the baseline rate assumption,
many posterior samples will correspond to <span
class="math inline">\(r&gt;1\)</span> (elevated risk), whereas if events
are in line with expectations, the posterior will concentrate around
<span class="math inline">\(r \approx 1\)</span>. The MCMC process
yields empirical estimates like the <strong>posterior mean or median of
r</strong>, credible intervals for r, and the <strong>posterior
probability of various hypotheses</strong> (e.g. the probability <span
class="math inline">\(r&gt;1\)</span>). Waterhouse <em>et al.</em> note
that the posterior distribution is <em>“instrumental in the evaluation
of blinded adverse events that may be potential signals”</em>, as it
provides a quantitative basis to decide if the data suggest an elevated
risk. All inference is done without unblinding the data by treatment
arm; the model leverages the prior and total counts to infer what the
treatment-specific rates might be.</p>
</div>
<div id="relative-risk-rr-in-the-posterior-and-its-interpretation"
class="section level3 unnumbered">
<h3 class="unnumbered">Relative Risk (RR) in the Posterior and Its
Interpretation</h3>
<p>The <strong>relative risk (r)</strong> is the primary measure of
interest in BDRIBS. It is derived from the model parameters as <span
class="math inline">\(r =
\frac{\lambda\_{\text{drug}}}{\lambda\_{\text{control}}}\)</span>, the
ratio of the event rate in the investigational drug arm to that in the
control arm. In the context of the blinded data, we do not directly
observe <span class="math inline">\(\lambda\_{\text{drug}}\)</span> or
<span class="math inline">\(\lambda\_{\text{control}}\)</span>, but each
MCMC draw effectively imputes what those rates could be (consistent with
the total event count and the priors), and thus yields a draw of <span
class="math inline">\(r\)</span>. The collection of MCMC draws produces
the <strong>posterior distribution of the relative risk</strong>.</p>
<p>Interpreting this posterior is analogous to interpreting any Bayesian
posterior for a treatment effect: it provides a probability distribution
for the true risk ratio given the data. From it, we can compute:</p>
<ul>
<li>The <strong>posterior mean or median of r</strong>, which is a point
estimate of the risk ratio after seeing the data. If this estimate is
near 1.0, it suggests no excess risk has been detected; if it is
substantially above 1, it indicates a higher rate of the event on drug
is more likely.</li>
<li>A <strong>credible interval</strong> for r (e.g. 95% credible
interval), which gives the range within which the true r lies with a
certain probability (credible intervals reflect the combined uncertainty
from both prior and data). A credible interval that lies mostly above 1
(e.g. lower bound &gt; 1) would indicate strong evidence of increased
risk.</li>
<li>The <strong>posterior probability that r exceeds certain
thresholds.</strong> This is perhaps the most actionable interpretation
in the BDRIBS framework. For instance, one can compute <span
class="math inline">\(P(r&gt;1 \mid \text{data})\)</span>, the
probability that the investigational drug’s event rate is higher than
control’s. More stringent thresholds can be used too, such as <span
class="math inline">\(P(r &gt; 2 \mid \text{data})\)</span> if one is
interested in the probability of at least doubling of risk.</li>
</ul>
<p>In BDRIBS, the focus is often on <span class="math inline">\(P(r &gt;
1 \mid \text{data})\)</span> as a measure of whether there is evidence
of any increased risk. This probability starts near 0.5 (50%) under the
prior if we assumed no prior bias favoring either arm (since if no data,
there’s an even chance <span class="math inline">\(r&gt;1\)</span> vs
<span class="math inline">\(r&lt;1\)</span> in a non-informative prior).
As data accumulate, if the event count is higher than expected under the
null (no risk difference), this probability will rise above 0.5; if it
stays around 0.5 or drops, it suggests no signal (or even perhaps fewer
events than expected on drug). Stakeholders consider not just point
estimates but these probabilities of risk exceeding certain benchmarks
to decide if a “signal” is emerging. For example, in one case study, as
more data were pooled, the posterior probability <span
class="math inline">\(P(r&gt;1)\)</span> increased from 0.80 to 0.90,
which was a notable enough rise that the team decided to refer the
safety signal for unblinded review. A high probability (close to 1) that
<span class="math inline">\(r&gt;1\)</span> means the data strongly
suggest the drug has a higher event rate than control; conversely, a low
probability (near 0) would indicate the drug might actually have a lower
rate (or the data favor that interpretation), though in most safety
monitoring the concern is on the upper side.</p>
<p>It’s important to note that a Bayesian <strong>posterior probability
of, say, 0.9 that <span class="math inline">\(r&gt;1\)</span></strong>
does not mean that in 90% of future repetitions of the trial we’d see an
effect – rather, it means “given the data observed so far and our prior,
we believe there is a 90% chance that the true event rate on drug
exceeds that on control.” This direct probabilistic interpretation is a
strength of the Bayesian approach, as it is more intuitive for
decision-making than frequentist p-values. The <strong>relative risk
posterior</strong> thus gives the medical and safety team a clear
quantitative assessment of how likely it is that an imbalance in adverse
events is present, and how large that imbalance might be (by examining
the distribution of r).</p>
</div>
<div id="probability-thresholds-for-triggering-safety-signals"
class="section level3 unnumbered">
<h3 class="unnumbered">Probability Thresholds for Triggering Safety
Signals</h3>
<p>A crucial element of applying BDRIBS in practice is deciding on
<strong>threshold criteria</strong> for when a safety signal should
prompt further action (such as unblinding or regulatory notification).
In the FDA’s safety guidance (2021) that motivated methods like BDRIBS,
a “trigger rate” approach was described: if the blinded event rate
exceeds some pre-specified level, an unblinded analysis is done. BDRIBS
replaces a hard trigger rate with a <strong>probability-based
trigger</strong>. The safety surveillance team must define <strong>what
probability (and of what event) constitutes a trigger</strong> in the
Bayesian sense. Typically, this takes the form of evaluating <span
class="math inline">\(Pr(r &gt; c \mid \text{data})\)</span> for some
threshold <span class="math inline">\(c\)</span> (often <span
class="math inline">\(c=1\)</span>, meaning any increase) and comparing
it to a predefined probability cutoff. For example, the team might
decide in the Safety Surveillance Plan that if <strong><span
class="math inline">\(P(r&gt;1 \mid \text{current data}) &gt;
0.90\)</span> (90%)</strong>, this will be considered a signal
warranting further investigation. In other words, there is high
confidence that the event is occurring at a higher rate on drug than
control. Alternatively, one could set <span
class="math inline">\(c\)</span> to a value greater than 1 if small
increases are expected or not worrisome – e.g., use <span
class="math inline">\(Pr(r &gt; 1.5) &gt; 0.8\)</span> as a trigger,
depending on the context and the seriousness of the event. The threshold
probabilities (e.g. 80%, 90%, 95%) are chosen by balancing false alarms
vs. missed signals, and can be tuned based on the prior or simulations
to have a certain false positive rate.</p>
<p>In the BDRIBS workflow, once the posterior is obtained via MCMC, the
<strong>statistical team calculates the relevant probability (or
probabilities) from the posterior</strong> – most often the probability
that <span class="math inline">\(r&gt;1\)</span>. This is then compared
against the pre-specified <strong>trigger probability
threshold</strong>. If the threshold is exceeded (i.e., the data now
provide sufficiently high probability of increased risk), that event is
flagged for potential unblinding. If not, the event does not trigger an
alert and the trial continues blinded for that issue. Importantly, this
threshold-based approach formalizes the decision to break the blind:
instead of waiting for an arbitrary number of events or an intuition, it
uses a quantitative rule. For instance, <strong>“exceeding the
predesignated probability threshold”</strong> is explicitly Step 5 in
the workflow. Only when that happens does the multi-disciplinary team
move to a deeper review (Step 6) and possibly unblinding (Step 7). The
thresholds are typically set in advance in the Safety Surveillance Plan
(during planning Step 3) to avoid bias or ad hoc decisions. By design,
these thresholds are <strong>conservative</strong> – one might use a
high bar like 95% if one wants near-certainty before unblinding (to
avoid unnecessary unblinding for noise), or a slightly lower bar like
80–90% if earlier detection is prioritized and the cost of a false alarm
is deemed low.</p>
<p>In summary, probability thresholds in BDRIBS convert the posterior
evidence into a yes/no signal: when the <strong>posterior probability of
a concerning level of risk exceeds the chosen cutoff</strong>, BDRIBS
“triggers” an alert. This approach aligns with the FDA’s notion of a
“trigger” for unblinding, but provides a more nuanced and continuous
assessment than a single cutoff event rate. The output can be phrased
as: <em>“Given the data, there is a X% chance the risk is
elevated”</em>, and if X exceeds our threshold (e.g. 90%), we take
action. If not, we continue to monitor. In practice, teams might monitor
this probability over time – a low probability might increase gradually
as more events accrue, and crossing 90% could prompt a meeting to decide
next steps, as illustrated in the examples by Waterhouse <em>et
al.</em>.</p>
</div>
<div id="extension-to-pooled-data-from-multiple-studies"
class="section level3 unnumbered">
<h3 class="unnumbered">Extension to Pooled Data from Multiple
Studies</h3>
<p>While BDRIBS was originally conceived for a single ongoing trial,
Waterhouse <em>et al.</em> describe an extension of the method to
<strong>pooled blinded data across multiple studies</strong>. In a
development program, it is common to have the same investigational drug
being tested in parallel trials (possibly in different indications or
populations). The <strong>motivation for pooling</strong> is to increase
the total exposure and event counts, potentially detecting rare safety
signals earlier by combining information. The statistical framework of
BDRIBS can be applied to pooled data <strong>provided certain
assumptions hold</strong>: (1) The underlying event rate for the control
(and by extension for the treatment, in absence of effect) is assumed to
be <strong>approximately the same across the studies</strong> (i.e., the
populations are sufficiently similar in terms of baseline risk); and (2)
The trial designs are comparable (similar inclusion/exclusion criteria,
similar monitoring and definitions of events, etc.). When these
conditions are met, one can treat the sum of events and sum of exposure
from the multiple studies as if from one larger study for the purposes
of the model.</p>
<p>A practical consideration in pooling is that different studies might
have different <strong>randomization ratios</strong> (for instance, one
trial might be 1:1 drug:placebo, another 2:1). In the blinded aggregate,
we need an effective overall allocation probability <em>p</em> that
reflects the mixture of studies. Waterhouse <em>et al.</em> recommend
using a <strong>weighted average of the randomization ratios</strong> of
the pooled studies to determine an <strong>effective allocation
proportion</strong> for the combined data. Essentially, one can
calculate the total number of patients (or patient-years) on
investigational drug across all studies and the total on control across
all studies, then compute <span class="math inline">\(p\_{\text{pooled}}
= \frac{\text{Total drug exposure}}{\text{Total exposure}}\)</span>.
This <span class="math inline">\(p\_{\text{pooled}}\)</span> is then
used as the parameter in the Beta prior (or as the fixed allocation if
one were to fix <em>p</em>). For example, if Trial A is 1:1 (50% drug)
and Trial B is 2:1 (66.7% drug), and if Trial B has more patients, the
pooled effective allocation might turn out to be around 60% (depending
on weights by sample size) – Waterhouse <em>et al.</em> give an
illustration where the weighted average allocation ratio was about
1.18:1 (approximately 54% drug) when pooling two trials. Using this
pooled allocation, one then inputs the <strong>total pooled number of
events</strong> and <strong>total pooled exposure</strong> into the
BDRIBS model as if it were one combined trial. The prior for the
background rate can likewise be obtained by pooling the background
information or assuming the same λ_0 applies.</p>
<p>The outcome of the pooled analysis is a posterior for the common
relative risk across studies. If an event is truly drug-related, pooling
should provide stronger evidence (more events to inform a higher
posterior probability of elevated risk). In one example, after pooling
two studies’ data, the posterior probability <span
class="math inline">\(P(r&gt;1)\)</span> increased markedly, leading the
team to refer the signal for unblinded review. If the studies are
heterogeneous (e.g., very different patient populations or control
rates), pooling might violate model assumptions, so this extension is
used only when appropriate. Essentially, <strong>BDRIBS can function as
a Bayesian meta-analysis of blinded safety data</strong> across trials.
The method is flexible enough that each study could even have its own
λ_0 prior if needed, but the simpler approach described is to assume a
common rate and common relative risk, and use aggregated counts. The
<strong>randomization adjustment</strong> via a weighted average ensures
that the Beta prior for <em>p</em> correctly reflects the fact that, in
the combined dataset, the expected fraction of events on drug is driven
by the aggregate allocation. This prevents bias that would occur if one
naively set <em>p</em> = 0.5 when in fact more subjects were on drug in
the pooled data. With this extension, companies can surveil an entire
clinical development program for a signal in a particular adverse event
category, potentially spotting signals that no single study is powered
to detect.</p>
</div>
</div>
</div>
<div id="adverse-event" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Adverse Event</h1>
<ul>
<li><p><strong>Historical context:</strong><br />
Historically, <strong>safety has received less methodological
attention</strong> compared to efficacy. As a result, development of
advanced statistical techniques for safety evaluation is <strong>still
evolving</strong>, but catching up. Efficacy analyses have long relied
on sophisticated inferential models, whereas safety often remains
descriptive—this is changing.</p></li>
<li><p><strong>Descriptive statistics are central:</strong><br />
Safety analyses are <strong>typically descriptive</strong>, meant to
<strong>support medical interpretation</strong> rather than to test
formal hypotheses. This aligns with how safety data are often
<strong>exploratory in nature</strong>, looking for signals rather than
proving effects.</p></li>
<li><p><strong>Assumptions matter:</strong><br />
Like all statistical approaches, safety analyses rely on assumptions
(e.g., constant hazard, non-informative censoring, independence).
<strong>Acknowledging and validating these assumptions</strong> is vital
to ensuring results are meaningful and trustworthy.</p></li>
<li><p><strong>No one-size-fits-all approach:</strong><br />
While systematic methods are often used (like EAIRs or standard
incidence tables), they may not capture all nuances. Therefore,
<strong>flexibility to apply additional or alternative methods</strong>
(like time-to-event or competing risks models) is encouraged, depending
on the context.</p></li>
<li><p><strong>Apply broad statistical principles:</strong><br />
Sound practices such as <strong>assumption checking</strong>, use of
<strong>normal approximations</strong> when valid, and even
<strong>meta-analysis</strong> (especially in pooled studies or signal
detection across trials) are equally important for safety data as they
are for efficacy.</p></li>
</ul>
<div id="safety-topics-of-special-interest" class="section level2"
number="4.1">
<h2><span class="header-section-number">4.1</span> Safety Topics of
Special Interest</h2>
<p>Safety topics may emerge throughout the drug lifecycle from various
sources:</p>
<table>
<colgroup>
<col width="26%" />
<col width="73%" />
</colgroup>
<tbody>
<tr class="odd">
<td>Source</td>
<td>Description</td>
</tr>
<tr class="even">
<td>Toxicology and nonclinical data</td>
<td>Suggest potential human toxicities</td>
</tr>
<tr class="odd">
<td>Known class effects</td>
<td>Effects typical to a drug class</td>
</tr>
<tr class="even">
<td>Literature</td>
<td>Reports of adverse effects</td>
</tr>
<tr class="odd">
<td>Post-marketing data</td>
<td>New or more frequent/severe adverse drug reactions discovered after
approval</td>
</tr>
<tr class="even">
<td>Phase I to IV clinical trials</td>
<td>Single events or imbalances in aggregate analyses indicating
potential safety concerns</td>
</tr>
<tr class="odd">
<td>Regulatory requests</td>
<td>Specific demands for analysis or reporting</td>
</tr>
<tr class="even">
<td>Safety reports review</td>
<td>Periodic Safety Update Reports (PSUR), Development Safety Update
Reports (DSUR)</td>
</tr>
</tbody>
</table>
<div id="definitions-of-important-terms"
class="section level3 unnumbered">
<h3 class="unnumbered">Definitions of Important Terms</h3>
<table>
<colgroup>
<col width="16%" />
<col width="83%" />
</colgroup>
<tbody>
<tr class="odd">
<td>Term</td>
<td>Definition</td>
</tr>
<tr class="even">
<td><strong>Adverse Event (AE)</strong></td>
<td>&gt; Any untoward medical occurrence associated with the use of a
study drug in humans, whether or not considered study-drug-related.</td>
</tr>
<tr class="odd">
<td><strong>Adverse Drug Reaction (ADR)</strong></td>
<td>&gt; An undesirable effect reasonably likely caused by a study drug,
either as part of its pharmacological action or unpredictable in
occurrence.</td>
</tr>
<tr class="even">
<td><strong>Percent (AE reporting context)</strong></td>
<td>&gt; Number of patients with an event divided by the number of
patients at risk, multiplied by 100. Also called event rate, incidence
rate, crude incidence rate, or cumulative incidence.</td>
</tr>
<tr class="odd">
<td><strong>Exposure-Adjusted Event Rate (EAER)</strong></td>
<td>&gt; Number of events (all occurrences counted) divided by total
time exposed. Also known as person-time absolute rate. Time units may be
adjusted (e.g., events per 100 person-years).</td>
</tr>
<tr class="even">
<td><strong>Exposure-Adjusted Incidence Rate (EAIR)</strong></td>
<td>&gt; Number of patients with an event divided by total time at risk.
For patients with events, time from first dose to first event; for
others, total assessment interval time. Also called person-time
incidence rate.</td>
</tr>
<tr class="odd">
<td><strong>Safety Topics of Interest</strong></td>
<td>&gt; Broad term including AESIs, identified or potential risks
needing characterization, potential toxicities (e.g., hepatic), drug
class-related findings, or regulatory requests.</td>
</tr>
<tr class="even">
<td><strong>Study-Size Adjusted Percentage</strong></td>
<td>&gt; Weighted percentage from multiple controlled studies,
calculated by weighting observed percentages within studies by relative
study size in pooled population. Also called study-size-adjusted
incidence percentage.</td>
</tr>
<tr class="odd">
<td><strong>Treatment-Emergent Adverse Event (TEAE)</strong></td>
<td>&gt; An AE occurring after first administration of intervention that
is new or worsened. Implementation varies across industry.</td>
</tr>
</tbody>
</table>
</div>
<div id="general-recommendations-for-safety-data-analyses-and-displays"
class="section level3 unnumbered">
<h3 class="unnumbered">General Recommendations for Safety Data Analyses
and Displays</h3>
<ul>
<li><strong>Odds Ratios</strong> can take any value between <span
class="math inline">\(-\infty\)</span> and <span
class="math inline">\(+\infty\)</span> on the log scale, making them
robust for detecting differences regardless of control event rates.</li>
<li><strong>Relative Risks</strong> can be misleading when control event
rates are high because the maximum risk ratio is bounded.</li>
<li><strong>Confidence Intervals</strong> provide insight into the
uncertainty of estimates but must be interpreted with caution,
especially when wide.</li>
<li>The choice of metric and statistical summaries should be guided by
the goal of the analysis (public health impact vs. signal detection) and
the audience.</li>
</ul>
<div
id="choice-of-comparative-metric-for-incidence-proportionspercentages"
class="section level4 unnumbered">
<h4 class="unnumbered">Choice of Comparative Metric for Incidence
Proportions/Percentages</h4>
<ul>
<li><strong>Purpose:</strong> Establish the <strong>risk
profile</strong> of a drug by comparing adverse event (AE) data between
patients receiving the investigational drug and a control group (placebo
or active control).</li>
<li>Metrics fall into two categories:</li>
</ul>
<table>
<colgroup>
<col width="9%" />
<col width="21%" />
<col width="42%" />
<col width="25%" />
</colgroup>
<tbody>
<tr class="odd">
<td>Metric Type</td>
<td>Examples</td>
<td>Characteristics</td>
<td>Use Case/Comments</td>
</tr>
<tr class="even">
<td><strong>Absolute Scale</strong></td>
<td>Risk difference</td>
<td>Directly reflects magnitude of affected patients; easier for rare
events</td>
<td>Good for understanding <strong>public health impact</strong></td>
</tr>
<tr class="odd">
<td><strong>Relative Scale</strong></td>
<td>Relative risk, Odds ratio, Hazard ratio</td>
<td>Useful as flagging mechanisms to identify events needing further
investigation</td>
<td>Good for understanding <strong>relative impact</strong></td>
</tr>
</tbody>
</table>
<ul>
<li><p><strong>Risk Difference:</strong></p>
<ul>
<li>Reflects the absolute magnitude of patients affected.</li>
<li>Easier to implement for <strong>low-frequency events</strong>.</li>
<li>Less effective for identifying rare events needing further
investigation.</li>
</ul></li>
<li><p><strong>Relative Risk:</strong></p>
<ul>
<li>Easier to understand than odds ratio.</li>
<li>Problematic when control event rates are high (e.g., if control rate
= 50%, relative risk maxes at 2).</li>
<li>Good for understanding <strong>relative impact</strong>.</li>
<li>Not invariant to coding changes (event vs. no event).</li>
</ul></li>
<li><p><strong>Odds Ratio:</strong></p>
<ul>
<li><p>Has better mathematical properties, including:</p>
<ul>
<li>Log odds ratio ranges from <span
class="math inline">\(-\infty\)</span> to <span
class="math inline">\(+\infty\)</span> regardless of control event
rate.</li>
<li>Invariant to coding changes (reciprocal when switching event/no
event).</li>
</ul></li>
<li><p>More effective for <strong>signal detection</strong> regardless
of background rate.</p></li>
<li><p>Less intuitive for lay audiences.</p></li>
<li><p>Useful as a <strong>flagging mechanism</strong> for further
investigation.</p></li>
</ul></li>
<li><p><strong>Summary of Metric Usefulness:</strong></p></li>
</ul>
<table>
<colgroup>
<col width="11%" />
<col width="23%" />
<col width="19%" />
<col width="28%" />
<col width="16%" />
</colgroup>
<tbody>
<tr class="odd">
<td>Metric</td>
<td>Understand Public Health Impact</td>
<td>Understand Relative Impact</td>
<td>Use for Signal Detection</td>
<td>Ease of Interpretation</td>
</tr>
<tr class="even">
<td>Risk Difference</td>
<td>Excellent</td>
<td>Poor</td>
<td>Difficult</td>
<td>Easy</td>
</tr>
<tr class="odd">
<td>Relative Risk</td>
<td>Poor</td>
<td>Good</td>
<td>Difficult (with high background rate)</td>
<td>Easy</td>
</tr>
<tr class="even">
<td>Odds Ratio</td>
<td>Moderate</td>
<td>Excellent</td>
<td>Excellent</td>
<td>Difficult</td>
</tr>
</tbody>
</table>
<ul>
<li><p><strong>Presentation Recommendations:</strong></p>
<ul>
<li>For public presentation, absolute differences or risk ratios are
preferred over odds ratios.</li>
<li>Interactive displays should allow users to select among multiple
metrics.</li>
<li>For Adverse Events of Special Interest (AESIs), showing <strong>both
absolute and relative metrics</strong> is often warranted.</li>
</ul></li>
</ul>
</div>
<div id="p-values-and-confidence-intervals-in-safety-assessments"
class="section level4 unnumbered">
<h4 class="unnumbered">P-values and Confidence Intervals in Safety
Assessments</h4>
<ul>
<li><p>There is ongoing debate about the <strong>value of p-values and
confidence intervals (CIs)</strong> in safety data.</p></li>
<li><p>According to the <strong>FDA Clinical Review Template</strong>
and <strong>ICH E9 Section 6.4</strong>:</p>
<ul>
<li>P-values and CIs are <strong>descriptive</strong>, not inferential,
unless trials are specifically powered for hypothesis testing.</li>
<li>CIs can aid interpretation by showing uncertainty.</li>
<li>P-values can serve as a <strong>flagging mechanism</strong> to
highlight differences warranting further attention.</li>
</ul></li>
<li><p><strong>Recommendations on Use:</strong></p>
<ul>
<li>Include some measure of uncertainty: confidence intervals, p-values,
posterior credible intervals, or posterior probabilities.</li>
<li>This white paper favors <strong>confidence intervals</strong> over
p-values to provide a crude estimate of evidence strength.</li>
<li>If p-values are used, report <strong>actual p-values</strong> rather
than threshold indicators (e.g., asterisks), to emphasize interpretation
over hypothesis testing.</li>
<li>Avoid basing adverse drug reaction (ADR) conclusions solely on
p-values or confidence intervals.</li>
</ul></li>
<li><p><strong>Potential Issues and Interpretation
Challenges:</strong></p></li>
</ul>
<table>
<colgroup>
<col width="33%" />
<col width="66%" />
</colgroup>
<tbody>
<tr class="odd">
<td>Concern</td>
<td>Explanation</td>
</tr>
<tr class="even">
<td>High p-values or CIs including 0 (or 1)</td>
<td>May cause unwarranted dismissal of potential signals</td>
</tr>
<tr class="odd">
<td>Misinterpretation of p-values</td>
<td>Could lead to unnecessary concern over too many outcomes</td>
</tr>
<tr class="even">
<td>Wide confidence intervals</td>
<td>Often arise from low event frequencies; high upper bounds may cause
undue alarm</td>
</tr>
</tbody>
</table>
<ul>
<li><p>Educating interpreters of safety analyses on these nuances is
<strong>critical</strong>.</p></li>
<li><p><strong>Recommendation for Reporting Safety Data:</strong></p>
<ul>
<li>Include within-arm descriptive statistics.</li>
<li>Report measures of difference between arms (e.g., risk
difference).</li>
<li>Accompany differences with confidence intervals.</li>
</ul></li>
<li><p><strong>Decision-Making Frameworks for ADR
Identification:</strong></p></li>
</ul>
<table>
<colgroup>
<col width="22%" />
<col width="77%" />
</colgroup>
<tbody>
<tr class="odd">
<td>Framework</td>
<td>Description</td>
</tr>
<tr class="even">
<td><strong>CIOMS Working Group</strong></td>
<td>Flexible framework considering frequency, timing, preclinical
findings, mechanism of action</td>
</tr>
<tr class="odd">
<td><strong>Bradford Hill Criteria</strong></td>
<td>Provides criteria for causality assessment based on multiple
evidence sources</td>
</tr>
</tbody>
</table>
<ul>
<li>When only percentages are displayed (without p-values, CIs, or
comparative metrics), it can be difficult to identify events for further
scrutiny.</li>
<li>Objective processes or team decisions are necessary to prioritize AE
review.</li>
</ul>
</div>
</div>
</div>
<div
id="meddra-hierarchical-structure-and-adverse-events-of-special-interest-aesi"
class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> MedDRA Hierarchical
Structure and Adverse Events of Special Interest (AESI)</h2>
<p><strong>MedDRA Hierarchy Levels</strong></p>
<table>
<colgroup>
<col width="44%" />
<col width="56%" />
</colgroup>
<tbody>
<tr class="odd">
<td>Level</td>
<td>Description</td>
</tr>
<tr class="even">
<td><strong>SOC (System Organ Class)</strong></td>
<td>Highest level grouping (e.g., “Neoplasms”)</td>
</tr>
<tr class="odd">
<td><strong>HLGT (High Level Group Terms)</strong></td>
<td>Subgroups within SOC</td>
</tr>
<tr class="even">
<td><strong>HLT (High Level Terms)</strong></td>
<td>Further subgroups within HLGT</td>
</tr>
<tr class="odd">
<td><strong>PT (Preferred Terms)</strong></td>
<td>Single medical concepts or events</td>
</tr>
<tr class="even">
<td><strong>LLT (Lowest Level Terms)</strong></td>
<td>Synonyms or variations of PTs</td>
</tr>
</tbody>
</table>
<ul>
<li>Each <strong>PT</strong> is assigned one <strong>primary
SOC</strong>, and may have additional <strong>secondary SOCs</strong>
based on site of manifestation or other factors.</li>
<li>Study databases usually populate only <strong>primary
SOC/HLT</strong> for PTs unless otherwise specified.</li>
<li>Analysts must specify if they want to include PTs with
<strong>primary SOC/HLT only</strong> or also those with
<strong>secondary SOC/HLT</strong>.</li>
<li>When sponsors create <strong>Custom MedDRA Queries (CMQs)</strong>,
the list of PTs used should be provided in a dataset.</li>
<li>Regulatory agencies should review PT lists to ensure appropriateness
and facilitate analysis integration.</li>
</ul>
<p><strong>Examples of PT Assignments with Primary and Secondary
SOCs</strong></p>
<table>
<colgroup>
<col width="17%" />
<col width="22%" />
<col width="20%" />
<col width="40%" />
</colgroup>
<tbody>
<tr class="odd">
<td>PT Term</td>
<td>Primary SOC</td>
<td>Secondary SOC</td>
<td>Notes</td>
</tr>
<tr class="even">
<td>Congenital absence of bile ducts</td>
<td>Congenital, familial and genetic disorders</td>
<td>Hepatobiliary disorders</td>
<td>Secondary SOC based on site of manifestation</td>
</tr>
<tr class="odd">
<td>Skin cancer</td>
<td>Neoplasms benign, malignant, unspecified</td>
<td>Skin and subcutaneous tissue disorders</td>
<td>Primary SOC assignment depends on site of manifestation for cysts
and polyps</td>
</tr>
<tr class="even">
<td>Enterocolitis infectious ducts</td>
<td>Infections and infestations</td>
<td>Gastrointestinal disorders</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div id="exposure-adjusted-incidence-rates-eairs" class="section level2"
number="4.3">
<h2><span class="header-section-number">4.3</span> Exposure-Adjusted
Incidence Rates (EAIRs)</h2>
<div id="applicability" class="section level3 unnumbered">
<h3 class="unnumbered">Applicability</h3>
<p>In the context of clinical studies, specifically as a <strong>measure
of the rate of occurrence of adverse events (AEs)</strong> associated
with exposure to a drug. It categorizes when these incidence rates most
accurately reflect the true risk, and when they may not.</p>
<p><strong>Incidence rates most accurately represent true risk
when:</strong></p>
<ul>
<li><p><strong>All study participants are treated and followed up for
the same duration</strong>: This ensures that any difference in AE rates
is not due to differing lengths of exposure or follow-up time. Uniform
exposure and observation periods across all subjects allow for more
reliable comparisons.</p></li>
<li><p><strong>The duration of drug exposure is very short</strong>: In
short-term treatments, the likelihood of external factors influencing AE
rates is minimized. As a result, AEs observed are more likely to be
directly attributable to the drug rather than prolonged exposure or
confounding variables over time.</p></li>
<li><p><strong>The AE is acute and occurs very soon after
exposure</strong>: When an adverse event is known to develop shortly
after drug administration, unadjusted incidence rates can reliably
reflect the true risk. The close temporal proximity between exposure and
AE minimizes uncertainty in causal interpretation.</p></li>
</ul>
<p><strong>Incidence rates may not accurately represent true risk
when:</strong></p>
<ul>
<li><p><strong>Different treatment or follow-up durations exist between
treatment arms by design</strong>: For example, if one treatment group
is followed for 6 months and another for 12 months, the difference in
follow-up time introduces bias in comparing incidence rates, as longer
durations naturally provide more opportunity for AEs to occur.</p></li>
<li><p><strong>There is a high or unequal rate of study participant
discontinuation between treatment arms</strong>: If more participants
discontinue treatment in one group than the other, the total exposure
time differs, which can distort comparisons and lead to inaccurate
estimates of AE risk.</p></li>
<li><p><strong>The duration of treatment exposure is very long</strong>:
Over longer periods, more external variables may come into play (e.g.,
aging, comorbidities, background medication use), which can dilute or
obscure the direct relationship between the drug and the AE.</p></li>
<li><p><strong>The AE is very rare or very common</strong>: If an AE is
extremely rare, the sample size might be insufficient to detect a
meaningful difference, leading to unstable or misleading incidence
rates. If the AE is very common, background noise may mask the specific
contribution of the drug, reducing the specificity of the risk
estimate.</p></li>
</ul>
<p>In summary, <strong>exposure-unadjusted incidence rates are best used
in controlled conditions with consistent exposure and follow-up</strong>
and are most informative for acute, clearly drug-related adverse events.
When conditions vary across groups or the observation period is
extended, these rates become less reliable for assessing true risk, and
<strong>exposure-adjusted analyses or more complex statistical
modeling</strong> may be necessary.</p>
</div>
<div id="defination-and-assumptions" class="section level3 unnumbered">
<h3 class="unnumbered">Defination and Assumptions</h3>
<p><strong>Exposure-Adjusted Incidence Rates (EAIRs):
Overview</strong></p>
<p>EAIRs quantify how frequently an adverse event occurs relative to the
total amount of time participants are exposed to treatment. It’s
typically expressed as the number of participants with an event per 100
patient-years.</p>
<p><strong>Definition of EAIRs</strong></p>
<p>EAIR is calculated as:</p>
<p><span class="math display">\[
\text{EAIR} = 100 \times \frac{n}{\sum_{i=1}^{N} T_{\text{Exp}}(i)}
\]</span></p>
<ul>
<li><strong>Numerator (<code>n</code>)</strong>: Number of study
participants who experienced the AE.</li>
<li><strong>Denominator</strong>: Sum of exposure time (in years) across
all participants:
<ul>
<li>For participants with an AE: from drug start to first AE.</li>
<li>For participants without an AE: from drug start to end of
follow-up.</li>
</ul></li>
</ul>
<p><strong>Key Assumptions for EAIRs</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Constant Hazard</strong>
<ul>
<li>The risk of the AE is assumed to remain constant over the
observation period.</li>
<li>This is often not the case, especially for chronic or delayed
AEs.</li>
<li>The assumption is more reasonable in short-duration trials.</li>
</ul></li>
<li><strong>Non-informative Censoring</strong>
<ul>
<li>Participants who discontinue early are assumed to have the same AE
risk as those who complete the study.</li>
<li>If discontinuation is related to the AE, this assumption is
violated.</li>
</ul></li>
</ol>
<p><strong>Worked Example</strong></p>
<ul>
<li><strong>Number of participants</strong>: 5</li>
<li><strong>Total participant-years at risk</strong>: 4 years</li>
<li><strong>Participants with AEs</strong>: 3</li>
</ul>
<p><strong>Exposure durations:</strong></p>
<ul>
<li>Participant 1: 1.0 year (no AE)</li>
<li>Participant 2: 0.6 year (AE)</li>
<li>Participant 3: 0.9 year (no AE)</li>
<li>Participant 4: 0.8 year (AE)</li>
<li>Participant 5: 0.7 year (AE)</li>
</ul>
<p>Total exposure time = 0.6 + 0.8 + 0.9 + 1.0 + 0.7 = 4.0 years<br />
Number with AEs = 3</p>
<p>So:</p>
<p><span class="math display">\[
\text{EAIR} = 100 \times \frac{3}{4} = 75.0 \text{ per 100
patient-years}
\]</span></p>
<p><strong>Interpretation:</strong> If 100 patients were treated for 1
year, 75 of them are expected to experience the AE of interest under
similar conditions.</p>
</div>
<div id="confidence-intervals-for-eairs"
class="section level3 unnumbered">
<h3 class="unnumbered">Confidence intervals for EAIRs</h3>
<p>How to calculate <strong>Confidence Intervals (CIs) for
Exposure-Adjusted Incidence Rates (EAIRs)</strong> and highlights the
importance of selecting an appropriate method, particularly when the
number of events is small.</p>
<p>Confidence intervals help express the <strong>uncertainty</strong>
around the point estimate of the EAIR, indicating the likely range in
which the true incidence rate lies.</p>
<p><strong>Two Methods to Calculate 95% Confidence Intervals for
EAIRs:</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Exact Poisson Confidence Intervals</strong>
<ul>
<li><p>Preferred when event counts are small, because they do not rely
on assumptions of normality.</p></li>
<li><p>Based on the Chi-square (χ²) distribution.</p></li>
<li><p>The lower and upper bounds (LCL and UCL) are calculated
using:</p>
<p><span class="math display">\[
LCL = 100 \times \frac{\chi^2_{2n,\alpha/2}}{2 \sum_{i=1}^{N}
T_{\text{Exp}}(i)}
\quad \text{and} \quad
UCL = 100 \times \frac{\chi^2_{2(n+1),1-\alpha/2}}{2 \sum_{i=1}^{N}
T_{\text{Exp}}(i)}
\]</span></p></li>
</ul></li>
<li><strong>Normal Approximation</strong>
<ul>
<li><p>A simpler method using the normal distribution (Z-value of 1.96
for 95% CI).</p></li>
<li><p>Suitable when <strong>event counts are large</strong> and Poisson
distribution can be approximated by the normal distribution.</p></li>
<li><p>Formula:</p>
<p><span class="math display">\[
100 \times \left( \frac{n}{\sum T_{\text{Exp}}(i)} \pm 1.96 \times
\sqrt{\frac{n}{\left(\sum T_{\text{Exp}}(i)\right)^2}} \right)
\]</span></p></li>
</ul></li>
</ol>
<hr />
<p><strong>Worked Example:</strong></p>
<ul>
<li><strong>Participants</strong>: 5<br />
</li>
<li><strong>Total time at risk</strong>: 4 participant-years<br />
</li>
<li><strong>Number of AEs</strong>: 3<br />
</li>
<li><strong>EAIR</strong>: 75.0 per 100 participant-years</li>
</ul>
<p><strong>Using the Two CI Methods:</strong></p>
<ul>
<li><strong>Exact Poisson 95% CI</strong>: (15.47, 219.18)
<ul>
<li>Interpreted as a wide interval indicating considerable uncertainty,
especially due to the small number of events.</li>
</ul></li>
<li><strong>Normal 95% CI</strong>: (-9.87, 159.87)
<ul>
<li>Problematic because the <strong>lower limit is negative</strong>,
which is not meaningful for incidence rates.</li>
<li>Highlights why <strong>normal approximation is unreliable</strong>
with small event counts.</li>
</ul></li>
</ul>
<p><strong>Key Takeaways:</strong></p>
<ul>
<li>Use <strong>Poisson CIs</strong> when the number of events is small
or when accurate estimation is critical.</li>
<li>Always <strong>scale EAIR and its CI</strong> according to the
desired follow-up unit (e.g., per 100 patient-years).</li>
<li><strong>Avoid normal approximation</strong> in early-phase trials or
rare event settings—it can lead to misleading, even impossible values
(like negative rates).</li>
<li><strong>Every AE requires its own time-at-risk calculation</strong>:
This is one of the most frequent sources of EAIR errors. Different AEs
have different time windows (from drug start to AE or to censoring), and
each must be calculated independently.</li>
<li><strong>Denominator (time at risk) should not be calculated at the
TFL level</strong>: Instead, it should be included in the ADaM dataset
as a pre-derived variable to ensure consistency, reproducibility, and
ease of QC.</li>
<li><strong>Create a dedicated time-to-event ADaM dataset (e.g.,
ADTTAE)</strong>: Housing all AE-specific time-at-risk variables in one
dataset streamlines programming and QC processes and supports reliable
derivation of both EAIRs and confidence intervals.</li>
<li><strong>Consider specifying EAIR denominators in the table
shells</strong>: This makes statistical QC easier and reduces the risk
of mismatches during table generation.</li>
</ul>
</div>
<div id="interpretation-of-eairs" class="section level3 unnumbered">
<h3 class="unnumbered">Interpretation of EAIRs</h3>
<p><strong>EAIRs are descriptive statistics that can help with medical
interpretation of safety data</strong></p>
<p>EAIRs help normalize the occurrence of adverse events (AEs) by
accounting for the amount of time each participant is at risk (i.e.,
exposed to the drug). This allows for fairer comparisons across
treatment groups, especially when follow-up times vary. However, their
interpretability has limits and depends on several underlying
assumptions.</p>
<hr />
<p><strong>Interpretation of exposure-adjusted incidence rates is only
straightforward under assumption of constant event rate over
time</strong></p>
<p>This is a key statistical assumption behind EAIRs. It assumes the
hazard (risk) of an event remains <strong>constant over time</strong>,
which simplifies the interpretation. However, in real-world scenarios,
risk may vary over time due to accumulation of exposure, adaptive
resistance, seasonality, etc.</p>
<p>If the risk is <strong>not constant</strong>, then EAIRs become
harder to interpret, as they could under- or over-estimate the true risk
at different time points.</p>
<hr />
<p><strong>Interpretation of EAIRs depends on several
factors:</strong></p>
<ul>
<li><strong>Study design</strong>: Different designs (e.g., crossover,
parallel, open-label) impact how and when exposure is measured.</li>
<li><strong>Characteristics of the indication</strong>: Some diseases
may naturally carry time-varying risks or have episodic flare-ups.</li>
<li><strong>Study participant population</strong>: Age, sex,
comorbidities, and other population-specific factors can affect AE
risk.</li>
<li><strong>Characteristics of the adverse events themselves</strong>,
especially:
<ul>
<li><strong>Frequency</strong>: Whether the event is very rare or very
common can affect the stability and interpretability of the rate.</li>
<li><strong>Time dependency</strong>:
<ul>
<li><strong>Time to onset</strong>: If AEs tend to occur early or late
after exposure, this affects whether EAIRs capture the true risk.</li>
<li><strong>Seasonality</strong>: For diseases or AEs affected by time
of year (e.g., flu, allergy), constant hazard assumptions may not
apply.</li>
</ul></li>
</ul></li>
</ul>
<hr />
<p><strong>Confidence intervals can aid in the interpretation of
EAIRs</strong></p>
<p>Confidence intervals (CIs) reflect the <strong>uncertainty or
variability</strong> of the EAIR estimate.</p>
<ul>
<li>CIs provide a <strong>crude but informative range</strong> of where
the true value may lie.</li>
<li>They should be treated as descriptive statistics—not
hypothesis-testing tools.</li>
<li>CIs are generally <strong>preferred over p-values</strong>,
especially in safety analyses, where the focus is on understanding risk,
not testing for differences.</li>
</ul>
<hr />
<p><strong>Notes to consider when reviewing competitor
publications:</strong></p>
<ul>
<li><strong>Terminology varies</strong>: EAIRs may also be referred to
as <em>incidence density</em>, <em>incidence per person-time</em>, or
similar terms. It’s important to confirm definitions before making
comparisons.</li>
<li><strong>Incorrect exposure definitions are common</strong>: Some
publications may use the full study duration (e.g., up to last
follow-up) as the time at risk for all participants. This is not
correct. <strong>Exposure time should end when the first instance of the
AE occurs</strong>, since the individual is no longer at risk of
experiencing the first occurrence of that event.</li>
</ul>
</div>
<div id="alternative-and-supplementary-methods-to-consider"
class="section level3 unnumbered">
<h3 class="unnumbered">Alternative and supplementary methods to
consider</h3>
<p>Each method has its own strengths and assumptions, and please
<strong>check those assumptions</strong> carefully when selecting an
analytical approach.</p>
<p>When analyzing safety data, EAIRs are just one tool. Depending on
your objectives, event frequency, timing, and recurrence, you may need
to consider:</p>
<ul>
<li>Stratified or time-windowed EAIRs</li>
<li>Time-to-event methods for first occurrences</li>
<li>Event-based rates for multiple AEs</li>
<li>Competing risk or recurrent event models for complex cases</li>
</ul>
<hr />
<p><strong>Exposure-adjusted incidence rates for discrete
periods</strong></p>
<p>This method involves dividing the study period into separate
intervals (e.g., weeks or months) and calculating EAIRs for each
interval. This has two key advantages: - The <strong>constant hazard
assumption</strong> is more likely to be reasonable over shorter,
defined timeframes than over the entire study period. - It allows for
description of how AE risk <strong>changes over time</strong>, helping
identify periods of higher or lower risk (e.g., early onset
toxicity).</p>
<hr />
<p><strong>Time-to-event analyses</strong></p>
<p>This refers to survival-type methods, primarily: -
<strong>Kaplan-Meier analysis</strong>, which estimates the probability
of event-free survival over time. - These methods <strong>do not require
the assumption of constant hazard</strong>. - They are particularly
useful when focusing on <strong>time to the first occurrence</strong> of
an adverse event, rather than repeated or cumulative counts.</p>
<p>This approach provides a detailed look at the <strong>temporal aspect
of safety</strong>, helping answer: “When are events most likely to
occur?”</p>
<hr />
<p><strong>Exposure-adjusted event rates</strong></p>
<p>These are used to: - Account for <strong>recurrent events</strong>,
where the same AE can happen more than once in a participant. - It uses
the same framework as EAIRs but tracks <strong>event counts rather than
participant counts</strong>, offering a broader view of overall AE
burden. - Assumptions are generally <strong>similar to EAIRs</strong>,
including the need for constant rate assumptions and appropriate
handling of exposure time.</p>
<p>This method is valuable for chronic or cycling conditions where
multiple AEs per participant are expected.</p>
<hr />
<p><strong>Other multiple event-based analyses</strong></p>
<p>This category includes more advanced statistical methods designed to
address complex event patterns:</p>
<ul>
<li><strong>Recurrent event models</strong>: These go beyond just first
events and model all repeated occurrences.</li>
<li><strong>Mean cumulative function</strong>: Summarizes the average
number of events per subject over time.</li>
<li><strong>Competing risks models</strong>: Adjust for scenarios where
<strong>different types of events compete</strong>, such as when death
precludes the occurrence of a non-fatal AE.</li>
</ul>
<p>These methods are more complex but better reflect <strong>real-world
clinical scenarios</strong>, especially when multiple outcomes are
interrelated.</p>
</div>
</div>
</div>
<div id="structure-benefit-risk-assessment" class="section level1"
number="5">
<h1><span class="header-section-number">5</span> Structure Benefit-Risk
Assessment</h1>
<div id="door-desirability-of-outcome-ranking" class="section level2"
number="5.1">
<h2><span class="header-section-number">5.1</span> DOOR (Desirability of
Outcome Ranking)</h2>
<div id="background-and-motivation" class="section level3 unnumbered">
<h3 class="unnumbered">Background and Motivation</h3>
<p><strong>1. Limitations of Traditional Benefit-Risk (BR)
Assessments</strong></p>
<ul>
<li><p><strong>Separate Evaluation of Efficacy and
Safety</strong>:<br />
Often, efficacy (how well a treatment works) and safety (side effects or
adverse events) are analyzed in isolation. For example, a drug might be
reported as 50% effective and having 30% safety issues, but we don’t
know how these outcomes are distributed across the same patients. This
separation can lead to misleading conclusions when trying to understand
the full impact of a treatment.</p></li>
<li><p><strong>Ignoring Associations Between Outcomes</strong>:<br />
It’s important to know whether the same patients who benefit from a
treatment are also the ones who suffer side effects. For instance, do
successful outcomes come at the cost of more safety problems? Without
looking at this association, we can’t fully understand the
trade-offs.</p></li>
<li><p><strong>Overlooking Cumulative Patient Experience</strong>:<br />
Each patient experiences both benefits and risks together, not
separately. Traditional approaches often summarize outcomes in
percentages, ignoring how each individual patient is affected as a
whole. This simplification can hide clinically meaningful
patterns.</p></li>
<li><p><strong>Neglecting Patient Heterogeneity</strong>:<br />
Not all patients respond the same way. Some may benefit greatly with few
risks, while others may experience no benefit and many side effects.
Traditional methods don’t adequately account for this variability,
leading to generalizations that may not apply to subgroups.</p></li>
</ul>
<p><strong>2. CIOMS and New Directions in BR Assessment</strong></p>
<p><strong>CIOMS report</strong>, a respected international guideline
for benefit-risk evaluation of medicinal products. It introduces two
major shifts:</p>
<ul>
<li><p><strong>Structured and Proactive Benefit-Risk
Design</strong>:<br />
Instead of waiting until a trial is over and doing a benefit-risk
assessment retrospectively, researchers should now <strong>incorporate
BR thinking into trial design</strong> from the start. That means
clearly defining benefit and risk outcomes, understanding how they
relate, and planning how to evaluate them jointly.</p></li>
<li><p><strong>Patient-Centric Benefit-Risk Assessment</strong>:<br />
The new trend is to place <strong>patients’ perspectives and
experiences</strong> at the center of benefit-risk evaluation. It’s not
just about whether a treatment works statistically, but whether the
benefit justifies the risk <strong>for a real patient</strong>.
Different patients value outcomes differently—some may tolerate side
effects for a small benefit, others may not. This approach helps ensure
that regulatory decisions and clinical guidance better reflect patient
needs and values.</p></li>
</ul>
</div>
<div id="how-does-door-work" class="section level3 unnumbered">
<h3 class="unnumbered">How Does DOOR Work?</h3>
<p>DOOR is a <strong>patient-centric paradigm</strong> that supports the
design, monitoring, analysis, and reporting of clinical trials. It
shifts the focus from traditional, separated endpoints to
<strong>comprehensive, integrated outcomes</strong> that reflect what
matters most to patients. The table outlines four key features of
DOOR:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Patient-Centered Approach</strong><br />
DOOR prioritizes outcomes that are meaningful to patients—such as
combining information about treatment efficacy, safety, and quality of
life. This reflects real-world decision-making, where patients consider
multiple aspects simultaneously rather than in isolation.</p></li>
<li><p><strong>Holistic Evaluation</strong><br />
Rather than analyzing efficacy and safety separately, DOOR integrates
all benefit-risk dimensions into a single <strong>composite
outcome</strong>, giving a unified and intuitive understanding of the
overall clinical impact.</p></li>
<li><p><strong>Ordinal Ranking System</strong><br />
Outcomes are placed in <strong>ordered categories</strong>, from the
most desirable (e.g., cure without side effects) to the least desirable
(e.g., no improvement with severe side effects or death). This helps
translate clinical trial data into a more interpretable framework for
decision-making.</p></li>
<li><p><strong>Flexibility in Design</strong><br />
DOOR is adaptable—it can be tailored to the specific needs of different
diseases, therapeutic areas, or patient populations by selecting
<strong>clinically meaningful events</strong> for ranking. This makes it
relevant across diverse trial settings.</p></li>
</ol>
<p>The process of implementing DOOR in a clinical trial follows three
main steps:</p>
<ol style="list-style-type: decimal">
<li><strong>Define and Rank Outcomes</strong>
<ul>
<li>Based on the <strong>patient journey</strong>, outcomes are defined
that capture both benefit and risk dimensions.</li>
<li>These outcomes are then <strong>ranked from most desirable to least
desirable</strong>, reflecting clinical and patient-centered
priorities.</li>
</ul></li>
<li><strong>Assign Patients to DOOR Categories</strong>
<ul>
<li>Each patient is assigned to one of the predefined DOOR categories
according to the outcome they experienced.</li>
<li>For example, if the categories are:
<ol style="list-style-type: decimal">
<li>Full recovery, no adverse events<br />
</li>
<li>Recovery with mild adverse events<br />
</li>
<li>No recovery<br />
</li>
<li>Death<br />
→ Each patient will be placed into one of these based on their results
in the trial.</li>
</ol></li>
</ul></li>
<li><strong>Compare Treatment Arms</strong>
<ul>
<li>The treatment groups are compared <strong>based on the
probability</strong> that a patient from one group has a better DOOR
outcome than a patient from another group.</li>
<li>Two types of analyses can be conducted:
<ol style="list-style-type: decimal">
<li><strong>Rank-based</strong> comparison (using the full ordinal
scale)<br />
</li>
<li><strong>Grade-based</strong> (partial credit) approach that gives
different weights to each level, allowing more granularity (e.g., if one
group has more patients in slightly better categories).</li>
</ol></li>
</ul></li>
</ol>
<p><img src="02_Plots/Safety%20Evaluation/DOOR_Structure1.png" /></p>
<p><img src="02_Plots/Safety%20Evaluation/DOOR_Structure2.png" /></p>
<p><strong>Example of DOOR Components (Important Events) for Adaptive
covid-19 Treatment Trial(ACTT-1):</strong></p>
<ul>
<li>Death<br />
</li>
<li>Hospitalization with invasive mechanical ventilation / ECMO<br />
</li>
<li>Serious adverse event (SAE) not resolved or resolved with
sequelae</li>
</ul>
<table>
<colgroup>
<col width="34%" />
<col width="34%" />
<col width="31%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>DOOR Rank Category</strong></th>
<th><strong>Remdesivir Frequency (N=541)</strong></th>
<th><strong>Placebo Frequency (N=521)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Alive with no events</td>
<td>433</td>
<td>382</td>
</tr>
<tr class="even">
<td>Alive with 1 event</td>
<td>42</td>
<td>57</td>
</tr>
<tr class="odd">
<td>Alive with 2 events</td>
<td>8</td>
<td>6</td>
</tr>
<tr class="even">
<td>Death</td>
<td>58</td>
<td>76</td>
</tr>
</tbody>
</table>
<p><strong>Why is DOOR Powerful?</strong></p>
<ul>
<li><strong>Reflects real-world patient experience</strong>: Instead of
reducing outcomes to binary variables (e.g., success/failure), DOOR
captures nuanced information across multiple dimensions.</li>
<li><strong>Improves interpretability</strong>: Ordinal categories are
more intuitive for clinicians, patients, and regulators.</li>
<li><strong>Supports better decision-making</strong>: When treatments
have similar efficacy but differ in safety or quality-of-life impact,
DOOR makes these differences more visible.</li>
</ul>
</div>
<div id="two-door-analyses-methods" class="section level3 unnumbered">
<h3 class="unnumbered">Two DOOR Analyses Methods</h3>
<p><strong>Rank-based Analysis Approach</strong></p>
<p>This approach focuses on <strong>pairwise comparisons</strong>
between individuals across treatment groups using the <strong>DOOR
probability</strong>, which reflects the chance that a participant from
one group has a <strong>more desirable outcome</strong> than a
participant from the other group.</p>
<ul>
<li><p><strong>Key Concept</strong>:<br />
It estimates the <strong>probability that a randomly chosen patient from
the experimental group has a better (or equal) outcome than one from the
control group</strong>.</p></li>
<li><p><strong>Methodology</strong>:<br />
The <strong>Wilcoxon-Mann-Whitney (WMW) statistic</strong> is used to
estimate this probability. It is a nonparametric method suitable for
<strong>ordinal outcomes</strong>, calculated by comparing all possible
patient pairs across groups and counting how often one outcome is better
than the other.</p></li>
<li><p><strong>Advantages</strong>:</p>
<ul>
<li>Captures the <strong>relative benefit</strong> of one treatment over
another at the individual level.</li>
<li>Does not rely on assumptions of normality or equal variances.</li>
</ul></li>
<li><p><strong>Note on Composite Outcomes</strong>:<br />
Since DOOR often includes composite outcomes (like “alive with 1 event”,
“alive with 2 events”, etc.), it’s helpful to <strong>break down each
component</strong> separately to explore <strong>how different events
contribute to the overall outcome</strong>.</p></li>
</ul>
<hr />
<p><strong>Grade-based Analysis Approach</strong></p>
<p>Also known as <strong>partial credit analysis</strong>, this approach
assigns <strong>numeric scores</strong> to DOOR outcome categories based
on <strong>their perceived desirability</strong>, which may vary by
patients or clinicians.</p>
<ul>
<li><p><strong>Key Concept</strong>:<br />
Treats DOOR outcomes as if they lie on a <strong>continuous 0–100
scale</strong>, where 100 represents the best outcome (e.g., “alive with
no events”) and 0 the worst (e.g., death), with intermediate outcomes
scored accordingly (e.g., partial credit for 1 or 2 adverse
events).</p></li>
<li><p><strong>Purpose</strong>:</p>
<ul>
<li>Evaluates how <strong>treatment groups differ in mean DOOR
scores</strong>, allowing a nuanced understanding of intervention
effects.</li>
<li>Uses statistical tests like <strong>Welch’s t-test</strong> to
compare group means.</li>
</ul></li>
<li><p><strong>Advantages</strong>:</p>
<ul>
<li>Reflects <strong>personalized preferences</strong> by adjusting
scores based on what patients and clinicians value.</li>
<li>Provides a <strong>more flexible and interpretable measure</strong>
of benefit-risk trade-offs.</li>
</ul></li>
</ul>
<hr />
<table>
<colgroup>
<col width="26%" />
<col width="37%" />
<col width="36%" />
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Rank-Based Analysis</th>
<th>Grade-Based Analysis</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Type of Comparison</td>
<td>Pairwise patient comparison</td>
<td>Group-level mean score comparison</td>
</tr>
<tr class="even">
<td>Statistic</td>
<td>DOOR probability (via WMW test)</td>
<td>Mean score difference (Welch’s t-test)</td>
</tr>
<tr class="odd">
<td>Outcome Scale</td>
<td>Ordinal</td>
<td>Treated as continuous (0–100 scale)</td>
</tr>
<tr class="even">
<td>Interpretability</td>
<td>Probability a patient has a better outcome</td>
<td>Average desirability score</td>
</tr>
<tr class="odd">
<td>Flexibility for Preferences</td>
<td>Limited</td>
<td>High (can reflect personalized scoring)</td>
</tr>
<tr class="even">
<td>Focus</td>
<td>Relative ranking</td>
<td>Absolute importance/utility of outcomes</td>
</tr>
</tbody>
</table>
</div>
<div id="door-analyses-rank-based-analysis"
class="section level3 unnumbered">
<h3 class="unnumbered">DOOR Analyses: Rank-based Analysis</h3>
<ol style="list-style-type: decimal">
<li>Overview: What is Rank-Based DOOR Analysis?**</li>
</ol>
<p>DOOR (Desirability of Outcome Ranking) uses <strong>rank-based
statistics</strong> to compare overall patient outcomes between
treatment groups. This method does not focus on isolated endpoints but
evaluates the <strong>probability that a patient receiving the
experimental treatment (E)</strong> has a <strong>more desirable
outcome</strong> than a patient receiving the control treatment (C). The
summary measure is called the <strong>DOOR probability</strong>.</p>
<p>The rank-based DOOR analysis provides a <strong>flexible,
interpretable, and robust</strong> way to compare treatments in clinical
trials by:</p>
<ul>
<li>Integrating efficacy, safety, and quality-of-life into a single
ranking</li>
<li>Quantifying how often patients in one arm have better outcomes than
those in the other</li>
<li>Offering statistical tools (CI, hypothesis testing, p-values) for
inference</li>
<li>Allowing adjustment for ties and approximation methods when data are
less ideal</li>
</ul>
<p>This method supports <strong>patient-centered</strong>,
<strong>holistic decision-making</strong> and has become an increasingly
favored analytic approach in benefit-risk evaluations.</p>
<p><strong>2. Estimating DOOR Probability</strong></p>
<p>The DOOR probability is estimated using the
<strong>Wilcoxon-Mann-Whitney (WMW)</strong> statistic. This
nonparametric method compares every patient in group E to every patient
in group C and assigns scores based on outcome rankings:</p>
<p><span class="math display">\[
\hat{\pi}_{E \geq C} = \frac{1}{n_E n_C} \sum_{i=1}^{n_E}
\sum_{j=1}^{n_C} \phi(y_i^E, y_j^C)
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\phi(y_i^E, y_j^C)\)</span> is:
<ul>
<li>1 if <span class="math inline">\(y_i^E &gt; y_j^C\)</span></li>
<li>½ if <span class="math inline">\(y_i^E = y_j^C\)</span></li>
<li>0 if <span class="math inline">\(y_i^E &lt; y_j^C\)</span></li>
</ul></li>
</ul>
<p>This represents the average probability that a randomly chosen
patient from the experimental group has a <strong>better or
equal</strong> outcome compared to a patient from the control group.</p>
<p><strong>3. Interpretation of DOOR Probability</strong></p>
<p><span class="math display">\[
\pi_{E \geq C} = P[Y^E &gt; Y^C] + \frac{1}{2}P[Y^E = Y^C]
\]</span></p>
<ul>
<li>If <span class="math inline">\(\pi_{E \geq C} &gt; 0.5\)</span>: The
experimental treatment is <strong>more desirable</strong> overall.</li>
<li>If <span class="math inline">\(\pi_{E \geq C} &lt; 0.5\)</span>: The
control is <strong>more desirable</strong>.</li>
<li><span class="math inline">\(\pi_{C \geq E} = 1 - \pi_{E \geq
C}\)</span></li>
</ul>
<p>This approach:</p>
<ul>
<li>Accounts for <strong>both central tendency and dispersion</strong>
(Simonoff et al., 1986)</li>
<li>Reflects a <strong>population-level causal effect</strong> rather
than individual-level effects (Fay et al., 2018)</li>
</ul>
<p><strong>4. Confidence Interval (CI) Methods for DOOR
Probability</strong></p>
<p>There are several ways to estimate the CI for DOOR probability, each
with pros and cons:</p>
<table style="width:100%;">
<colgroup>
<col width="20%" />
<col width="52%" />
<col width="26%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Method</strong></th>
<th><strong>Feature</strong></th>
<th><strong>Reference</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Wald-type CI</td>
<td>Easy to construct, symmetric, but may exceed [0,1] in extreme
cases</td>
<td>Ryu &amp; Agresti, 2008</td>
</tr>
<tr class="even">
<td>Halperin et al. (1989)</td>
<td>Easy to construct, asymmetric CI using quadratic inequality</td>
<td>Halperin et al., 1989</td>
</tr>
<tr class="odd">
<td>Logit transformation-based CI</td>
<td>Uses logit scale for CI, then transforms back; handles
asymmetry</td>
<td>Edwardes, 1995</td>
</tr>
<tr class="even">
<td>Score/Pseudo-score/Likelihood</td>
<td>More accurate, handles asymmetry; computationally more
demanding</td>
<td>Ryu &amp; Agresti, 2008</td>
</tr>
<tr class="odd">
<td>Bootstrap</td>
<td>Flexible, but computationally intensive</td>
<td>van Duin et al., CID 2018</td>
</tr>
</tbody>
</table>
<p><strong>5. Hypothesis Testing for DOOR Probability</strong></p>
<p>Used to test whether the experimental treatment is statistically
superior:</p>
<ul>
<li><p><strong>One-sided</strong>:<br />
<span class="math inline">\(H_0: \pi_{E \geq C} \leq
\delta_0\)</span><br />
<span class="math inline">\(H_1: \pi_{E \geq C} &gt;
\delta_0\)</span></p></li>
<li><p><strong>Two-sided</strong>:<br />
<span class="math inline">\(H_0: \pi_{E \geq C} =
\delta_0\)</span><br />
<span class="math inline">\(H_1: \pi_{E \geq C} \ne
\delta_0\)</span><br />
where <span class="math inline">\(\delta_0 = 0.5\)</span> is typically
used.</p></li>
</ul>
<p><span class="math display">\[
z_{WMW} = \frac{\hat{\pi}_{E \geq C} - \delta_0}{\sqrt{\hat{V}_0}}
\]</span></p>
<ul>
<li><span class="math inline">\(\hat{V}_0\)</span> is the variance
estimate under the null hypothesis (see Lehmann &amp; D’Abrera,
1975).</li>
<li>Reject <span class="math inline">\(H_0\)</span> if the
<strong>p-value is below α (e.g., 0.05)</strong>.</li>
</ul>
<p><strong>6. Calculating P-Values</strong></p>
<p>Two approaches:</p>
<ul>
<li><strong>One-sided</strong>:<br />
<span class="math inline">\(p_{\text{1sided}}(z_{WMW}) = \Pr[Z &gt;
z_{WMW}]\)</span></li>
<li><strong>Two-sided</strong>:<br />
<span class="math inline">\(p_{\text{2sided}}(z_{WMW}) = \Pr[|Z| &gt;
|z_{WMW}|]\)</span></li>
</ul>
<p>When outcomes are heavily tied or sample sizes are small:</p>
<ul>
<li><strong>Continuity correction</strong>: Adjust the statistic by
0.5</li>
<li><strong>t-approximation</strong>: Use a t-distribution with <span
class="math inline">\(n_E + n_C - 1\)</span> degrees of freedom instead
of the normal distribution</li>
</ul>
<p><img src="02_Plots/Safety%20Evaluation/DOOR_Issues1.png" /></p>
</div>
<div id="door-analyses-grade-based-analysis-partial-credit-analysis"
class="section level3 unnumbered">
<h3 class="unnumbered">DOOR Analyses: Grade-based Analysis (Partial
Credit Analysis)</h3>
<p>Unlike rank-based DOOR analysis, which simply orders patient
outcomes, <strong>grade-based analysis assigns specific scores</strong>
to each outcome category to reflect <strong>their clinical and
patient-perceived importance</strong>. This method translates ordinal
categories into a <strong>continuous score</strong> on a 0–100 scale,
allowing more nuanced comparisons between groups.</p>
<p><strong>1. Assign Scores to DOOR Categories</strong></p>
<p>Each DOOR rank category is assigned a score that reflects its
desirability:</p>
<table>
<thead>
<tr class="header">
<th><strong>DOOR Rank Category</strong></th>
<th><strong>Score</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Alive with no events</td>
<td>100</td>
</tr>
<tr class="even">
<td>Alive with 1 event</td>
<td>Partial Score 1 (0 &lt; S₁ ≤ 100)</td>
</tr>
<tr class="odd">
<td>Alive with 2 events</td>
<td>Partial Score 2 (0 &lt; S₂ ≤ S₁)</td>
</tr>
<tr class="even">
<td>Alive with 3 events</td>
<td>Partial Score 3 (0 &lt; S₃ ≤ S₂)</td>
</tr>
<tr class="odd">
<td>Death</td>
<td>0</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Partial Scores</strong> (S₁, S₂, S₃) are chosen based on:
<ul>
<li>Clinical judgment</li>
<li>Patient values</li>
<li>Strategic differentiation between outcomes</li>
</ul></li>
</ul>
<p>This design provides <strong>flexibility</strong>, allowing the
analysis to reflect various stakeholder perspectives (e.g., patients
might value mild side effects differently from clinicians).</p>
<p><strong>2. Analyze Scores as Continuous Outcomes</strong></p>
<p>Once every patient has been assigned a score based on their DOOR
outcome:</p>
<ul>
<li>The scores are analyzed <strong>as if they were continuous</strong>
(like a patient satisfaction score).</li>
<li>A two-sample comparison is then performed, usually with
<strong>Welch’s t-test</strong> (which allows for unequal
variances).</li>
</ul>
<p>The result is an <strong>estimated difference in mean DOOR
scores</strong> between the treatment arms (e.g., Remdesivir
vs. Placebo).</p>
<p><img src="02_Plots/Safety%20Evaluation/DOOR_Score.png" /></p>
<p><strong>3. What Does Partial Credit Help With?</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Strategic Spacing</strong><br />
It allows deliberate differences in scores between categories—e.g.,
death vs. survival with one adverse event may be weighted much more
heavily than 1 event vs. 2 events.</p></li>
<li><p><strong>Personalized Interpretation</strong><br />
Customizes the analysis for how <strong>patients and clinicians</strong>
value trade-offs in outcomes.</p></li>
<li><p><strong>Robustness Checks</strong><br />
Analysts can test how results change under different partial credit
assumptions to assess the stability of conclusions.</p></li>
</ol>
</div>
</div>
<div id="mcda-multi-criteria-decision-analysis" class="section level2"
number="5.2">
<h2><span class="header-section-number">5.2</span> MCDA (Multi-Criteria
Decision Analysis)</h2>
</div>
</div>
<div id="laboratory" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Laboratory</h1>
<div id="individual-studies" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Individual
Studies</h2>
<p>Laboratory data should be presented using a combination of visual and
tabular displays that are clear, clinically meaningful, and consistent
across treatment arms. The primary display format recommended is a
<strong>three-panel figure</strong>. The <strong>top panel</strong> is a
<strong>box plot</strong> showing observed laboratory values over time.
The box plot includes the median, mean (as white dots), interquartile
range (25th–75th percentiles), and whiskers at the 5th and 95th
percentiles. Individual participant data points are overlaid and
color-coded—<strong>red</strong> for values above the upper limit of
normal (ULN), <strong>blue</strong> for values below the lower limit of
normal (LLN), and <strong>gray</strong> for values within range. These
colors reflect subject-specific reference ranges, which can vary by
demographic factors.</p>
<p>The <strong>middle panel</strong> is a <strong>summary text
line</strong> for each time point, showing the number of observations
and the counts of high or low values. These counts are color-coded to
match the plot and provide a quick numerical summary. Unlike earlier
versions, detailed statistics like mean, standard deviation, min, and
max are not recommended in this display to conserve space and allow more
time points to be shown.</p>
<p>The <strong>bottom panel</strong> is a <strong>line plot</strong>
displaying the group means over time with 95% confidence intervals. This
addition helps reviewers compare trends across treatment arms more
easily and complements the box plot above.</p>
<p>In addition to observed value plots, <strong>change-from-baseline box
plots</strong> (similar format) are recommended. These do not include
reference limit coloring or counts of high/low, since there are no
defined thresholds for change values. Importantly, the practice of
including “change from baseline to last observation” has been
discouraged, as it has limited value in identifying safety signals.</p>
<p>For <strong>shift analyses</strong>, separate <strong>scatter
plots</strong> should be used to show shifts from baseline to
post-baseline values. <strong>Figure 6.3</strong> focuses on
<strong>maximum values</strong>, and <strong>Figure 6.4</strong> on
<strong>minimum values</strong>. Each treatment group should be shown in
a <strong>separate panel</strong> with identical axes for comparability.
Reference lines for ULN/LLN are removed due to population variability,
and instead, special <strong>symbols (e.g., stars)</strong> indicate
subjects who shifted from normal to abnormal ranges.</p>
<p>A <strong>summary shift table (Table 6.1)</strong> is used to
quantify these shifts, displaying the percentage of participants moving
between normal and abnormal categories, with comparisons between
treatment arms including 95% confidence intervals. Related lab analytes
should be grouped together to support integrated interpretation.</p>
<p>For <strong>qualitative or ordinal lab analytes</strong> (e.g.,
“normal/abnormal” or “+/++/+++”), a similar summary table format is
used. These focus on the shift from “normal” at baseline to “abnormal”
post-baseline, without distinguishing between degrees of
abnormality.</p>
<p>Finally, to ensure comprehensive review, <strong>a listing
format</strong> is recommended for participants who <strong>lack
baseline values</strong> but have abnormal post-baseline results. This
ensures that potential safety signals are not overlooked due to missing
baseline data. The listing should include any abnormal post-baseline
values for such subjects.</p>
<p><img src="02_Plots/Visualization/LB/BOX_Raw.png" /></p>
<p><img src="02_Plots/Visualization/LB/BOX_CFB.png" /></p>
<p><img src="02_Plots/Visualization/LB/Scatter_Max_min.png" /></p>
<p><img src="02_Plots/Visualization/LB/Table_High_Low.png" /></p>
<p><img src="02_Plots/Visualization/LB/Listing.png" /></p>
</div>
<div id="integrated-summaries" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Integrated
Summaries</h2>
<p>In integrated summaries of laboratory data across multiple studies,
the recommended approach focuses on simplified and consolidated displays
to support cross-study safety evaluation. Unlike individual study
presentations, which emphasize time trends through box plots and line
plots, integrated summaries prioritize <strong>minimum and maximum
values</strong> observed during baseline and post-baseline periods. A
key display is <strong>Table 6.2</strong>, which summarizes these
extremes along with <strong>changes from baseline</strong>, providing
<strong>group-level statistics</strong> (mean, standard deviation) and
<strong>treatment comparison metrics</strong> adjusted for study effect
with 95% confidence intervals. Changes are typically used as the modeled
outcome, but minimum or maximum post-baseline values can also serve as
alternatives. Related analytes are grouped to assist clinical review.
While box plots could be used in integrated summaries when visit
schedules are consistent across studies, the recommendation is to rely
on <strong>summary tables</strong> instead, as integrated box plots may
obscure study-specific patterns and offer limited added value beyond
individual study plots.</p>
<p>For <strong>shift analyses</strong>, the updated guidance proposes a
single <strong>summary table (Table 6.3)</strong> that captures shifts
from low/normal to high and from high/normal to low for all lab analytes
in one consolidated format, replacing the prior approach that separated
analytes by category (e.g., metabolic, renal) and included multiple sets
of box plots and scatter plots. The rationale is to avoid redundancy and
potential confusion introduced by pooled scatter plots, which may
conflate diverse study-level effects. Instead, the emphasis is on the
summary table’s ability to clearly highlight group-level imbalances.</p>
<p>For <strong>qualitative lab measures</strong> (e.g., normal/abnormal,
or ordinal values like “+”, “++”, etc.), a similar table format is
recommended. Here, only the shift from <strong>baseline normal to
post-baseline abnormal</strong> is evaluated, aligning with previous
recommendations. Finally, integrated summaries are intended to
<strong>complement</strong>, not duplicate, the individual study
displays—by combining time-based visuals at the study level with concise
summary statistics at the integrated level, reviewers are provided a
balanced and efficient safety assessment framework.</p>
<p><img src="02_Plots/Visualization/LB/Table_Summary.png" /></p>
<p><img src="02_Plots/Visualization/LB/Table_Shift.png" /></p>
</div>
</div>
<div id="referencwe" class="section level1" number="7">
<h1><span class="header-section-number">7</span> Referencwe</h1>
<p>PHUSE. (2017). Analyses &amp; Displays Associated with Adverse
Events: Focus on Adverse Events in Phase 2–4 Clinical Trials and
Integrated Summary [White paper]. PhRMA. Available as: Analyses and
Displays Associated with Adverse Events Focus on Adverse Events in Phase
2-4 Clinical Trials and Integrated Summary.pdf</p>
<p>PHUSE. (2015). Analyses &amp; Displays Associated with Outliers or
Shifts from Normal to Abnormal: Focus on Vital Signs, Electrocardiogram
&amp; Laboratory Analyte Measurements in Phase 2–4 Clinical Trials and
Integrated Summary [White paper]. PhRMA. Available as: Analyses &amp;
Displays Associated with Outliers or Shifts from Normal To Abnormal
Focus on Vital Signes &amp; Electrocardiogram &amp; Laboratory Analyte
Measurements in Phase 2-4 Clinical Trials and Integrated Summary.pdf</p>
<p>PHUSE. (2013). Analyses &amp; Displays Associated with Measures of
Central Tendency - Focus on Vital Sign, Electrocardiogram, &amp;
Laboratory Analyte Measurements in Phase 2-4 Clinical Trials &amp;
Integrated Submission Documents. [serial online]. Available from: <a
href="https://phuse.s3.eu-central-1.amazonaws.com/Deliverables/Standard+Analyses+and+Code+Sharing/Analyses+%26+Displays+Associated+with+Measures+of+Central+Tendency-+Focus+on+Vital+Sign,+Electrocardiogram+%26+Laboratory+Analyte+Measurements+in-"
class="uri">https://phuse.s3.eu-central-1.amazonaws.com/Deliverables/Standard+Analyses+and+Code+Sharing/Analyses+%26+Displays+Associated+with+Measures+of+Central+Tendency-+Focus+on+Vital+Sign,+Electrocardiogram+%26+Laboratory+Analyte+Measurements+in-</a>
+Phase+2-4+Clinical+Trials+and+Integrated+Submissions.pdf</p>
<p>PHUSE (2015). Analyses and Displays Associated with Outliers or
Shifts from Normal to Abnormal: Focus on Vital Signs, Electrocardiogram,
and Laboratory Analyte Measurements in Phase 2-4 Clinical Trials and
Integrated Summary Documents. [serial online]. Available from: <a
href="https://phuse.s3.eu-central-1.amazonaws.com/Deliverables/Standard+Analyses+and+Code+Sharing/Analyses+%26+Displays+Associated+with+Outliers+or+Shifts+from+Normal+To+Abnormal+Focus+on+Vital+Signes+%26+Electrocardiogram+%26+Laboratory+Analyte+Measurements+in+Phase+2-4+Clinical+Trials+and+Integrated+Summary.pdf"
class="uri">https://phuse.s3.eu-central-1.amazonaws.com/Deliverables/Standard+Analyses+and+Code+Sharing/Analyses+%26+Displays+Associated+with+Outliers+or+Shifts+from+Normal+To+Abnormal+Focus+on+Vital+Signes+%26+Electrocardiogram+%26+Laboratory+Analyte+Measurements+in+Phase+2-4+Clinical+Trials+and+Integrated+Summary.pdf</a></p>
<div id="bssd-1" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> BSSD</h2>
<ol style="list-style-type: decimal">
<li><p>Amit O, Heiberger R, Lane P. Graphical approaches to the analysis
of safety data from clinical trials. <em>Pharm Stat</em>.
2008;7(1):20–35.</p></li>
<li><p>Crowe B, et al. Recommendations for safety planning, data
collection, evaluation and reporting during drug, biologic and vaccine
development: <em>A Report of the Safety Planning, Evaluation and
Reporting Team</em>. <em>Clin Trials</em>. 2009;6:430–440.</p></li>
<li><p>Crowe B, Chuang-Stein C, Lettis S, Brueckner A. Reporting adverse
drug reactions in product labels. <em>Ther Innov Regul Sci</em>.
2016;50(4):455–463.</p></li>
<li><p>FDA CDER CBER. <em>Guidance for Industry: Good Pharmacovigilance
Practices and Pharmacoepidemiologic Assessment</em>. Food and Drug
Administration, Silver Spring, MD; 2005.</p></li>
<li><p>FDA CDER CBER. <em>Guidance for Industry and Investigators:
Safety Reporting Requirements for INDs and BA/BE Studies</em>. Food and
Drug Administration, Silver Spring, MD; 2012.</p></li>
<li><p>FDA CDER CBER. <em>Draft Guidance for Industry: Safety Assessment
for IND Safety Reporting</em>. Food and Drug Administration, Silver
Spring, MD; 2015.</p></li>
<li><p>Investigational New Drug Application. <em>Code of Federal
Regulations</em>. Title 21, Vol 5, Section 312.32 IND Safety Reporting.
2018. [21CFR312.32].</p></li>
<li><p>Council for International Organizations of Medical Sciences
(CIOMS). <em>Management of Safety Information from Clinical Trials:
Report of CIOMS Working Group VI</em>. Geneva, Switzerland: CIOMS;
2005.</p></li>
<li><p>Council for International Organizations of Medical Sciences
(CIOMS) Working Group VIII. <em>Practical Aspects of Signal Detection in
Pharmacovigilance: Report of CIOMS Working Group VIII</em>. Geneva,
Switzerland: CIOMS; 2010.</p></li>
<li><p>Schnell PM, Ball G. A Bayesian exposure-time method for clinical
trial safety monitoring with blinded data. <em>Therapeutic Innovation
&amp; Regulatory Science</em>. 2016;50(6):833–838. <a
href="doi:10.1177/2168479016659015"
class="uri">doi:10.1177/2168479016659015</a></p></li>
<li><p>Mukhopadhyay S, Waterhouse B, Hartford A. Bayesian detection of
potential risk using inference on blinded safety data. <em>Pharm
Stat</em>. 2018;17(6):823–834. <a href="doi:10.1002/pst.1909"
class="uri">doi:10.1002/pst.1909</a></p></li>
<li><p>Kerman J. Neutral non-informative and informative conjugate Beta
and Gamma prior distributions. <em>Electron J Stat</em>.
2011;5:1450–1470. <a href="doi:10.1214/11-EJS648"
class="uri">doi:10.1214/11-EJS648</a></p></li>
<li><p>Collett D. <em>Modelling Binary Data</em>. 2nd ed. Chapman &amp;
Hall/CRC; 2002.</p></li>
<li><p>Wen S, Dey S. Bayesian monitoring of safety signals in blinded
clinical trial data. <em>Ann Public Health Res</em>.
2015;2(2):1019.</p></li>
<li><p>Yao B, Zhu L, Jiang Q, Xia A. Safety monitoring in clinical
trials. <em>Pharmaceutics</em>. 2013;5:94–106. <a
href="doi:10.3390/pharmaceutics5010094"
class="uri">doi:10.3390/pharmaceutics5010094</a></p></li>
<li><p>Waterhouse et al. (2020/2022), <em>“Using the BDRIBS method to
support the decision to refer an event for unblinded evaluation,”</em>
<strong>Pharmaceutical Statistics</strong>, 21(2):372-385.</p></li>
</ol>
</div>
<div id="exposure-adjusted-incidence-rates-eairs-1"
class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Exposure-Adjusted
Incidence Rates (EAIRs)</h2>
<p>Crowe B, Chang-Stein C, Lettis S, Brueckner A. Reporting Adverse Drug
Reactions in Product Labels. Therapeutic Innovation &amp; Regulatory
Science, 2016; 50(4):455-463</p>
<p>Fay MP, Feuer EJ. Confidence intervals for directly standardized
rates: A method based on the gamma distribution. Statistics in Medicine
1997; 16(7):791-801</p>
<p>Kraemer HC. Events per person-time (incidence rate): A misleading
statistic. Statistics in Medicine, 2009; 28:1028–1039</p>
<p>Rücker G, Schumacher M. Simpson’s paradox visualized: The example of
the Rosiglitazone meta-analysis. BMC Medical Research Methodology 2008;
8(34):18-20</p>
<p>Ulm K., A simple method to calculate the confidence interval of a
standardized mortality ratio. American Journal of Epidemiology, 1990;
131(12):373-375</p>
<p>Zhou Y, Ke C, Jiang Q, Shahin S, Snapinn S. Choosing Appropriate
Metrics to Evaluate Adverse Events in Safety Evaluation. Therapeutic
Innovation &amp; Regulatory Science, 2015; 49(3):398-404</p>
<p>R Core Team (2023). <em>R: A Language and Environment for Statistical
Computing</em>. R Foundation for Statistical Computing, Vienna, Austria.
<a href="https://www.R-project.org/"
class="uri">https://www.R-project.org/</a></p>
<p>H. Wickham (2016). ggplot2: Elegant Graphics for Data Analysis.
Springer-Verlag New York. <a href="https://ggplot2.tidyverse.org"
class="uri">https://ggplot2.tidyverse.org</a></p>
<p>Zhu J, Sabanés Bové D, Stoilova J, Garolini D, de la Rua E,
Yogasekaram A, Wang H, Collin F, Waddell A, Rucki P, Liao C, Li J
(2024). <em>tern: Create Common TLGs Used in Clinical Trials</em>. R
package version 0.9.6. <a href="https://CRAN.R-project.org/package=tern"
class="uri">https://CRAN.R-project.org/package=tern</a></p>
</div>
<div id="door" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> DOOR</h2>
<p>Toshimitsu Hamasaki, Daniel Rubin and Scott. R Evans. DISS short
course(2024):The DOOR is Open-Pragmatic Benefit:Risk Evaluation Using
Outcomes to Analyze Patients Rather than Patients to Analyze
Outcomes​</p>
<p>Evans, S. R., Follmann, D., &amp; Powers, J. H. (2015).”Desirability
of outcome ranking (DOOR) and response adjusted for duration of
antibiotic risk (RADAR).”Clinical Infectious Diseases, 61(5),
800-806.DOI: 10.1093/cid/civ495​</p>
<p>Beigel, J. H., Tomashek, K. M., Dodd, L. E., Mehta, A. K., Zingman,
B. S., Kalil, A. C., … &amp; ACTT-1 Study Group Members. (2020).
Remdesivir for the treatment of Covid-19 — Final report. The New England
Journal of Medicine, 383(19), 1813–1826. <a
href="https://doi.org/10.1056/NEJMoa2007764"
class="uri">https://doi.org/10.1056/NEJMoa2007764</a>​</p>
<p><a
href="https://arlg.org/desirability-of-outcome-ranking-door/">Desirability
of Outcome Ranking (DOOR) | ARLG</a></p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
