<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title> MRMC Study: Design and Evaluation</title>

<script src="site_libs/header-attrs-2.29/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.5.2/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Zehui Bai</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
    Home
  </a>
</li>
<li>
  <a href="02-Clinical_Experience.html">
    <span class="fa fa-user"></span>
     
    Clinical Experience
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-sliders"></span>
     
    Sample Size
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="03-SSC-Everything-to-Know-About-Sample-Size-Determination.html">Everything to Know About Sample Size Determination</a>
    </li>
    <li>
      <a href="03-SSC-Choosing-the-Effect-Size-for-Sample-Size-Calculations.html">Choosing the Effect Size</a>
    </li>
    <li>
      <a href="03-SSC-Biosimilar-Trials.html">Statistical Considerations for the Design and Analysis of Biosimilar Trials</a>
    </li>
    <li>
      <a href="03-SSC-Sample-Size-and-Power-for-Non-Parametric-Analysis.html">Sample Size and Power for Non-Parametric Analysis</a>
    </li>
    <li>
      <a href="03-SSC-Power-for-Complex-Hypotheses.html">Power for Complex Hypotheses</a>
    </li>
    <li>
      <a href="03-SSC-Alternatives-to-Power.html">Bayesian methods - Alternatives to Power</a>
    </li>
    <li>
      <a href="03-SSC-Sample-Size-for-Pilot-Studies.html">Sample Size for Pilot Studies</a>
    </li>
    <li>
      <a href="03-SSC-Case-Continuous-Endpoint.html">Sample Size Determination for Continuous Endpoint</a>
    </li>
    <li>
      <a href="03-SSC-Case-Categorical-Endpoint.html">Sample Size Determination for Categorical Endpoint</a>
    </li>
    <li>
      <a href="03-SSC-Sample-Size-Determination-for-Counts-and-Rates.html">Sample Size Determination for Counts and Rates</a>
    </li>
    <li>
      <a href="03-SSC-Case-Survival-Endpoint.html">Sample Size Determination for Survival Endpoint</a>
    </li>
    <li>
      <a href="03-SSC-Case-Repeated-Measures.html">Sample Size Determination for Repeated Measures</a>
    </li>
    <li>
      <a href="03-SSC-IA-Sequential-Design.html">Statistical Considerations for Group Sequential Design</a>
    </li>
    <li>
      <a href="03-SSC-IA-Adaptive-Design.html">Statistical Considerations for Adaptive Design</a>
    </li>
    <li>
      <a href="03-SSC-Multiple-Test.html">Sample Size for Multiple Test</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-pencil-square-o"></span>
     
    Study Design
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="04-Design-General-Consideration.html">General Consideration</a>
    </li>
    <li>
      <a href="04-Design-Estimands-Framework.html">Estimands Framework</a>
    </li>
    <li>
      <a href="04-Design-Estimands-Practice.html">Estimands and Sensitivity Analyses</a>
    </li>
    <li>
      <a href="04-Design-Phase-I-Trials---Design-Considerations.html">Phase I Trials - Design Considerations</a>
    </li>
    <li>
      <a href="04-Design-Phase-II-Trials---Design-Considerations.html">Phase II Trials - Design Considerations</a>
    </li>
    <li>
      <a href="04-Design-Phase-III-Trials---Design-Considerations.html">Phase III Trials - Design Considerations</a>
    </li>
    <li>
      <a href="04-Design-Phase-IV-Trials---Design-Considerations.html">Phase IV Trials - Design Considerations for Post Marketing Surveillance</a>
    </li>
    <li>
      <a href="04-Design-Complex-Sequential-Trials.html">Complex Sequential Analysis Trials</a>
    </li>
    <li>
      <a href="04-Design-Adaptive-Clinical-Trials.html">Adaptive Clinical Trials</a>
    </li>
    <li>
      <a href="04-Design-Bayesian-Clinical-Trials.html">Bayesian Clinical Trials</a>
    </li>
    <li>
      <a href="04-Design-Dose-Escalation-and-Stratification-Designs.html">Dose Escalation and Stratification Designs in Early Oncology Development</a>
    </li>
    <li>
      <a href="04-Design-Single-Arm-Clinical-Trials.html">Single Arm Clinical Trials</a>
    </li>
    <li>
      <a href="04-Design-Diagnostic-Study-Design-and-Evaluation.html">Diagnostic Study-Design and Evaluation</a>
    </li>
    <li>
      <a href="04-Design-Diagnostic-Study-MRMC.html">Diagnostic Study-Multireader Multicase (MRMC)</a>
    </li>
    <li>
      <a href="04-Design-Vaccine-Design.html">Vaccine Trials</a>
    </li>
    <li>
      <a href="04-Design-Externally-Controlled-Trials.html">Externally Controlled Trials</a>
    </li>
    <li>
      <a href="04-Design-Noninferiority-Trials.html">Noninferiority Trials</a>
    </li>
    <li>
      <a href="04-Design-Bioequivalence-and-Biosimilar-Trials.html">Bioequivalence and Biosimilar Trials</a>
    </li>
    <li>
      <a href="04-Design-Exploring-Survival-Analysis-Designs-for-Clinical-Trials.html">Exploring Survival Analysis Designs for Clinical Trials</a>
    </li>
    <li>
      <a href="04-Design-Hierarchical-composite-endpoints.html">Hierarchical Composite Endpoints</a>
    </li>
    <li>
      <a href="04-Design-Regulatory-Submission.html">Regulatory Submission from Stats Perspective</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-line-chart"></span>
     
    Data Visualization
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="05-Plot-Time-To-Event.html">Time to Event Data Visualization</a>
    </li>
    <li>
      <a href="05-Plot-PRO-Data.html">Patient Reported Outcome Data Visualization</a>
    </li>
    <li>
      <a href="05-Plot-SSC-and-Power-Calculation.html">Sample Size and Power Calculations Visualization</a>
    </li>
    <li>
      <a href="05-Plot-Correlation.html">Correlation Visualization</a>
    </li>
    <li>
      <a href="05-Plot-Model-Table.html">Clinical Data and Model visualization</a>
    </li>
    <li>
      <a href="05-Plot-ScatterPlot.html">Scatter and Line Plot</a>
    </li>
    <li>
      <a href="05-Plot-BarPlot.html">Bar Chart</a>
    </li>
    <li>
      <a href="05-Plot-PieChart.html">Pie Chart</a>
    </li>
    <li>
      <a href="05-Plot-BoxPlot.html">Box Plot</a>
    </li>
    <li>
      <a href="05-Plot-Histogram.html">Histogram</a>
    </li>
    <li>
      <a href="05-Plot-Forest-Plot.html">Forest Plot</a>
    </li>
    <li>
      <a href="05-Plot-Flow-Chart.html">Flow Chart</a>
    </li>
    <li>
      <a href="05-Plot-Some-Interesting.html">Some Interesting Plots</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-calculator"></span>
     
    Statistical Analysis
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="06-Analysis-Linear-Regression.html">Linear Regression</a>
    </li>
    <li>
      <a href="06-Analysis-Logistic-Regression.html">Logistic Regression</a>
    </li>
    <li>
      <a href="06-Analysis-Mixed-Model.html">Mixed Model</a>
    </li>
    <li>
      <a href="06-Analysis-MMRM.html">Mixed Model Repeated Measures</a>
    </li>
    <li>
      <a href="06-Analysis-GEE.html">Generalized Estimating Equation</a>
    </li>
    <li>
      <a href="06-Analysis-ANOVA.html">Analysis of Variance</a>
    </li>
    <li>
      <a href="06-Analysis-Survival-Analysis.html">Survival Analysis</a>
    </li>
    <li>
      <a href="06-Analysis-Survival-Competing-Risk.html">Survival Analysis - Competing Risk</a>
    </li>
    <li>
      <a href="06-Analysis-Missing-Data.html">Missing Data Analysis</a>
    </li>
    <li>
      <a href="06-Analysis-MI-Implementation.html">Multiple Imputation Analysis</a>
    </li>
    <li>
      <a href="06-Analysis-SSD.html">Safety Signal Detection and Evaluation</a>
    </li>
    <li>
      <a href="06-Analysis-Meta-Analysis.html">Meta Analysis</a>
    </li>
    <li>
      <a href="06-Analysis-PK-and-PD.html">PK and PD Analysis</a>
    </li>
    <li>
      <a href="06-Analysis-Time-Series-Analysis.html">Time Series Analysis</a>
    </li>
    <li>
      <a href="06-Analysis-SEM.html">Structural Equation Modeling</a>
    </li>
    <li>
      <a href="06-Analysis-Factor-Analysis.html">Factor Analysis</a>
    </li>
    <li>
      <a href="06-Analysis-Propensity-Score.html">Propensity Score Analysis</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-bar-chart"></span>
     
    Machine Learning
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="07-ML-Bayesian-Theory.html">Bayesian Theory</a>
    </li>
    <li>
      <a href="07-ML-Bayesian-Analysis.html">Bayesian Analysis</a>
    </li>
    <li>
      <a href="07-ML-Regularization-Penalized-Regression.html">Regularization Penalized Regression</a>
    </li>
    <li>
      <a href="07-ML-Loss-Regression.html">Loss Functions in Machine Learning</a>
    </li>
    <li>
      <a href="07-ML-PCA.html">Principal Component Analysis</a>
    </li>
    <li>
      <a href="07-ML-KNN.html">K-Nearest Neighbors</a>
    </li>
    <li>
      <a href="07-ML-SVM.html">Support Vector Machine</a>
    </li>
    <li>
      <a href="07-ML-Tree-Models.html">Tree Models</a>
    </li>
    <li>
      <a href="07-ML-LDA.html">Linear Discriminant Analysis</a>
    </li>
    <li>
      <a href="07-ML-Cluster-Analysis.html">Cluster Analysis</a>
    </li>
    <li>
      <a href="07-ML-Neural-Networks.html">Neural Network</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-stethoscope"></span>
     
    Clinical Analysis (ADaM)
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="08-Clinical-Disposition-Baseline.html">Disposition and Baseline</a>
    </li>
    <li>
      <a href="08-Clinical-Efficacy.html">Efficacy Analysis</a>
    </li>
    <li>
      <a href="08-Clinical-Pharmacokinetics-Analysis.html">Pharmacokinetics Analysis</a>
    </li>
    <li>
      <a href="08-Clinical-Adverse-Event.html">Adverse Event Visualization</a>
    </li>
    <li>
      <a href="08-Clinical-Laboratory-Data.html">Laboratory Data Visualization</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="08-CV.html">
    <span class="fa fa-file-pdf-o"></span>
     
    CV
  </a>
</li>
<li>
  <a href="mailto:zehuibai@outlook.com">
    <span class="fa fa-envelope-o"></span>
     
    Contact me
  </a>
</li>
<li>
  <a href="https://github.com/Zehuibai">
    <span class="fa fa-github"></span>
     
    GitHub
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore"><p><img src="logo.png"
style="width:3in" /><br />
MRMC Study: Design and Evaluation</p></h1>

</div>


<div id="roc-paradigm" class="section level1" number="1">
<h1><span class="header-section-number">1</span> ROC Paradigm</h1>
<div id="four-paradigm" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Four Paradigm</h2>
<p><strong>Measuring observer performance: four paradigms</strong></p>
<p>A given paradigm can lend itself to different analyses. In historical
order the paradigms are:</p>
<ul>
<li><ol style="list-style-type: decimal">
<li>the receiver operating characteristic (ROC) paradigm;</li>
</ol></li>
<li><ol start="2" style="list-style-type: decimal">
<li>the free-response ROC (FROC) paradigm;</li>
</ol></li>
<li><ol start="3" style="list-style-type: decimal">
<li>the location ROC (LROC) paradigm</li>
</ol></li>
<li><ol start="4" style="list-style-type: decimal">
<li>the region of interest (ROI) paradigm.</li>
</ol></li>
</ul>
<p>Each paradigm assumes that the truth is known independently of the
modalities to be compared. This implies that one cannot use diagnoses
from one of the modalities to define truth – if one did, the measurement
would be biased in favor of the modality used to define truth. It is
also assumed that the true disease status of the image is known to the
researcher but the radiologist is “blinded” to this information.</p>
<p><strong>(1) the receiver operating characteristic (ROC)
paradigm</strong></p>
<p>In the ROC paradigm the observer renders a single decision per image.
The decision could be communicated using a binary scale (ex. 0 or 1) or
declared by use of the terms “negative” or “positive,” abbreviations of
“negative for disease” (the radiologist believes the patient is
non-diseased) and “positive for disease” (the radiologist believes the
patient is diseased), respectively. Alternatively, the radiologist could
give an ordered numeric label, termed a rating, to each case where the
rating is a number with the property that higher values correspond to
greater radiologist’s confidence in presence of disease. A suitable
ratings scale could be the consecutive integers 1 through 6, where “1”
is “definitely non-diseased” and “6” is “definitely diseased”.</p>
<p>If data is acquired on a binary scale, then the performance of the
radiologist can be plotted as a single operating point on an ROC plot.
The x-axis of the plot is false positive fraction (FPF), i.e., the
fraction of non-diseased cases incorrectly diagnosed as diseased. The
y-axis of the plot is true positive fraction (TPF), i.e., the fraction
of diseased cases correctly diagnosed as diseased.</p>
<p><strong>(2) the free-response ROC (FROC) paradigm</strong></p>
<p>In the FROC paradigm the observer marks and rates all regions in the
image that are sufficiently suspicious for disease. A mark is the
location of the suspicious region and the rating is an ordered label,
characterizing the degree of suspicion attached to the suspicious
region.</p>
<p><strong>(3) the location ROC (LROC) paradigm</strong></p>
<p>In the LROC paradigm the observer gives an overall ROC-type rating to
the image, and indicates the location of the most suspicious region in
the image.</p>
<p><strong>(4) the region of interest (ROI) paradigm</strong></p>
<p>In the ROI paradigm the researcher divides each image into a number
of adjacent non-overlapping regions of interest (ROIs) that cover the
clinical area of interest. The radiologist’s task is to evaluate each
ROI for presence of disease and give an ROC-type rating to it.</p>
</div>
<div id="basic-analysis-approach" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Basic Analysis
Approach</h2>
<p>The basic approach is to obtain data, according to one of the above
paradigms, from a group of radiologists interpreting a common set of
images in one or more modalities. The way the data is collected, and the
structure of the data, depends on the selected paradigm. The next step
is to adopt an objective measure of performance, termed a figure or
merit (FOM) and a procedure for estimating it for each modality-reader
combination. Assuming two modalities, e.g., a new modality and the
conventional one, one averages FOM over all readers within each
modality. If the difference between the two averages (new modality minus
the conventional one) is positive, that is an indication of improvement.
Next comes the statistical part: is the difference large enough so as to
be unlikely to be due to chance. This part of the analysis, termed
significance testing, yields a probability, or p-value, that the
observed difference or larger could result from chance even though the
modalities have identical performances. If the p-value is very small,
that it is taken as evidence that the modalities are not identical in
performance, and if the difference is in the right direction, the new
modality is judged better.</p>
</div>
<div id="roc" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> ROC</h2>
<div id="confusion-matrix" class="section level3" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> Confusion
Matrix</h3>
<p>Let <span class="math inline">\(\mathrm{D}\)</span> represent the
radiologist’s decision with <span
class="math inline">\(\mathrm{D}=1\)</span> representing the decision
“case is diagnosed as non-diseased” and <span
class="math inline">\(\mathrm{D}=2\)</span> representing the decision
“case is diagnosed as diseased”. Let <span
class="math inline">\(\mathbf{T}\)</span> denote the truth with <span
class="math inline">\(\mathrm{T}=1\)</span> representing “case is
actually non-diseased” and <span class="math inline">\(T=2\)</span>
representing “case is actually diseased”. Each decision, one of two
values, will be associated with one of two truth states, resulting in an
entry in one of 4 cells arranged in a <span class="math inline">\(2
\times 2\)</span> layout, termed the decision vs. truth table, Table 2.1
which is of fundamental importance in observer performance. The cells
are labeled as follows. The abbreviation <span
class="math inline">\(\mathrm{TN}\)</span>, for true negative,
represents a <span class="math inline">\(\mathrm{D}=1\)</span> decision
on a <span class="math inline">\(\mathrm{T}=1\)</span> case. <span
class="math inline">\(\mathrm{FN}\)</span>, for false negative,
represents a <span class="math inline">\(\mathrm{D}=1\)</span> decision
on a <span class="math inline">\(\mathrm{T}=2\)</span> case (also termed
a “miss”). FP, for false positive, represents <span
class="math inline">\(\mathrm{a} \mathbf{D}=2\)</span> decision on a
<span class="math inline">\(\mathrm{T}=1\)</span> case (a “false-alarm”)
and <span class="math inline">\(\mathrm{TP}\)</span>, for true positive,
represents <span class="math inline">\(\mathrm{a}=2\)</span> decision on
a <span class="math inline">\(\mathrm{T}=2\)</span> case (a “hit”).</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><strong>T=1</strong></th>
<th align="center"><strong>T=2</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>D=1</strong></td>
<td align="center">TN</td>
<td align="center">FN</td>
</tr>
<tr class="even">
<td align="center"><strong>D=2</strong></td>
<td align="center">FP</td>
<td align="center">TP</td>
</tr>
</tbody>
</table>
<div class="figure" style="text-align: center">
<img src="02_Plots/MRMC/Formula-for-Sensitivity-Specificity.png" alt="Formula for Sensitivity, Specificity, Positive Predictive Value (PPV), Negative Predictive Value (NPV) Calculation" width="100%" />
<p class="caption">
Formula for Sensitivity, Specificity, Positive Predictive Value (PPV),
Negative Predictive Value (NPV) Calculation
</p>
</div>
</div>
<div id="analytic-expressions-for-specificity-and-sensitivity"
class="section level3" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> Analytic
Expressions for Specificity and Sensitivity</h3>
<p>Specificity corresponding to threshold <span
class="math inline">\(\zeta\)</span> is the probability that a Z-sample
from a non-diseased case is smaller than <span
class="math inline">\(\zeta\)</span>. By definition, this is the CDF
corresponding to the threshold <span
class="math inline">\(\zeta\)</span>. In other words:</p>
<p><span class="math display">\[
\left.\begin{array}{rl}
S p(\zeta) &amp; \equiv P\left(Z_{k_1 1}&lt;\zeta \mid Z_{k_1 1} \sim
N(0,1)\right) \\
&amp; =\Phi(\zeta)
\end{array}\right\}
\]</span></p>
<p>The expression for sensitivity can be derived. Consider that the
random variable obtaining by shifting the origin to <span
class="math inline">\(\mu\)</span>. A little thought should convince the
reader that <span class="math inline">\(Z_{k_2 2}-\mu\)</span> is
distributed as <span class="math inline">\(N(0,1)\)</span>. Therefore,
the desired probability is: <span class="math display">\[
\left.\begin{array}{rl}
S e(\zeta) &amp; \equiv P\left(Z_{k_2 2} \geq \zeta\right) \\
&amp; =P\left(\left(Z_{k_2 2}-\mu\right) \geq(\zeta-\mu)\right) \\
&amp; =1-P\left(\left(Z_{k_2 2}-\mu\right)&lt;(\zeta-\mu)\right) \\
&amp; =1-\Phi(\zeta-\mu)
\end{array}\right\}
\]</span></p>
<p>A little thought, based on the definition of the CDF function and the
symmetry of the unit-normal pdf function, should convince the reader
that: <span class="math display">\[
\left.\begin{array}{rl}
1-\Phi(\zeta) &amp; =-\Phi(\zeta) \\
1-\Phi(\zeta-\mu) &amp; =\Phi(\mu-\zeta)
\end{array}\right\}
\]</span></p>
<p>Instead of carrying the “1 minus” around one can use the more compact
notation. Summarizing, the analytical formulae for the specificity and
sensitivity for the equal-variance binormal model are: <span
class="math display">\[
\begin{gathered}
S p(\zeta)=\Phi(\zeta) \\
S e(\zeta)=\Phi(\mu-\zeta)
\end{gathered}
\]</span></p>
<p>Sensitivity and specificity are restricted to the range 0 to 1 . The
observer’s performance could be characterized by specifying sensitivity
and specificity, i.e., a pair of numbers. If both sensitivity and
specificity of an imaging system are greater than the corresponding
values for another system, then the first system is unambiguously better
than the second. But if sensitivity is greater for the first but
specificity is greater for the second the comparison is ambiguous. A
scalar measure is desirable that combines sensitivity and specificity
into a single measure of diagnostic performance.</p>
<p>The parameter <span class="math inline">\(\mu\)</span> satisfies the
requirements of a scalar performancer measure, termed a figure of merit
(FOM). Eqn. (3.15) can be solved for <span
class="math inline">\(\mu\)</span> as follows. Inverting the equations
yields: <span class="math display">\[
\left.\begin{array}{rl}
\zeta &amp; =\Phi^{-1}(\operatorname{Sp}(\zeta)) \\
\mu-\zeta &amp; =\Phi^{-1}(\operatorname{Se}(\zeta))
\end{array}\right\}
\]</span></p>
<p>Eliminating <span class="math inline">\(\zeta\)</span> yields: <span
class="math display">\[
\left.\mu=\Phi^{-1}(\operatorname{Sp}(\zeta))+\Phi^{-1}(\operatorname{Se}(\zeta))\right\}
\]</span></p>
<p>This is a useful relation, as it converts a pair of numbers into a
scalar performance measure. Now it is almost trivial to compare two
modalities: the one with the higher <span
class="math inline">\(\mu\)</span> is better. In reality, the comparison
is not trivial since like sensitivity and specificity <span
class="math inline">\(\mu\)</span> has to be estimated from a finite
dataset and one must account for sampling variability.</p>
<div class="figure" style="text-align: center">
<img src="02_Plots/MRMC/MRMC_EqualVarianceModel.png" alt="The equal-variance binormal model" width="75%" />
<p class="caption">
The equal-variance binormal model
</p>
</div>
<p>Fig. above shows the equal-variance binormal model for <span
class="math inline">\(\mu=3\)</span> and <span
class="math inline">\(\zeta=1\)</span>. The blue-shaded area, including
the “common” portion with the vertical red lines, is the probability
that a z-sample from a non-diseased case exceeds <span
class="math inline">\(\zeta=1\)</span>, which is the complement of
specificity, i.e., false positive fraction, which is 1 pnorm(1) <span
class="math inline">\(=0.159\)</span>. The red shaded area, including
the “common” portion with the vertical red lines, is the probability
that a <span class="math inline">\(z\)</span>-sample from a diseased
case exceeds <span class="math inline">\(\zeta=1\)</span>, which is
sensitivity or true positive fraction, which is pnorm(31) <span
class="math inline">\(=0.977\)</span>.</p>
</div>
<div id="roc-curve" class="section level3" number="1.3.3">
<h3><span class="header-section-number">1.3.3</span> ROC Curve</h3>
<p>The receiver operating characteristic (ROC) is defined as the plot of
sensitivity (y-axis) vs. 1-specificity (x-axis). Equivalently, it is the
plot of TPF (y-axis) vs. FPF (x-axis).</p>
<p><span class="math display">\[
\left.\begin{array}{rl}
\operatorname{FPF}(\zeta) &amp; \equiv 1-\operatorname{Sp}(\zeta) \\
&amp; =\Phi(-\zeta) \\
\operatorname{TPF}(\zeta) &amp; \equiv \operatorname{Se}(\zeta) \\
&amp; =\Phi(\mu-\zeta)
\end{array}\right\}
\]</span></p>
<p>Specifying <span class="math inline">\(\zeta\)</span> selects a
particular operating point on this curve and varying <span
class="math inline">\(\zeta\)</span> from <span
class="math inline">\(+\infty\)</span> to <span
class="math inline">\(-\infty\)</span> causes the operating point to
trace out the ROC curve from the origin <span
class="math inline">\((0,0)\)</span> to <span
class="math inline">\((1,1)\)</span>. Note that as <span
class="math inline">\(\zeta\)</span> increases the operating point moves
down the curve. The operating point <span class="math inline">\(O(\zeta
\mid \mu)\)</span> for the equal variance binormal model is: <span
class="math display">\[
O(\zeta \mid \mu)=(\Phi(-\zeta), \Phi(\mu-\zeta))
\]</span></p>
<p>The operating point predicted by the above equation lies exactly on
the theoretical ROC curve. This condition can only be achieved with very
large numbers of cases. With finite datasets the operating point will
almost never be exactly on the theoretical curve. The ROC curve is the
locus of the operating point for fixed <span
class="math inline">\(\mu\)</span> and variable <span
class="math inline">\(\zeta\)</span>. Fig. below shows examples of
equal-variance binormal model ROC curves for different values of <span
class="math inline">\(\mu\)</span>. Each has the property that TPF is a
monotonically increasing function of FPF and the slope decreases
monotonically as the operating point moves up the curve. As <span
class="math inline">\(\mu\)</span> increases the curves get
progressively upward-left shifted, approaching the top-left corner of
the ROC plot. In the limit <span
class="math inline">\(\mu=\infty\)</span> the curve degenerates into two
line segments, a vertical one connecting the origin to <span
class="math inline">\((0,1)\)</span> and a horizontal one connecting
<span class="math inline">\((0,1)\)</span> to <span
class="math inline">\((1,1)\)</span> - the ROC plot for a perfect
observer.</p>
<div class="figure" style="text-align: center">
<img src="02_Plots/MRMC/MRMC_ROC.png" alt="ROC plots predicted by the equal variance binormal model for different values of mu" width="75%" />
<p class="caption">
ROC plots predicted by the equal variance binormal model for different
values of mu
</p>
</div>
<p><strong>The chance diagonal</strong></p>
<p>In Fig. above the ROC curve for <span
class="math inline">\(\mu=0\)</span> is the positive diagonal of the ROC
plot, termed the chance diagonal. Along this curve <span
class="math inline">\(\mathrm{TPF}=\mathrm{FPF}\)</span> and the
observer’s performance is at chance level. For <span
class="math inline">\(\mu=0\)</span> the pdf of the diseased
distribution is identical to that of the non-diseased distribution: both
are centered at the origin. Therefore, no matter the choice of threshold
<span class="math inline">\(\zeta\)</span>, <span
class="math inline">\(\mathrm{TPF}=\mathrm{FPF}\)</span>. Setting <span
class="math inline">\(\mu=0\)</span> yields: <span
class="math display">\[
\operatorname{TPF}(\zeta)=\operatorname{FPF}(\zeta)=\Phi(-\zeta)
\]</span></p>
<p>In this case the red and blue curves in Fig. above coincide. The
observer is unable to find any difference between the two distributions.
This can happen if the cancers are of such low visibility that diseased
cases are indistinguishable from non-diseased ones, or the observer’s
skill level is so poor that the observer is unable to make use of
distinguishing characteristics between diseased and non-diseased cases
that do exist.</p>
<p><strong>Area under the ROC curve</strong></p>
<p>The area AUC (abbreviation for area under curve) under the ROC curve
suggests itself as a measure of performance that is independent of
threshold and therefore circumvents the ambiguity issue of comparing
sensitivity/specificity pairs, and has other advantages. It is defined
by the following integrals:</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
A_{z;\sigma = 1} &amp;= \int_{0}^{1}TPF(\zeta)d(FPF(\zeta))\\
&amp;=\int_{0}^{1}FPF(\zeta)d(TPF(\zeta))\\
\end{aligned}
\end{equation}
\]</span></p>
</div>
<div id="roc-counts-table" class="section level3" number="1.3.4">
<h3><span class="header-section-number">1.3.4</span> ROC Counts
table</h3>
<p>In a positive-directed rating scale with five discrete levels, the
ratings could be the ordered labels:</p>
<ul>
<li>“1”: definitely non-diseased,</li>
<li>“2”: probably non-diseased,</li>
<li>“3”: could be non-diseased or diseased,</li>
<li>“4”: probably diseased,</li>
<li>“5”: definitely diseased.</li>
</ul>
<p>At the conclusion of the ROC study an ROC counts table is
constructed.</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><strong>r=5</strong></th>
<th align="center"><strong>r=4</strong></th>
<th align="center"><strong>r=3</strong></th>
<th align="center"><strong>r=2</strong></th>
<th align="center"><strong>r=1</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>non-diseased</strong></td>
<td align="center">1</td>
<td align="center">2</td>
<td align="center">8</td>
<td align="center">19</td>
<td align="center">30</td>
</tr>
<tr class="even">
<td align="center"><strong>diseased</strong></td>
<td align="center">22</td>
<td align="center">12</td>
<td align="center">5</td>
<td align="center">6</td>
<td align="center">5</td>
</tr>
</tbody>
</table>
<p>TABLE: Representative counts table.</p>
<p>Table below illustrates how ROC operating points are calculated from
the cell counts. In this table: r&gt;=5 means “counting ratings greater
than or equal to 5”, r&gt;=4 means “counting ratings greater than or
equal to 4”, etc</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><strong>r&gt;=5</strong></th>
<th align="center"><strong>r&gt;=4</strong></th>
<th align="center"><strong>r&gt;=3</strong></th>
<th align="center"><strong>r&gt;=2</strong></th>
<th align="center"><strong>r&gt;=1</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>FPF</strong></td>
<td align="center">0.0167</td>
<td align="center">0.05</td>
<td align="center">0.1833</td>
<td align="center">0.5</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center"><strong>TPF</strong></td>
<td align="center">0.4400</td>
<td align="center">0.68</td>
<td align="center">0.7800</td>
<td align="center">0.9</td>
<td align="center">1</td>
</tr>
</tbody>
</table>
<p>TABLE: Computation of operating points from cell counts.</p>
</div>
</div>
<div id="scoreratings" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Score/Ratings</h2>
<p>The ratings are to be thought of as ordered labels, not as numeric
values.</p>
<p>The temptation to regard confidence levels / ratings as numeric
values can be particularly strong when one uses a large number of bins
to collect the data. One could use of quasi-continuous ratings scale,
implemented for example, by having a slider-bar user interface for
selecting the rating. The slider bar typically extends from 0 to 100,
and the rating could be recorded as a floating-point number, e.g.,
63.45. Here too one cannot assume that the difference between a
zero-rated case and a 10 rated case is a tenth of the difference between
a zero-rated case and a 100 rated case. So averaging the ratings is not
allowed. Additionally, one cannot assume that different observers use
the labels in the same way. One observer’s 4-rating is not equivalent to
another observers 4-rating.</p>
<p>The reason for the quotes in the title to this section is that a
single operating point on a laboratory ROC plot, no matter how obtained,
has little relevance to how radiologists operate in the clinic. However,
some consider it useful to quote an operating point from an ROC study.
For a 5-rating ROC study, Table 4.1, it is not possible to unambiguously
calculate the operating point of the observer in the binary task of
discriminating between non-diseased and diseased cases. One possibility
would be to use the “three and above” ratings to define the operating
point, but one might jus have well have chosen “two and above”. A second
possibility is to instruct the radiologist that a “four and above”
rating, for example, implies the case would be reported “clinically” as
diseased. However, the radiologist can only pretend so far that this
study, which has no clinical consequences, is somehow a “clinical”
study.</p>
<p>If a single laboratory study based operating point is desired
(Nishikawa 2012), the best strategy, in my opinion, is to obtain the
rating via two questions. This method is also illustrated in Table 3.1
of a book on detection theory (Macmillan and Creelman 2004). The first
question is “is the case diseased?” The binary (Yes/No) response to this
question allows unambiguous calculation of the operating point. The
second question is: “what is your confidence in your previous decision?”
and allow three responses, namely Low, Medium and High. The
dual-question approach is equivalent to a 6-point rating scale, Fig.
4.3. The answer to the first question, is the patient diseased, allows
unambiguous construction of a single “clinical” operating point for
disease presence. The answer to the second question yields multiple
operating points.</p>
<div class="figure" style="text-align: center">
<img src="02_Plots/MRMC/MRMC_Score.png" alt="A method for acquiring ROC data on an effectively 6-point scale that also yields an unambiguous single operating point for declaring patients diseased. Note the reversal of the ordering of the final ratings in the last “column” in the lower half of the figure" width="75%" />
<p class="caption">
A method for acquiring ROC data on an effectively 6-point scale that
also yields an unambiguous single operating point for declaring patients
diseased. Note the reversal of the ordering of the final ratings in the
last “column” in the lower half of the figure
</p>
</div>
<p>The ordering of the ratings can be understood as follows. The four,
five and six ratings are as expected. If the radiologist states the
patient is diseased and the confidence level is high that is clearly the
highest end of the scale, i.e., six, and the lower confidence levels,
five and four, follow, as shown. If, on the other hand, the radiologist
states the patient is non-diseased, and the confidence level is high,
then that must be the lowest end of the scale, i.e., “1”. The lower
confidence levels in a negative decision must be higher than “1”, namely
“2” and “3”, as shown. As expected, the low confidence ratings, namely
“3” (non-diseased, low confidence) and “4” (diseased, low confidence)
are adjacent to each other. With this method of data-collection, there
is no confusion as to what rating defines the single desired operating
point as this is determined by the binary response to the first
question. The 6-point rating scale is also sufficiently fine to not
smooth out the ability of the radiologist to maintain distinct different
levels. In my experience, using this scale one expects rating noise of
about ± 1/2 a rating bin, i.e., the same difficult case, shown on
different occasions to the same radiologist (with sufficient time lapse
or other intervening cases to minimize memory effects) is expected to
elicit a “3” or “4”, with roughly equal probability.</p>
</div>
</div>
<div id="binormal-model" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Binormal Model</h1>
<div id="equal-variance-binormal-model" class="section level2"
number="2.1">
<h2><span class="header-section-number">2.1</span> Equal Variance
Binormal Model</h2>
<p>The equal variance binormal model in ROC (Receiver Operating
Characteristic) analysis is a statistical approach used to evaluate the
performance of diagnostic tests. It assumes that the distributions of
the test results for both the diseased and non-diseased groups follow
normal (Gaussian) distributions and that these distributions have the
same variance.</p>
<p>Here’s a breakdown of the key components and how the model works:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Binormal Assumption</strong>: This model assumes that the
test results for both positive (diseased) and negative (non-diseased)
populations are normally distributed. This is a simplification often
used to make the model more mathematically tractable.</p></li>
<li><p><strong>Equal Variance</strong>: The assumption of equal variance
means that while the means of the two distributions might differ
(indicating that the test can distinguish between diseased and
non-diseased cases), the spread (variance) of the results around the
mean is the same for both groups.</p></li>
<li><p><strong>ROC Curve Construction</strong>: The ROC curve is
constructed by plotting the true positive rate (TPR, sensitivity)
against the false positive rate (FPR, 1-specificity) at various
threshold settings of the diagnostic test. Under the equal variance
binormal model, the ROC curve can be characterized analytically using
the cumulative distribution functions of the normal
distributions.</p></li>
<li><p><strong>Analysis and Metrics</strong>: The area under the ROC
curve (AUC) is a common metric derived from ROC analysis. In the context
of the equal variance binormal model, the AUC can be interpreted as the
probability that a randomly chosen diseased subject has a higher test
result than a randomly chosen non-diseased subject. Higher AUC values
indicate better diagnostic performance.</p></li>
</ol>
<p>This model is particularly useful because of its mathematical
simplicity and the ease with which it can be used to calculate various
statistics relevant to test performance. However, it’s essential to
verify that the equal variance assumption holds in practice for the
specific diagnostic test being analyzed, as deviations from this
assumption can lead to inaccuracies in the ROC analysis.</p>
</div>
<div id="unequal-variance-binormal-model" class="section level2"
number="2.2">
<h2><span class="header-section-number">2.2</span> Unequal Variance
Binormal Model</h2>
<ul>
<li>When the equal variance assumption does not hold (as you’ve
observed), the unequal variance model becomes more appropriate. This
model allows for each group to have its own variance, offering a
potentially better fit for datasets where diseased and non-diseased
groups exhibit different levels of score dispersion.</li>
</ul>
<div class="figure" style="text-align: center">
<img src="02_Plots/MRMC/MRMC_BinormalModel.png" alt="The pdfs of the two binormal model distributions for μ=1.5 and  σ=1.5." width="75%" />
<p class="caption">
The pdfs of the two binormal model distributions for μ=1.5 and σ=1.5.
</p>
</div>
<p><strong>Specificity and Sensitivity</strong></p>
<ul>
<li><p><strong><span class="math inline">\(Z_t\)</span></strong>: The
random z-sample from truth state <span class="math inline">\(t\)</span>
(1 for non-diseased, 2 for diseased).</p></li>
<li><p>For non-diseased cases (<span
class="math inline">\(t=1\)</span>), the z-scores are normally
distributed as <span class="math inline">\(N(0,1)\)</span>.</p></li>
<li><p>For diseased cases (<span class="math inline">\(t=2\)</span>),
the scores follow the transformation <span
class="math inline">\(\frac{Z_2 - \mu}{\sigma} \sim N(0,1)\)</span>,
where <span class="math inline">\(\mu\)</span> and <span
class="math inline">\(\sigma\)</span> are the mean and standard
deviation of the diseased group.</p></li>
<li><p><strong>Specificity (Sp)</strong> for a cutoff <span
class="math inline">\(\xi\)</span> is defined as: <span
class="math display">\[
Sp(\xi) = P(Z_1 \leq \xi) = \Phi(\xi)
\]</span> where <span class="math inline">\(\Phi\)</span> is the
cumulative distribution function (CDF) of the standard normal
distribution.</p></li>
<li><p><strong>Sensitivity (Se)</strong> for the same cutoff <span
class="math inline">\(\xi\)</span> in the context of the diseased group
is: <span class="math display">\[
Se(\xi|\mu, \sigma) = P(Z_2 &gt; \xi) = 1 - P\left(\frac{Z_2 -
\mu}{\sigma} \leq \frac{\xi - \mu}{\sigma}\right) = 1 -
\Phi\left(\frac{\xi - \mu}{\sigma}\right)
\]</span> which simplifies to: <span class="math display">\[
Se(\xi|\mu, \sigma) = \Phi\left(\frac{\mu - \xi}{\sigma}\right)
\]</span></p></li>
</ul>
<p><strong>ROC Curve Equations:</strong></p>
<ul>
<li><p>The <strong>False Positive Fraction (FPF)</strong>, which is 1
minus specificity, is expressed as: <span class="math display">\[
FPF(\xi) = 1 - Sp(\xi) = \Phi(-\xi)
\]</span></p></li>
<li><p>The <strong>True Positive Fraction (TPF)</strong>, which is
sensitivity, is expressed as: <span class="math display">\[
TPF(\xi|\mu, \sigma) = \Phi\left(\frac{\mu - \xi}{\sigma}\right)
\]</span></p></li>
<li><p>The TPF as a function of FPF can be derived by expressing <span
class="math inline">\(\xi\)</span> in terms of FPF: <span
class="math display">\[
\xi = -\Phi^{-1}(FPF)
\]</span> substituting back, the TPF in terms of FPF becomes: <span
class="math display">\[
TPF = \Phi\left(\frac{\mu + \Phi^{-1}(FPF)}{\sigma}\right)
\]</span></p></li>
</ul>
</div>
<div id="proper-roc-models" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Proper ROC
models</h2>
<ol style="list-style-type: decimal">
<li><strong>Issue with the Binormal Model</strong>:
<ul>
<li>The binormal model often generates ROC curves that cross the
diagonal line of no discrimination, i.e., the line where the true
positive rate equals the false positive rate. This crossing indicates
regions where the diagnostic performance is worse than random chance,
which is scientifically questionable and termed “improper.”</li>
<li>The problem arises because the shape parameter <span
class="math inline">\(b\)</span> in the binormal model rarely equals
exactly 1, the only value that would prevent the curve from crossing the
chance diagonal. A value <span class="math inline">\(b &lt; 1\)</span>
results in a curve crossing near the upper right corner, while <span
class="math inline">\(b &gt; 1\)</span> leads to a crossing near the
origin.</li>
</ul></li>
</ol>
<p><strong>Methods to Achieve Proper ROC Curves</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Radiological Search Model (RSM)</strong>:
<ul>
<li>Developed by Chakraborty and colleagues, implemented in the RJafroc
package. This model is designed based on patterns observed in
radiological searches, making it suitable for medical imaging
applications.</li>
</ul></li>
<li><strong>Contaminated Binormal Model (CBM)</strong>:
<ul>
<li>Introduced by Dorfman, Berbaum, and Brandser, and available through
DBM-MRMC software. It accounts for data contamination effects and is
also implemented in the RJafroc package.</li>
</ul></li>
<li><strong>Binormal Model-Based Proper ROC Fitting</strong>:
<ul>
<li>Developed by Metz and Pan, this method is part of the PROPROC
software, integrated within the DBM-MRMC software. It addresses the
shortcomings of the traditional binormal model by ensuring the ROC
curves are proper.</li>
</ul></li>
<li><strong>Bigamma Model</strong>:
<ul>
<li>Introduced by Dorfman et al., it offers a straightforward
implementation approach, though specific software for it is not widely
available.</li>
</ul></li>
<li><strong>Location ROC (LROC) Fit (LROCFIT)</strong>:
<ul>
<li>Based on the LROC paradigm by Swensson, which differs by requiring
localization data. This model ensures proper ROC curves but requires
specific types of data that include localization information.</li>
</ul></li>
</ol>
</div>
</div>
<div id="empirical-auc" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Empirical AUC</h1>
<div id="empirical-auc-non-par-model" class="section level2"
number="3.1">
<h2><span class="header-section-number">3.1</span> Empirical AUC
(Non-Par Model)</h2>
<p>The ROC plot is defined as the plot of sensitivity (y-axis)
vs. 1-specificity (x-axis). Equivalently, it is the plot of TPF vs. FPF
. An equal variance binormal model was introduced in an earlier chapter
which allows an ROC plot to be fitted to a single observed operating
point. The more commonly used ratings paradigm was introduced in the
previous chapter.</p>
<p>The empirical Area Under the Curve (AUC) in the context of ROC
(Receiver Operating Characteristic) analysis is a non-parametric measure
used to evaluate the performance of a diagnostic test or classifier. The
AUC represents the probability that a classifier will rank a randomly
chosen positive instance higher than a randomly chosen negative
instance. Here’s how it is typically calculated and interpreted:</p>
<p><strong>Calculation of Empirical AUC</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Data Preparation</strong>: You start with a dataset where
you have the actual outcomes (positive or negative) and the predicted
probabilities or scores from the classifier.</p></li>
<li><p><strong>Ranking Scores</strong>: Rank all instances by the
classifier’s scores or probabilities, with higher scores indicating a
stronger prediction for the positive class.</p></li>
<li><p><strong>Counting Pairs</strong>: For each pair of one positive
and one negative instance, count the pair as a “concordant pair” if the
positive instance has a higher score than the negative instance. If they
have the same score, it’s typically counted as a half of a concordant
pair.</p></li>
<li><p><strong>Calculation</strong>: The empirical AUC is calculated as
the ratio of the number of concordant pairs (plus half the number of
ties, if ties are accounted for) to the total number of pairs of
positive and negative instances: <span class="math display">\[
\text{AUC} = \frac{\text{Number of concordant pairs} + 0.5 \times
\text{Number of ties}}{\text{Total pairs of positive and negative
instances}}
\]</span></p></li>
</ol>
<ul>
<li><strong>0.5</strong>: An AUC of 0.5 suggests no discriminative
ability between positive and negative classes. It’s as good as random
guessing.</li>
<li><strong>1.0</strong>: An AUC of 1.0 indicates perfect
discrimination. The classifier or test always ranks all positive
instances higher than any negative instance.</li>
<li><strong>Between 0.5 and 1.0</strong>: Values greater than 0.5 but
less than 1.0 indicate varying degrees of ability to discriminate
between the classes. The closer to 1.0, the better the model is at
distinguishing between the two classes.</li>
</ul>
<p><strong>Advantages of Empirical AUC</strong></p>
<ul>
<li><strong>Non-parametric</strong>: It does not make any assumptions
about the underlying distributions of the classifier scores.</li>
<li><strong>Robustness</strong>: The AUC is often less affected by the
imbalance in the dataset between the positive and negative classes.</li>
<li><strong>Interpretability</strong>: The value of AUC is easy to
understand and communicate, making it a popular choice for evaluating
binary classification models.</li>
</ul>
<p>The empirical AUC provides a straightforward and effective way to
measure how well a classifier can separate two classes without relying
on assumptions about the score distributions. This makes it highly
useful in practical applications where theoretical distributions might
not be known or applicable.</p>
</div>
<div id="wilcoxon-statistic" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Wilcoxon
Statistic</h2>
<ol style="list-style-type: decimal">
<li><strong>AUC as Shaded Area</strong>:
<ul>
<li>In ROC analysis, the area under the curve (AUC) represents the
probability that a randomly chosen positive instance scores higher than
a randomly chosen negative instance. It’s a measure of the test’s
overall ability to discriminate between the conditions being tested
(e.g., diseased vs. non-diseased).</li>
<li>Visually, this area can be conceptualized as the sum of areas under
segments of the ROC curve, which could resemble a combination of
triangles and trapezoids, depending on the curve’s shape.</li>
</ul></li>
<li><strong>Simplifying AUC Calculation</strong>:
<ul>
<li>Directly calculating the areas of these geometric shapes can be
complex. However, there’s a simpler method using a statistical measure
known as the Wilcoxon statistic (also known as the Mann-Whitney-Wilcoxon
statistic). This statistic effectively quantifies the AUC by considering
the ranks of individual scores from both groups.</li>
<li>The Wilcoxon statistic is calculated by ranking all observations
from both groups together and then summing the ranks for one of the
groups. It reflects the extent to which the scores of one group are
consistently higher than the scores of the other group.</li>
</ul></li>
</ol>
</div>
<div id="bambers-equivalence-theorem" class="section level2"
number="3.3">
<h2><span class="header-section-number">3.3</span> Bamber’s Equivalence
theorem</h2>
<p>The empirical area (AUC) calculated via the Wilcoxon statistic has
been proven equivalent to the geometric area under the ROC curve. This
equivalence is part of what’s known as Bamber’s Theorem. It reassures
analysts that using the Wilcoxon statistic to compute the AUC is not
only simpler but also theoretically sound.</p>
<p>The Wilcoxon statistic, commonly used in non-parametric statistical
tests, serves as a measure of the difference between two independent
samples. In the context of ROC analysis, this statistic quantifies the
degree to which the distribution of scores for the positive class (e.g.,
diseased) is shifted relative to the negative class (e.g., healthy). The
mathematical representation suggests:</p>
<p><span class="math display">\[ W = \text{AUC} \]</span></p>
<p>This equation means that the Wilcoxon statistic (<span
class="math inline">\(W\)</span>) is numerically equivalent to the area
under the empirical ROC curve (AUC). This AUC represents the probability
that a randomly selected positive instance is correctly ranked higher
than a randomly selected negative instance by the test.</p>
<p>Before Bamber’s theorem was established, the relationship between
plotting an empirical ROC curve and calculating the Wilcoxon statistic
was not formally recognized. Bamber’s equivalence theorem demonstrated
that these two approaches are not only related but are quantitatively
identical. This revelation has profound implications:</p>
<ul>
<li><strong>Validation of Non-Parametric Methods</strong>: This theorem
validates the use of the Wilcoxon statistic as a robust non-parametric
method for evaluating the performance of diagnostic tests.</li>
<li><strong>Enhanced Interpretation</strong>: The equivalence provides a
meaningful way to interpret the empirical AUC, linking it directly to a
well-understood statistical measure, thus reinforcing its utility in
medical decision-making and diagnostic test evaluation.</li>
</ul>
<p><strong>Numerical Illustration Using the <code>trapz</code>
Function</strong></p>
<p>To illustrate this theorem numerically, one could use the
<code>trapz</code> function, which is available in many statistical and
mathematical software packages (like MATLAB, Python’s NumPy, or R). The
function calculates the area under a curve defined by two arrays, <span
class="math inline">\(x\)</span> and <span
class="math inline">\(y\)</span>, representing the false positive
fraction (FPF) and the true positive fraction (TPF), respectively.
Here’s how it can be used:</p>
<pre class="r"><code>trapz = function(x, y)
{ ### computes the integral of y with respect to x using trapezoidal integration.
  idx = 2:length(x)
  return (as.double( (x[idx] - x[idx-1]) %*% (y[idx] + y[idx-1])) / 2)
}


Wilcoxon &lt;- function (zk1, zk2)
{
  K1 = length(zk1)
  K2 = length(zk2)
  W &lt;- 0
  for (k1 in 1:K1) {
    W &lt;- W + sum(zk1[k1] &lt; zk2)
    W &lt;- W + 0.5 * sum(zk1[k1] == zk2)
  }
  W &lt;- W/K1/K2
  return (W)
}


RocOperatingPoints &lt;- function( K1, K2 ) {
  nOpPts &lt;- length(K1) - 1 # number of op points
  FPF &lt;- array(0,dim = nOpPts)
  TPF &lt;- array(0,dim = nOpPts)
   
  for (r in (nOpPts+1):2) {
    FPF[r-1] &lt;- sum(K1[r:(nOpPts+1)])/sum(K1)
    TPF[r-1] &lt;- sum(K2[r:(nOpPts+1)])/sum(K2)    
  }
  FPF &lt;- rev(FPF)
  TPF &lt;- rev(TPF)
  
  return( list(
    FPF = FPF,
    TPF = TPF
  ) )
}

RocCountsTable = array(dim = c(2,5))
RocCountsTable[1,]  &lt;- c(30,19,8,2,1)
RocCountsTable[2,]  &lt;- c(5,6,5,12,22)

zk1  &lt;- rep(1:length(RocCountsTable[1,]),RocCountsTable[1,])#convert frequency table to array
zk2  &lt;- rep(1:length(RocCountsTable[2,]),RocCountsTable[2,])#do:

w  &lt;- Wilcoxon (zk1, zk2)
cat(&quot;The Wilcoxon statistic is = &quot;, w, &quot;\n&quot;)</code></pre>
<pre><code>## The Wilcoxon statistic is =  0.8606667</code></pre>
<pre class="r"><code>#&gt; The Wilcoxon statistic is =  0.8606667
ret &lt;- RocOperatingPoints(RocCountsTable[1,], RocCountsTable[2,])
FPF &lt;- ret$FPF;FPF &lt;- c(0,FPF,1)
TPF &lt;- ret$TPF;TPF &lt;- c(0,TPF,1)
AUC &lt;- trapz(FPF,TPF) # trapezoidal integration
cat(&quot;direct integration yields AUC = &quot;, AUC, &quot;\n&quot;)</code></pre>
<pre><code>## direct integration yields AUC =  0.8606667</code></pre>
<pre class="r"><code>#&gt; direct integration yields AUC =  0.8606667</code></pre>
</div>
</div>
<div id="afroc" class="section level1" number="4">
<h1><span class="header-section-number">4</span> AFROC</h1>
<p>Commonly, an MRMC ROC study comprises a cohort of ‘c’ patients—some
of whom are diagnosed with the disease of interest and others who are
not. These individuals undergo multiple diagnostic evaluations, which
may involve different display modes, or analysis via distinct computer
algorithms. The resultant images are then interpreted by a group of ‘r’
readers, typically trained radiologists, who are blinded both to the
actual disease status of the patients and to outcomes from other tests
or readers. Table 1 exemplifies this factorial design, often described
as a “paired-patient, paired-reader” layout. Alternative configurations
are also viable, including “unpaired-patient paired-reader,”
“paired-patient unpaired-reader,” and “unpaired-patient unpaired-reader”
designs.</p>
<div class="figure" style="text-align: center">
<img src="02_Plots/MRMC/MRMC_Layout.PNG" alt="Layout for Multi-Reader Factorial Design" width="75%" />
<p class="caption">
Layout for Multi-Reader Factorial Design
</p>
</div>
<p>The factorial, or paired-patient paired-reader, framework introduces
several principal sources of variability in measuring diagnostic test
accuracy. These include variability across patients, attributable to
differences in tissue densities or disease characteristics, and
variability among readers, due to variations in training, experience,
and innate ability. Furthermore, the potential for correlated
interpretations among readers introduces additional complexity. These
correlations may arise due to the same patients undergoing all tests,
the same readers evaluating all tests, or all readers assessing the same
images. Some analytical models explicitly incorporate these
correlations, while others do so implicitly through interaction
terms.</p>
<p>The main sources of variability (noise) in MRMC studies include: -
Variability among patients. - Variability among readers. - Correlations
between readers’ interpretations due to: - Same patients undergoing each
test. - Same readers interpreting results from each test. - All readers
interpreting the same images.</p>
<p>Potential modeling approaches include designing models for the
confidence scores assigned by readers, transforming observed confidence
scores (e.g., pseudovalues), directly modeling the summary measure of
accuracy (e.g., ROC area), or combining models for both confidence
scores and summary accuracy measures.</p>
<p>Variation in reader performance is another crucial aspect. In phase
II studies, readers are typically selected from those available at the
conducting institution, making these results less generalizable to a
broader radiologist population. Thus, findings from such studies are
applicable primarily to the specific readers involved, referred to as
“fixed effects.” Conversely, phase III studies aim to select readers who
represent a well-defined broader population, making the study’s accuracy
estimates generalizable to other institutions. In this context, reader
performance variation is treated as a “random effect.”</p>
<p>Differences in test accuracy can manifest in various forms. The most
commonly analyzed difference is the variation in mean accuracy (e.g.,
ROC areas) between two tests. However, differences may also occur in the
distribution of accuracies or in how readers utilize the confidence
scale for decision-making in patient management. These latter
differences are critical even if summary measures of accuracy remain
consistent. The reviewed methods address these various differences, with
some focusing solely on mean differences and others extending to
additional variability.</p>
<p>In Table below, the features of five multireader methods are
delineated.</p>
<div class="figure" style="text-align: center">
<img src="02_Plots/MRMC/MRMC_KeyFeature.PNG" alt="Key Features of Five Methods" width="100%" />
<p class="caption">
Key Features of Five Methods
</p>
</div>
<div id="anova-of-pseudovalues-or-dorfman-berbaummetz-method-dbm"
class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> ANOVA of Pseudovalues
or “Dorfman-BerbaumMetz Method” (DBM)</h2>
<p>Dorfman et al. (12,14) introduced a novel approach to analyzing
multireader ROC data by employing ANOVA on pseudovalues derived via a
jackknife method. The core principle involves computing the jackknife
pseudovalue for each patient, defined as the weighted difference between
the accuracy estimated from all patients and the accuracy estimated
excluding the specific patient. These pseudovalues act as transformed
representations of the original dataset. Subsequently, a mixed-effects
ANOVA is utilized to assess the null hypothesis asserting uniform mean
accuracy across all diagnostic tests for the readers. Accuracy is
assessed through various summary measures such as sensitivity,
specificity, and the area under the ROC curve, which can be estimated
through both parametric and nonparametric methods.</p>
<p>The statistical model is presented in the first row of Table below,
adhering to the notation used by Dorfman et al. (12,14), where fixed
effects are symbolized by Greek letters and random effects by uppercase
English letters. The pseudovalue, denoted as Yijk, for the ith
diagnostic test, jth reader, and kth patient, is modeled as a linear
combination of an overall population mean, a fixed effect attributable
to the diagnostic test, a random effect associated with the jth reader
(which can also be treated as a fixed effect), a random effect due to
the kth patient, interactions among these effects, and a random error
component. The null hypothesis posits that the fixed effects of
treatment are equivalent across all diagnostic tests.</p>
<div class="figure" style="text-align: center">
<img src="02_Plots/MRMC/MRMC_STATModel.PNG" alt="Statistical Models of Five Methods" width="100%" />
<p class="caption">
Statistical Models of Five Methods
</p>
</div>
<p>Dorfman et al. assume that the random effects and the error term in
their model are normally and independently distributed. Notable
contributions from Charles Metz at the University of Chicago and the
late Donald Dorfman and his team at the University of Iowa include the
development of computer programs LABMRMC (15) and MRMC2.0 (16), both of
which are FORTRAN-based and operate on Windows platforms. These programs
share several components and facilitate the execution of the Dorfman et
al. MRMC analysis.</p>
<p>Furthermore, a new method for estimating sample size based on the
Dorfman-Berbaum-Metz (DBM) method has been recently devised and is
currently under evaluation, as detailed in the Appendix.</p>
<p>Song (17) proposed several modifications to the ANOVA of pseudovalues
method. These adjustments involve generating pseudovalues by
sequentially removing one patient with the disease and one without,
thereby maintaining a balanced design. This methodology is particularly
suited to studies with an equal number of diseased and non-diseased
patients. It is crucial to note that the outcomes of Song’s method are
contingent upon the randomization process utilized, thereby rendering
the results non-unique.</p>
<p>Song’s variation of the pseudovalues ANOVA method, which allows for
the treatment of readers as fixed but can also accommodate a
random-readers model, is exemplified in example 2 of this article. This
modification provides a nuanced approach that potentially enhances the
robustness and applicability of the statistical analysis in multireader
ROC studies.</p>
</div>
<div id="anova-with-corrected-f-test-or-obuchowskirockette-method-or"
class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> ANOVA with Corrected
F-test or “ObuchowskiRockette Method” (OR)</h2>
<p>Rather than adopting the approach of modeling jack-knife
pseudovalues, Obuchowski and Rockette (18,19) opted to model the
accuracy—such as the ROC curve area—of the j-th reader using the i-th
diagnostic test on the l-th reading occasion (refer to Table below).
Similar to the ANOVA of pseudovalues method, accuracy in their model can
be represented by any chosen accuracy index, with both parametric and
nonparametric estimates being viable options. Obuchowski and Rockette’s
methodology assumes that the accuracy indices adhere to a
treatment-by-reader ANOVA framework, incorporating errors that are
correlated.</p>
<div class="figure" style="text-align: center">
<img src="02_Plots/MRMC/MRMC_STATModel.PNG" alt="Statistical Models of Five Methods" width="100%" />
<p class="caption">
Statistical Models of Five Methods
</p>
</div>
<p>This correlation structure is defined by three specific
correlations:</p>
<ul>
<li><span class="math inline">\(r_1\)</span> represents the correlation
in the error terms for the same reader across different diagnostic
tests.</li>
<li><span class="math inline">\(r_2\)</span> is the correlation in the
error terms among different readers using the same diagnostic test.</li>
<li><span class="math inline">\(r_3\)</span> is the correlation in the
error terms among different readers across different diagnostic
tests.</li>
</ul>
<p>These coefficients are critical in structuring the analysis model’s
error terms, particularly in the context of an ANOVA that accounts for
correlated errors across multiple levels (diagnostic tests, readers, and
patient interactions). Each correlation is defined as a function of
various variance components:</p>
<ul>
<li><span class="math inline">\(r_1 = \frac{\sigma^2_C +
\sigma^2_{BC}}{\sigma^2_C + \sigma^2_{BC} + \sigma^2_{AC} +
\sigma^2_{ABC}}\)</span></li>
<li><span class="math inline">\(r_2 = \frac{\sigma^2_C +
\sigma^2_{AC}}{\sigma^2_C + \sigma^2_{BC} + \sigma^2_{AC} +
\sigma^2_{ABC}}\)</span></li>
<li><span class="math inline">\(r_3 = \frac{\sigma^2_C}{\sigma^2_C +
\sigma^2_{BC} + \sigma^2_{AC} + \sigma^2_{ABC}}\)</span></li>
</ul>
<p>These correlations are systematically linked to functions of the
interaction terms in the DBM model (10). Notably, these correlations
play a crucial role in determining the sample size needed for future
MRMC studies (19). In their analysis, Obuchowski and Rockette employ a
mixed-effects ANOVA on the accuracy indices using the OR method,
treating both patients and readers as random effects, although it’s
noteworthy that the model can also be applied with readers as fixed
effects (18). They have adapted the standard ANOVA F-tests to
accommodate the correlations among and between readers. These adapted
correlations for the F-tests must be derived from empirical data.
Publicly accessible FORTRAN software is available to facilitate these
analyses (20).</p>
</div>
<div id="multivariate-wmw-statistic" class="section level2"
number="4.3">
<h2><span class="header-section-number">4.3</span> Multivariate WMW
Statistic</h2>
<p>Song (17) proposed an extension to the nonparametric methodology
initially developed by DeLong et al. (6) for analyzing Receiver
Operating Characteristic (ROC) areas, adapting it to encompass a
multireader context. Whereas DeLong and colleagues employed U-statistics
to assess ROC areas across several diagnostic tests with a singular
reader per test, Song expanded this to a multivariate scenario to
incorporate data from multiple readers.</p>
<p>Specifically, Song devised a Wald statistic to test the hypothesis
that the accuracies of various diagnostic tests, as quantified by the
area under the ROC curve, are equivalent across these tests. This
extension does not stipulate a model, adhering instead to a fully
nonparametric approach. Nonetheless, in the construction of the test
statistic, readers are treated as a fixed effect, reflecting a
consistent influence across evaluations of diagnostic accuracy.</p>
<p>This method enhances the flexibility and applicability of ROC
analysis in settings where multiple readers are involved, providing a
robust tool for statistical assessment without the constraints imposed
by parametric assumptions.</p>
</div>
<div id="bootstrap-of-components-of-variance-or-bwc-method"
class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Bootstrap of
Components-of-Variance or “BWC Method”</h2>
<p>Beiden, Wagner, and Campbell (BWC method) adopted a
components-of-variance approach to the MRMC ROC problem, building on the
framework established by Roe and Metz (22). This method shares the same
foundational components-of-variance model as the DBM method described in
Table 3 and connects to the Obuchowski-Rockette (OR) method through
equation 1. Roe and Metz provided a comprehensive framework that applies
to a broad range of experimental designs involving a specific population
of readers and cases. BWC illustrated how this framework could be
applied to a subset of such experiments, allowing for the observation of
specific variances and the solution of a system of equations to estimate
the strengths of underlying, unobservable model variance components.</p>
<p>In practical applications where data sets are finite, bootstrap
experiments are employed as surrogates for population experiments. From
these, finite-sample estimates of variance components are derived using
the same system of equations. The BWC method utilizes the
jackknife-after-bootstrap technique for estimating the uncertainties of
these variance component estimates, which can then be used to size
future studies, as detailed in the Appendix.</p>
<p>Further, confidence intervals on the differences in accuracy
estimates across different modalities, averaged over readers, are
determined using the bias-corrected and accelerated bootstrap method.
This method is noted for its accuracy to the second order and is a more
refined, higher-order approach compared to standard methods.</p>
<p>The BWC method is fundamentally nonparametric and allows for the
accuracy measure, such as the ROC area, to be obtained either
parametrically or nonparametrically. While it incorporates
components-of-variance models similar to those used in ANOVA, the BWC
approach is distinct in its non-reliance on distributional assumptions
for model implementation and analysis.</p>
</div>
<div id="hierarchical-ordinal-regression-for-roc-curves"
class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Hierarchical Ordinal
Regression for ROC Curves</h2>
<p>Ishwaran and Gatsonis (13) have introduced Bayesian hierarchical
ordinal regression models (HROC) designed to effectively manage
multireader ROC data and other multilevel clustered data typically
encountered in radiology studies. The foundation of HROC models is a
latent variable framework wherein the continuous latent variable scale
is segmented into contiguous intervals defined by cutpoint parameters.
The number of these cutpoints is one less than the number of ordinal
categories. The placement of a latent variable within these intervals
dictates the ordinal response.</p>
<p>In scenarios involving repeated ordinal responses, the model
incorporates a multivariate latent variable. The specific ordinal
outcomes are then determined by the intervals into which these
multivariate latent variables fall. For repeated measurements, the HROC
models postulate a conditional multivariate normal distribution for the
latent variables. The mean and variance of this distribution are
structured around a Bayesian hierarchical prior, which is configured to
encapsulate potential influences from various sources such as patients,
individual readers, different hospitals, and other relevant factors.</p>
<p>Additionally, the HROC models are adaptable to include more than one
set of cutpoint parameters. This flexibility allows for unique cutpoints
across different readers, thus facilitating an analysis of how distinct
readers might interpret ordinal scales differently when assessing
diagnostic scans. This hierarchical structure also supports a diverse
array of marginal distributions for the unobserved latent variables; for
instance, the well-known binormal distribution is a particular case
within these models, and they can be extended to include normal location
mixture distributions as well.</p>
<p>Due to the complex nature of computing parameter estimates, their 95%
credible intervals (the Bayesian equivalent of 95% confidence
intervals), and Bayesian P values for diagnostic test comparisons, Gibbs
sampling algorithms are employed. These algorithms are crucial for the
effective estimation of the model parameters, ensuring a robust
analytical process within the Bayesian framework.</p>
</div>
<div id="summary" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> Summary</h2>
<p><strong>Methodological Spectrum:</strong></p>
<ul>
<li>At one end of the spectrum, Song’s method is entirely nonparametric,
making no assumptions about the sources of variability.</li>
<li>The DBM and BWC methods utilize simple additive models that account
for main effects and interactions among diagnostic tests, patients, and
readers but differ primarily in how they estimate model parameters.</li>
<li>The OR method, similar to DBM and BWC in its use of additive models,
specifically addresses correlations within and between readers and
diagnostic modalities.</li>
<li>The HROC method at the other end of the spectrum is more structured,
focusing on specific reader differences such as variations in readers’
cutpoints.</li>
</ul>
<p><strong>Comparative Findings:</strong></p>
<ul>
<li>Fixed-reader models generally produced narrower confidence intervals
(CIs) than random-reader models, which is expected due to fewer sources
of variability needing estimation in fixed-reader models.</li>
<li>The choice between fixed and random-reader models should depend on
whether results need to be generalized beyond the study’s specific
readers.</li>
<li>Nonparametric estimates tended to provide narrower CIs for
differences in mean accuracy compared to parametric estimates, an
unexpected but consistent finding across the datasets.</li>
</ul>
<p><strong>Specific Observations and Extensions:</strong></p>
<ul>
<li>The HROC method was notably comprehensive, capable of handling
covariates and detecting differences in reader cutpoints between tests,
providing a deeper understanding of variability among readers.</li>
<li>The BWC model was extended to better quantify differences in
variance structures across modalities, particularly useful in evaluating
the impact of computer-assisted modalities.</li>
</ul>
</div>
</div>
<div id="wafroc-auc" class="section level1" number="5">
<h1><span class="header-section-number">5</span> wAFROC AUC</h1>
<p><strong>Weighted Alternative Free-Response Receiver Operating
Characteristic (wAFROC)</strong></p>
<div id="dice-similarity-coefficient" class="section level2"
number="5.1">
<h2><span class="header-section-number">5.1</span> Dice Similarity
Coefficient</h2>
<p>The Dice similarity coefficient, also known as the Sørensen–Dice
index or simply Dice coefficient, is a statistical tool which measures
the similarity between two sets of data. This index has become arguably
the most broadly used tool in the validation of image segmentation
algorithms created with AI, but it is a much more general concept which
can be applied to sets of data for a variety of applications including
NLP.</p>
<p>The equation for this concept is: 2 * |X ∩ Y| / (|X/Y| + |Y/X|)</p>
<ul>
<li>where X and Y are two sets</li>
<li>a set with vertical bars either side refers to the cardinality of
the set, i.e. the number of elements in that set, e.g. |X| means the
number of elements in set X</li>
<li>∩ is used to represent the intersection of two sets, and means the
elements that are common to both sets</li>
</ul>
</div>
<div id="jaccard-index" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Jaccard Index</h2>
<p>The Jaccard Index, also known as the Jaccard similarity coefficient,
is a statistic used for gauging the similarity and diversity of sample
sets. It’s particularly common in the areas of data mining and computer
vision, especially for tasks like image segmentation and object
detection.</p>
<p>The Jaccard Index is defined as the size of the intersection divided
by the size of the union of two label sets. If we consider two sets, A
and B, where A is the predicted set of pixels (from an algorithm or
model) and B is the ground truth set of pixels (the actual area of
interest, such as a lesion or a particular feature in a medical image),
the Jaccard Index (JI) is calculated as:</p>
<p>JI = A ∩ B/A ∪ B</p>
<ul>
<li>Intersection (A ∩ B): This is the number of pixels that are common
to both the predicted area and the ground truth. In medical imaging,
this would be the area where both the CADe (Computer-Aided Detection)
system and the ground truth agree on a feature’s presence.<br />
</li>
<li>Union (A ∪ B): This is the total number of pixels that are in either
the predicted area or the ground truth, or in both.</li>
</ul>
<p>The Jaccard Index ranges from 0 to 1, where:</p>
<ul>
<li>A JI of 1 means perfect agreement between the prediction and the
ground truth.<br />
</li>
<li>A JI of 0 means no agreement.</li>
</ul>
<p>The Jaccard Index of 0.4 or 0.5 indicates the level of overlap
between the CADe system’s output and the actual truth. A higher JI
indicates a better performance of the CADe system in correctly
identifying and localizing lesions or pathologic features. The
percentages (57% for JI of 0.4 and 67% for JI of 0.5) are theoretical
values indicating the proportion of overlap between the device’s output
and the actual truth.</p>
</div>
</div>
<div id="package-imrmc-roc-analysis" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Package iMRMC ROC
Analysis</h1>
<p>The doIMRMC requires the input dataframe contain both the groud truth
and reader data.For more details on using doIMRMC see <a
href="https://cran.r-project.org/web/packages/iMRMC/iMRMC.pdf"
class="uri">https://cran.r-project.org/web/packages/iMRMC/iMRMC.pdf</a>.</p>
<ul>
<li><strong>Purpose</strong>: Designed for the validation of medical
imaging systems using Multi-Reader Multi-Case (MRMC) ROC analysis. It is
primarily used in studies where multiple readers interpret the same set
of images, and the goal is to assess diagnostic performance.</li>
<li><strong>Functionality</strong>:
<ul>
<li>Automates the computation of variance and confidence intervals for
AUC (Area Under the Curve) and other statistics.</li>
<li>Provides tools for handling both binary and ordinal ratings.</li>
<li>Offers extensive analysis options including fixed and random effects
models.</li>
</ul></li>
<li><strong>Differences</strong>:
<ul>
<li><code>iMRMC</code> focuses more broadly on ROC analysis rather than
specifically on localization performance (which is the focus of
AFROC).</li>
<li>The package streamlines complex statistical calculations and error
estimations that are otherwise manually coded in the script you
provided.</li>
</ul></li>
</ul>
<div id="data-structure" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Data Structure</h2>
<p>Multi-Reader Multi-Case (MRMC) Study</p>
<p>– Each case is reviewed by multiple clinicians/readers – Each reader
review multiple cases - Account for reader and case variability -
Generalize results to the population of readers and cases</p>
<div class="figure" style="text-align: center">
<img src="02_Plots/MRMC/MRMC_DS.PNG" alt="Multi-Reader Multi-Case Studies - Data Structure" width="100%" />
<p class="caption">
Multi-Reader Multi-Case Studies - Data Structure
</p>
</div>
<div id="u-statistics-analysis" class="section level3" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> U-statistics
Analysis</h3>
<p>The first variance estimation method uses U-statistics to provide
unbiased estimates of the variance components. This method lacks a
positivity constraint and can lead to negative estimates of variance
components and total variance.</p>
</div>
<div id="maximum-likelihood-estimate-mle-analysis"
class="section level3" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> Maximum Likelihood
Estimate (MLE) Analysis</h3>
<p>The second variance estimation method uses the non-parametric maximum
likelihood estimate (MLE) of the distribution of readers and cases,
which is the empirical distribution of readers and cases. Efron and
Tibshirani also refer to this estimation method as the “ideal”
bootstrap. The MLE estimate of variance components and total variance
cannot go negative. The tradeoff for positive variance estimates is a
positive bias.</p>
<p>The analysis results contained within the <code>MLEstat</code> data
frame are likely computed using V-statistics to approximate how reader
performance varies across the population from which the sample of
readers was drawn. This approximation uses the empirical distribution of
the data, and the nonparametric MLE approach ensures that the estimation
does not rely on predefined assumptions about the distribution
shape.</p>
<p>The mention of the “ideal bootstrap estimate” implies that the
methodology aims to achieve a robust estimate of the true distribution
of reader performance, incorporating the variability and uncertainty
inherent in empirical observations. Bootstrap methods involve resampling
the original data (with replacement) to create many simulated samples.
This process is used to estimate the sampling distribution of a
statistic. The ideal bootstrap estimate refers to an estimate of a
parameter that is considered to be highly accurate because it
incorporates variability across multiple simulated samples. It uses the
empirical distribution as the basis for generating these bootstrap
samples.</p>
</div>
</div>
<div id="apply-the-doimrmc" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Apply the
doIMRMC</h2>
<pre class="r"><code>library(iMRMC)
# Create a sample configuration file
config &lt;- sim.gRoeMetz.config()
# Simulate an MRMC ROC data set
dFrame.imrmc &lt;- sim.gRoeMetz(config)

## Apply sim.gRoeMetz to simulate MRMC data
## https://washstat.org/materials.html/20190923/Wen.pdf
## Page 26/29
df.MRMC &lt;- sim.gRoeMetz(config)
ggplot(subset(df.MRMC, modalityID %in% c(&quot;testA&quot;, &quot;testB&quot;)),
       aes(x=score,
           color=factor(unlist(lapply(as.character(caseID),
                                       function(x){strsplit(x, &#39;Case&#39;)[[1]][1]}))))) +
  geom_density(position = &quot;identity&quot;, alpha=0.2) +
  facet_grid(~modalityID) +
  labs(x = &quot;MRMC Reading Score&quot;, y = &quot;Density&quot;) +
  theme(legend.title = element_blank())</code></pre>
<p><img src="04-Design-Diagnostic-Study-MRMC_files/figure-html/unnamed-chunk-13-1.png" width="960" /></p>
<pre class="r"><code># Analyze the MRMC ROC data
result &lt;- doIMRMC(df.MRMC)

ROC.data &lt;- rbind(data.frame(FPF = result$ROC$testA.pooled$fpf,
                             TPF = result$ROC$testA.pooled$tpf),
                  data.frame(FPF = result$ROC$testB.pooled$fpf,
                             TPF = result$ROC$testB.pooled$tpf))
ROC.data$modality &lt;- rep(c(&quot;testA&quot;, &quot;testB&quot;), each = nrow(ROC.data)/2)

ggplot(ROC.data, aes(x=FPF,y=TPF,color=modality)) +
  geom_point(size=0.5) +
  labs(x=&quot;False Positive Fraction&quot;,y=&quot;True Positive Fraction&quot;)</code></pre>
<p><img src="04-Design-Diagnostic-Study-MRMC_files/figure-html/unnamed-chunk-13-2.png" width="960" /></p>
<pre class="r"><code>ggplot(ROC.data, aes(x=FPF,y=TPF,color=modality)) +
  geom_point(size=0.5) +
  geom_line() +
  labs(title = &quot;Reader Average of Empirical AUC Curve for Test A vs Test B&quot;,
       x=&quot;False Positive Rate (1 - specificity)&quot;,
       y=&quot;True Positive Rate (Sensitivity)&quot;,
       color = &quot;&quot;) +
  theme_bw() +
  theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="04-Design-Diagnostic-Study-MRMC_files/figure-html/unnamed-chunk-13-3.png" width="960" /></p>
<pre class="r"><code>library(knitr)
library(kableExtra)
AUCDf_MLEstat &lt;- data.frame(rbind(result$MLEstat$AUCA[1:2],result$MLEstat$varAUCA[1:2], 
                          sqrt(result$MLEstat$varAUCA[1:2])), 
                    row.names = c(&quot;AUC&quot;, &quot;variance of AUC&quot;, &quot;SE of AUC&quot;))
names(AUCDf_MLEstat) &lt;- result$MLEstat$modalityA[1:2]
AUCDf_MLEstat %&gt;%
  kable(caption = &quot;AUC for different modalities : MLEstat&quot;, format = &quot;html&quot;) %&gt;%
  kable_styling(latex_options = &quot;striped&quot;)</code></pre>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
AUC for different modalities : MLEstat
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
testA
</th>
<th style="text-align:right;">
testB
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
AUC
</td>
<td style="text-align:right;">
0.7277500
</td>
<td style="text-align:right;">
0.7673750
</td>
</tr>
<tr>
<td style="text-align:left;">
variance of AUC
</td>
<td style="text-align:right;">
0.0036929
</td>
<td style="text-align:right;">
0.0046352
</td>
</tr>
<tr>
<td style="text-align:left;">
SE of AUC
</td>
<td style="text-align:right;">
0.0607693
</td>
<td style="text-align:right;">
0.0680820
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>AUCDf_Ustat &lt;- data.frame(rbind(result$Ustat$AUCA[1:2],result$Ustat$varAUCA[1:2], 
                          sqrt(result$Ustat$varAUCA[1:2])), 
                    row.names = c(&quot;AUC&quot;, &quot;variance of AUC&quot;, &quot;SE of AUC&quot;))
names(AUCDf_Ustat) &lt;- result$Ustat$modalityA[1:2]
AUCDf_Ustat %&gt;%
  kable(caption = &quot;AUC for different modalities : Ustat&quot;, format = &quot;html&quot;) %&gt;%
  kable_styling(latex_options = &quot;striped&quot;)</code></pre>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
AUC for different modalities : Ustat
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
testA
</th>
<th style="text-align:right;">
testB
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
AUC
</td>
<td style="text-align:right;">
0.7277500
</td>
<td style="text-align:right;">
0.7673750
</td>
</tr>
<tr>
<td style="text-align:left;">
variance of AUC
</td>
<td style="text-align:right;">
0.0036062
</td>
<td style="text-align:right;">
0.0048726
</td>
</tr>
<tr>
<td style="text-align:left;">
SE of AUC
</td>
<td style="text-align:right;">
0.0600514
</td>
<td style="text-align:right;">
0.0698041
</td>
</tr>
</tbody>
</table>
<p>From the above tables, we can see that one of the variance estimation
in the Ustat is negative. This is because this color scale study is a
split-plot study but not fully crossed. That is, one reader only scored
part of the cases, not all of them.</p>
</div>
<div id="difference-of-auc" class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> Difference of
AUC</h2>
<p>The difference of AUC among different modalities are also estimated
by the two different estimation methods.</p>
<p>In the <code>iMRMC</code> package, <code>Ustat</code> and
<code>MLEstat</code> refer to different statistical methods used for
analyzing medical imaging data, particularly in the context of
evaluating the performance of imaging modalities.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Ustat</strong>: This is based on the non-parametric
U-statistic, which is used to compare two medical imaging modalities. It
provides an estimate of the area under the ROC curve (AUC) that does not
rely on any assumptions about the distribution of the underlying data.
U-statistic is useful for handling small sample sizes and non-normal
data distributions. The first variance estimation method uses
U-statistics to provide unbiased estimates of the variance components.
This method lacks a positivity constraint and can lead to negative
estimates of variance components and total variance. The second variance
estimation method uses the non-parametric maximum likelihood estimate
(MLE) of the distribution of readers and cases, which is the empirical
distribution of readers and cases. Efron and Tibshirani also refer to
this estimation method as the “ideal” bootstrap. The MLE estimate of
variance components and total variance cannot go negative. The tradeoff
for positive variance estimates is a positive bias.</p></li>
<li><p><strong>MLEstat</strong>: This represents the maximum likelihood
estimate (MLE) of the AUC. The MLE approach assumes a parametric model
for the underlying data, typically a binormal model for ROC analysis.
MLEstat can be more efficient in terms of statistical power compared to
non-parametric methods like Ustat, but it relies on the correctness of
the assumed data distribution.</p></li>
</ol>
<pre class="r"><code>AUCDf_MLEstat_Diff &lt;- data.frame(rbind(result$MLEstat$AUCAminusAUCB[3],result$MLEstat$varAUCAminusAUCB[3],
                          sqrt(result$MLEstat$varAUCAminusAUCB[3]), 
              result$MLEstat$AUCAminusAUCB[3] - 1.96 * sqrt(result$MLEstat$varAUCAminusAUCB[3]),
              result$MLEstat$AUCAminusAUCB[3] + 1.96 * sqrt(result$MLEstat$varAUCAminusAUCB[3])),
              row.names = c(&quot;difference of AUC&quot;, &quot;variance of difference of AUC&quot;, 
                            &quot;SE of different of AUC&quot;, &quot;95% CI lower bound&quot;, &quot;95% CI upper bound&quot;))
names(AUCDf_MLEstat_Diff) &lt;- paste(result$MLEstat$modalityA[3], result$MLEstat$modalityB[3], sep = &quot; vs. &quot;)
AUCDf_MLEstat_Diff%&gt;%
  kable(caption = &quot;Difference of AUC among different modalities : MLEstat&quot;, format = &quot;html&quot;) %&gt;%
  kable_styling(latex_options = &quot;striped&quot;)</code></pre>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
Difference of AUC among different modalities : MLEstat
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
testA vs. testB
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
difference of AUC
</td>
<td style="text-align:right;">
-0.0396250
</td>
</tr>
<tr>
<td style="text-align:left;">
variance of difference of AUC
</td>
<td style="text-align:right;">
0.0061634
</td>
</tr>
<tr>
<td style="text-align:left;">
SE of different of AUC
</td>
<td style="text-align:right;">
0.0785072
</td>
</tr>
<tr>
<td style="text-align:left;">
95% CI lower bound
</td>
<td style="text-align:right;">
-0.1934990
</td>
</tr>
<tr>
<td style="text-align:left;">
95% CI upper bound
</td>
<td style="text-align:right;">
0.1142490
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>AUCDf_Ustat_Diff &lt;- data.frame(rbind(result$Ustat$AUCAminusAUCB[3],result$Ustat$varAUCAminusAUCB[3],
                          sqrt(result$Ustat$varAUCAminusAUCB[3]), 
              result$Ustat$AUCAminusAUCB[3] - 1.96 * sqrt(result$Ustat$varAUCAminusAUCB[3]),
              result$Ustat$AUCAminusAUCB[3] + 1.96 * sqrt(result$Ustat$varAUCAminusAUCB[3])),
              row.names = c(&quot;difference of AUC&quot;, &quot;variance of difference of AUC&quot;, 
                            &quot;SE of different of AUC&quot;, &quot;95% CI lower bound&quot;, &quot;95% CI upper bound&quot;))
names(AUCDf_Ustat_Diff) &lt;- paste(result$Ustat$modalityA[3], result$Ustat$modalityB[3], sep = &quot; vs. &quot;)
AUCDf_Ustat_Diff%&gt;%
  kable(caption = &quot;Difference of AUC among different modalities : Ustat&quot;, format = &quot;html&quot;) %&gt;%
  kable_styling(latex_options = &quot;striped&quot;) </code></pre>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
Difference of AUC among different modalities : Ustat
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
testA vs. testB
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
difference of AUC
</td>
<td style="text-align:right;">
-0.0396250
</td>
</tr>
<tr>
<td style="text-align:left;">
variance of difference of AUC
</td>
<td style="text-align:right;">
0.0065055
</td>
</tr>
<tr>
<td style="text-align:left;">
SE of different of AUC
</td>
<td style="text-align:right;">
0.0806570
</td>
</tr>
<tr>
<td style="text-align:left;">
95% CI lower bound
</td>
<td style="text-align:right;">
-0.1977126
</td>
</tr>
<tr>
<td style="text-align:left;">
95% CI upper bound
</td>
<td style="text-align:right;">
0.1184626
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="package-mitoticfigurecounts" class="section level1" number="7">
<h1><span class="header-section-number">7</span> Package
mitoticFigureCounts</h1>
<div id="domrmcaucorcluster" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span>
doMRMCaucORcluster</h2>
<ul>
<li><a
href="https://github.com/DIDSR/mitoticFigureCounts/tree/5675b0beab5670a565b7614db356065bf0b0663d"
class="uri">https://github.com/DIDSR/mitoticFigureCounts/tree/5675b0beab5670a565b7614db356065bf0b0663d</a></li>
<li><a href="https://github.com/DIDSR/iMRMC/issues/147"
class="uri">https://github.com/DIDSR/iMRMC/issues/147</a></li>
</ul>
<pre class="r"><code>doMRMCaucORcluster &lt;- function(df) {
  modalities &lt;- levels(df$modalityID)
  nModalities &lt;- nlevels(df$modalityID)
  readers &lt;- levels(df$readerID)
  nReaders &lt;- nlevels(df$readerID)

  # Split the data frame by readers and modalities
  df.byModalityReader &lt;- split(df, list(df$readerID, df$modalityID))

  # Calculate covariances for each reader/modality combination ####
  auc &lt;- vector(mode = &quot;numeric&quot;, nModalities*nReaders)
  cov &lt;- matrix(-1.0,
                nrow = nModalities*nReaders,
                ncol = nModalities*nReaders)
  for (i in 1:(nModalities*nReaders)) {
    for (j in i:(nModalities*nReaders)) {

      print(i)
      print(j)
      df.merge &lt;- merge(df.byModalityReader[[i]],
                        df.byModalityReader[[j]],
                        by = &quot;targetID&quot;, all = TRUE)

      result &lt;- doAUCcluster(
        predictor1 = df.merge$score.x,
        predictor2 = df.merge$score.y,
        response   = df.merge$truth.x,
        clusterID  = df.merge$caseID.x,
        alpha      = 0.05)

      cov[i,j] &lt;- (result$auc.var.A + result$auc.var.B
                   - result$auc.var.AminusB)/2
      cov[j,i] &lt;- cov[i,j]
    }
    auc[i] &lt;- result$auc.A
  }

  # mrmcAnalysisOR ####
  auc &lt;- matrix(auc, nrow = 2, ncol = nReaders, byrow = TRUE)

  aucMTG.OR.new &lt;- mrmcAnalysisOR(auc, cov)

  aucMTG.OR.new$botCI &lt;- aucMTG.OR.new$theta.i - qt(0.975, df = aucMTG.OR.new$df.sgl) * aucMTG.OR.new$se.i
  aucMTG.OR.new$topCI &lt;- aucMTG.OR.new$theta.i + qt(0.975, df = aucMTG.OR.new$df.sgl) * aucMTG.OR.new$se.i

  print(aucMTG.OR.new)
}

# Initialize data #### 
df.orig &lt;- read.csv(&quot;./01_Datasets/dfClassify20180627.csv&quot;,
                    sep=&quot;,&quot;,
                    header=TRUE)
# df.orig &lt;- mitoticFigureCounts::dfClassify20180627

# Convert data to list-mode
readers &lt;- c(
  &quot;observer.1&quot;,
  &quot;observer.2&quot;,
  &quot;observer.3&quot;,
  &quot;observer.4&quot;,
  &quot;observer.5&quot;
)

df.convert &lt;- convertDF(df.orig, &quot;matrixWithTruth&quot;, &quot;listWithTruth&quot;, readers, nameTruth)
df.convert$caseID &lt;- df.convert$targetID
df.convert$readerID &lt;- factor(df.convert$readerID)
df.convert$locationID &lt;- df.convert$roiID
df.convert$modalityID &lt;- factor(df.convert$modalityID)

# Split the data by modality
df.convert &lt;- split(df.convert, df.convert$modalityID)

# Analyze modality.A ####
start &lt;- proc.time()

df.A &lt;- rbind(df.convert$microscope, df.convert$scanner.A)
df.A$modalityID &lt;- factor(df.A$modalityID)
result.A &lt;- doMRMCaucORcluster(df.A)</code></pre>
<pre><code>## [1] 1
## [1] 1
## [1] 1
## [1] 2
## [1] 1
## [1] 3
## [1] 1
## [1] 4
## [1] 1
## [1] 5
## [1] 1
## [1] 6
## [1] 1
## [1] 7
## [1] 1
## [1] 8
## [1] 1
## [1] 9
## [1] 1
## [1] 10
## [1] 2
## [1] 2
## [1] 2
## [1] 3
## [1] 2
## [1] 4
## [1] 2
## [1] 5
## [1] 2
## [1] 6
## [1] 2
## [1] 7
## [1] 2
## [1] 8
## [1] 2
## [1] 9
## [1] 2
## [1] 10
## [1] 3
## [1] 3
## [1] 3
## [1] 4
## [1] 3
## [1] 5
## [1] 3
## [1] 6
## [1] 3
## [1] 7
## [1] 3
## [1] 8
## [1] 3
## [1] 9
## [1] 3
## [1] 10
## [1] 4
## [1] 4
## [1] 4
## [1] 5
## [1] 4
## [1] 6
## [1] 4
## [1] 7
## [1] 4
## [1] 8
## [1] 4
## [1] 9
## [1] 4
## [1] 10
## [1] 5
## [1] 5
## [1] 5
## [1] 6
## [1] 5
## [1] 7
## [1] 5
## [1] 8
## [1] 5
## [1] 9
## [1] 5
## [1] 10
## [1] 6
## [1] 6
## [1] 6
## [1] 7
## [1] 6
## [1] 8
## [1] 6
## [1] 9
## [1] 6
## [1] 10
## [1] 7
## [1] 7
## [1] 7
## [1] 8
## [1] 7
## [1] 9
## [1] 7
## [1] 10
## [1] 8
## [1] 8
## [1] 8
## [1] 9
## [1] 8
## [1] 10
## [1] 9
## [1] 9
## [1] 9
## [1] 10
## [1] 10
## [1] 10
## $F_stat
## [1] 24.13445
## 
## $df.H
## [1] 8.30231
## 
## $p
## [1] 0.001054525
## 
## $theta.i
## [1] 0.7983317 0.7009676
## 
## $df.sgl
## [1]   26.30836 4815.92855
## 
## $se.i
## [1] 0.02136731 0.02137396
## 
## $se.dif
## [1] 0.01981891
## 
## $covOR
##          varR         varTR          cov1          cov2          cov3 
## -2.831249e-04  9.032934e-05  3.805371e-04  3.611080e-04  3.010335e-04 
##          varE        varR.1        varR.2        cov2.1        cov2.2 
##  1.031883e-03  2.198574e-04 -6.054485e-04  2.785362e-04  4.436798e-04 
##        varE.1        varE.2 
##  9.488074e-04  1.114959e-03 
## 
## $theta.hat
##           [,1]      [,2]      [,3]      [,4]      [,5]
## [1,] 0.7640974 0.7778612 0.8058058 0.8419253 0.8019686
## [2,] 0.7129630 0.6996997 0.7035369 0.6909409 0.6976977
## 
## $cov.hat
##               [,1]         [,2]         [,3]         [,4]         [,5]
##  [1,] 0.0008886398 0.0002800444 0.0003713059 0.0002753974 0.0003151405
##  [2,] 0.0002800444 0.0011171121 0.0002878674 0.0002193651 0.0001089266
##  [3,] 0.0003713059 0.0002878674 0.0008919458 0.0004000620 0.0003250308
##  [4,] 0.0002753974 0.0002193651 0.0004000620 0.0008527298 0.0002022214
##  [5,] 0.0003151405 0.0001089266 0.0003250308 0.0002022214 0.0009936096
##  [6,] 0.0004599338 0.0003022434 0.0004542182 0.0002345308 0.0003473709
##  [7,] 0.0001365180 0.0003616237 0.0003520178 0.0003133894 0.0001401003
##  [8,] 0.0004240792 0.0001780302 0.0004792379 0.0001640934 0.0003432123
##  [9,] 0.0003800614 0.0002882986 0.0004072226 0.0003118814 0.0003225292
## [10,] 0.0003550457 0.0002165725 0.0004234828 0.0002376537 0.0002900087
##               [,6]         [,7]         [,8]         [,9]        [,10]
##  [1,] 0.0004599338 0.0001365180 0.0004240792 0.0003800614 0.0003550457
##  [2,] 0.0003022434 0.0003616237 0.0001780302 0.0002882986 0.0002165725
##  [3,] 0.0004542182 0.0003520178 0.0004792379 0.0004072226 0.0004234828
##  [4,] 0.0002345308 0.0003133894 0.0001640934 0.0003118814 0.0002376537
##  [5,] 0.0003473709 0.0001401003 0.0003432123 0.0003225292 0.0002900087
##  [6,] 0.0010704991 0.0003187929 0.0005181985 0.0006470766 0.0005853567
##  [7,] 0.0003187929 0.0013456309 0.0002383782 0.0003243063 0.0003418173
##  [8,] 0.0005181985 0.0002383782 0.0012615782 0.0004552788 0.0004367112
##  [9,] 0.0006470766 0.0003243063 0.0004552788 0.0009451042 0.0005708816
## [10,] 0.0005853567 0.0003418173 0.0004367112 0.0005708816 0.0009519836
## 
## $botCI
## [1] 0.7544356 0.6590649
## 
## $topCI
## [1] 0.8422278 0.7428703</code></pre>
<pre class="r"><code>finish &lt;- proc.time()
print(finish - start)</code></pre>
<pre><code>##    user  system elapsed 
##   3.239   0.083   3.369</code></pre>
<pre class="r"><code># Analyze modality.B ####
start &lt;- proc.time()

df.B &lt;- rbind(df.convert$microscope, df.convert$scanner.B)
df.B$modalityID &lt;- factor(df.B$modalityID)
result.B &lt;- doMRMCaucORcluster(df.B)</code></pre>
<pre><code>## [1] 1
## [1] 1
## [1] 1
## [1] 2
## [1] 1
## [1] 3
## [1] 1
## [1] 4
## [1] 1
## [1] 5
## [1] 1
## [1] 6
## [1] 1
## [1] 7
## [1] 1
## [1] 8
## [1] 1
## [1] 9
## [1] 1
## [1] 10
## [1] 2
## [1] 2
## [1] 2
## [1] 3
## [1] 2
## [1] 4
## [1] 2
## [1] 5
## [1] 2
## [1] 6
## [1] 2
## [1] 7
## [1] 2
## [1] 8
## [1] 2
## [1] 9
## [1] 2
## [1] 10
## [1] 3
## [1] 3
## [1] 3
## [1] 4
## [1] 3
## [1] 5
## [1] 3
## [1] 6
## [1] 3
## [1] 7
## [1] 3
## [1] 8
## [1] 3
## [1] 9
## [1] 3
## [1] 10
## [1] 4
## [1] 4
## [1] 4
## [1] 5
## [1] 4
## [1] 6
## [1] 4
## [1] 7
## [1] 4
## [1] 8
## [1] 4
## [1] 9
## [1] 4
## [1] 10
## [1] 5
## [1] 5
## [1] 5
## [1] 6
## [1] 5
## [1] 7
## [1] 5
## [1] 8
## [1] 5
## [1] 9
## [1] 5
## [1] 10
## [1] 6
## [1] 6
## [1] 6
## [1] 7
## [1] 6
## [1] 8
## [1] 6
## [1] 9
## [1] 6
## [1] 10
## [1] 7
## [1] 7
## [1] 7
## [1] 8
## [1] 7
## [1] 9
## [1] 7
## [1] 10
## [1] 8
## [1] 8
## [1] 8
## [1] 9
## [1] 8
## [1] 10
## [1] 9
## [1] 9
## [1] 9
## [1] 10
## [1] 10
## [1] 10
## $F_stat
## [1] 10.6736
## 
## $df.H
## [1] 9.855737
## 
## $p
## [1] 0.008633451
## 
## $theta.i
## [1] 0.7983317 0.7349349
## 
## $df.sgl
## [1]  26.30836 557.88604
## 
## $se.i
## [1] 0.02136731 0.02313488
## 
## $se.dif
## [1] 0.0194049
## 
## $covOR
##          varR         varTR          cov1          cov2          cov3 
## -1.105293e-04  4.397234e-05  3.850625e-04  3.842192e-04  3.158881e-04 
##          varE        varR.1        varR.2        cov2.1        cov2.2 
##  1.009141e-03  2.198574e-04 -3.529714e-04  2.785362e-04  4.899023e-04 
##        varE.1        varE.2 
##  9.488074e-04  1.069475e-03 
## 
## $theta.hat
##           [,1]      [,2]      [,3]      [,4]      [,5]
## [1,] 0.7640974 0.7778612 0.8058058 0.8419253 0.8019686
## [2,] 0.7429096 0.7147147 0.7376543 0.7258926 0.7535035
## 
## $cov.hat
##               [,1]         [,2]         [,3]         [,4]         [,5]
##  [1,] 0.0008886398 0.0002800444 0.0003713059 0.0002753974 0.0003151405
##  [2,] 0.0002800444 0.0011171121 0.0002878674 0.0002193651 0.0001089266
##  [3,] 0.0003713059 0.0002878674 0.0008919458 0.0004000620 0.0003250308
##  [4,] 0.0002753974 0.0002193651 0.0004000620 0.0008527298 0.0002022214
##  [5,] 0.0003151405 0.0001089266 0.0003250308 0.0002022214 0.0009936096
##  [6,] 0.0003635490 0.0003201075 0.0004931587 0.0002132884 0.0003056105
##  [7,] 0.0001983410 0.0003925291 0.0003135956 0.0002796060 0.0001458199
##  [8,] 0.0003692806 0.0002807318 0.0005062129 0.0003404255 0.0003163333
##  [9,] 0.0003891151 0.0003601488 0.0004719073 0.0003333560 0.0003109619
## [10,] 0.0002635600 0.0002422056 0.0004129422 0.0002906221 0.0003296657
##               [,6]         [,7]         [,8]         [,9]        [,10]
##  [1,] 0.0003635490 0.0001983410 0.0003692806 0.0003891151 0.0002635600
##  [2,] 0.0003201075 0.0003925291 0.0002807318 0.0003601488 0.0002422056
##  [3,] 0.0004931587 0.0003135956 0.0005062129 0.0004719073 0.0004129422
##  [4,] 0.0002132884 0.0002796060 0.0003404255 0.0003333560 0.0002906221
##  [5,] 0.0003056105 0.0001458199 0.0003163333 0.0003109619 0.0003296657
##  [6,] 0.0011722580 0.0004990524 0.0004523130 0.0004926864 0.0005210242
##  [7,] 0.0004990524 0.0011591778 0.0004542182 0.0003623643 0.0005106737
##  [8,] 0.0004523130 0.0004542182 0.0009314197 0.0005043077 0.0005727739
##  [9,] 0.0004926864 0.0003623643 0.0005043077 0.0010365327 0.0005296096
## [10,] 0.0005210242 0.0005106737 0.0005727739 0.0005296096 0.0010479849
## 
## $botCI
## [1] 0.7544356 0.6894928
## 
## $topCI
## [1] 0.8422278 0.7803770</code></pre>
<pre class="r"><code>finish &lt;- proc.time()
print(finish - start)</code></pre>
<pre><code>##    user  system elapsed 
##   2.757   0.066   2.835</code></pre>
<pre class="r"><code># Analyze modality.C ####
start &lt;- proc.time()

df.C &lt;- rbind(df.convert$microscope, df.convert$scanner.C)
df.C$modalityID &lt;- factor(df.C$modalityID)
result.C &lt;- doMRMCaucORcluster(df.C)</code></pre>
<pre><code>## [1] 1
## [1] 1
## [1] 1
## [1] 2
## [1] 1
## [1] 3
## [1] 1
## [1] 4
## [1] 1
## [1] 5
## [1] 1
## [1] 6
## [1] 1
## [1] 7
## [1] 1
## [1] 8
## [1] 1
## [1] 9
## [1] 1
## [1] 10
## [1] 2
## [1] 2
## [1] 2
## [1] 3
## [1] 2
## [1] 4
## [1] 2
## [1] 5
## [1] 2
## [1] 6
## [1] 2
## [1] 7
## [1] 2
## [1] 8
## [1] 2
## [1] 9
## [1] 2
## [1] 10
## [1] 3
## [1] 3
## [1] 3
## [1] 4
## [1] 3
## [1] 5
## [1] 3
## [1] 6
## [1] 3
## [1] 7
## [1] 3
## [1] 8
## [1] 3
## [1] 9
## [1] 3
## [1] 10
## [1] 4
## [1] 4
## [1] 4
## [1] 5
## [1] 4
## [1] 6
## [1] 4
## [1] 7
## [1] 4
## [1] 8
## [1] 4
## [1] 9
## [1] 4
## [1] 10
## [1] 5
## [1] 5
## [1] 5
## [1] 6
## [1] 5
## [1] 7
## [1] 5
## [1] 8
## [1] 5
## [1] 9
## [1] 5
## [1] 10
## [1] 6
## [1] 6
## [1] 6
## [1] 7
## [1] 6
## [1] 8
## [1] 6
## [1] 9
## [1] 6
## [1] 10
## [1] 7
## [1] 7
## [1] 7
## [1] 8
## [1] 7
## [1] 9
## [1] 7
## [1] 10
## [1] 8
## [1] 8
## [1] 8
## [1] 9
## [1] 8
## [1] 10
## [1] 9
## [1] 9
## [1] 9
## [1] 10
## [1] 10
## [1] 10
## $F_stat
## [1] 25.87
## 
## $df.H
## [1] 8.5309
## 
## $p
## [1] 0.0007758205
## 
## $theta.i
## [1] 0.7983317 0.6939940
## 
## $df.sgl
## [1] 26.30836 23.85604
## 
## $se.i
## [1] 0.02136731 0.02815730
## 
## $se.dif
## [1] 0.02051365
## 
## $covOR
##         varR        varTR         cov1         cov2         cov3         varE 
## 0.0005038157 0.0001717699 0.0003395247 0.0003733608 0.0003070308 0.0009544594 
##       varR.1       varR.2       cov2.1       cov2.2       varE.1       varE.2 
## 0.0002198574 0.0011313140 0.0002785362 0.0004681854 0.0009488074 0.0009601115 
## 
## $theta.hat
##           [,1]      [,2]      [,3]      [,4]      [,5]
## [1,] 0.7640974 0.7778612 0.8058058 0.8419253 0.8019686
## [2,] 0.6847681 0.6312980 0.7173841 0.6988655 0.7376543
## 
## $cov.hat
##               [,1]         [,2]         [,3]         [,4]         [,5]
##  [1,] 0.0008886398 0.0002800444 0.0003713059 0.0002753974 0.0003151405
##  [2,] 0.0002800444 0.0011171121 0.0002878674 0.0002193651 0.0001089266
##  [3,] 0.0003713059 0.0002878674 0.0008919458 0.0004000620 0.0003250308
##  [4,] 0.0002753974 0.0002193651 0.0004000620 0.0008527298 0.0002022214
##  [5,] 0.0003151405 0.0001089266 0.0003250308 0.0002022214 0.0009936096
##  [6,] 0.0003795851 0.0003360848 0.0003595145 0.0002699671 0.0002261605
##  [7,] 0.0002302651 0.0003922809 0.0003831014 0.0002770705 0.0001646197
##  [8,] 0.0003054905 0.0002413320 0.0003648739 0.0003085304 0.0002287781
##  [9,] 0.0003040616 0.0003924751 0.0004068663 0.0002908292 0.0002559272
## [10,] 0.0002767223 0.0004195693 0.0004599338 0.0002941463 0.0002700541
##               [,6]         [,7]         [,8]         [,9]        [,10]
##  [1,] 0.0003795851 0.0002302651 0.0003054905 0.0003040616 0.0002767223
##  [2,] 0.0003360848 0.0003922809 0.0002413320 0.0003924751 0.0004195693
##  [3,] 0.0003595145 0.0003831014 0.0003648739 0.0004068663 0.0004599338
##  [4,] 0.0002699671 0.0002770705 0.0003085304 0.0002908292 0.0002941463
##  [5,] 0.0002261605 0.0001646197 0.0002287781 0.0002559272 0.0002700541
##  [6,] 0.0009803504 0.0003582057 0.0005402824 0.0005268679 0.0005746791
##  [7,] 0.0003582057 0.0009314665 0.0004027997 0.0004805146 0.0004580286
##  [8,] 0.0005402824 0.0004027997 0.0009257911 0.0003752824 0.0004146072
##  [9,] 0.0005268679 0.0004805146 0.0003752824 0.0010315296 0.0005505869
## [10,] 0.0005746791 0.0004580286 0.0004146072 0.0005505869 0.0009314197
## 
## $botCI
## [1] 0.7544356 0.6358616
## 
## $topCI
## [1] 0.8422278 0.7521264</code></pre>
<pre class="r"><code>finish &lt;- proc.time()
print(finish - start)</code></pre>
<pre><code>##    user  system elapsed 
##   2.808   0.047   2.867</code></pre>
<pre class="r"><code># Analyze modality.D ####
start &lt;- proc.time()

df.D &lt;- rbind(df.convert$microscope, df.convert$scanner.D)
df.D$modalityID &lt;- factor(df.D$modalityID)
names(df.D)</code></pre>
<pre><code>##  [1] &quot;targetID&quot;             &quot;cell.mark&quot;            &quot;wsiName&quot;             
##  [4] &quot;roiID&quot;                &quot;cellID.mskcc20171103&quot; &quot;xCell&quot;               
##  [7] &quot;yCell&quot;                &quot;truth&quot;                &quot;modalityID&quot;          
## [10] &quot;readerID&quot;             &quot;score&quot;                &quot;caseID&quot;              
## [13] &quot;locationID&quot;</code></pre>
<pre class="r"><code>df.D2 &lt;- df.D %&gt;% select(truth,modalityID,readerID,score,caseID,targetID)
str(df.D2)</code></pre>
<pre><code>## &#39;data.frame&#39;:    1550 obs. of  6 variables:
##  $ truth     : int  1 0 0 0 0 1 1 1 0 0 ...
##  $ modalityID: Factor w/ 2 levels &quot;microscope&quot;,&quot;scanner.D&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ readerID  : Factor w/ 5 levels &quot;observer.1&quot;,&quot;observer.2&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ score     : int  1 0 0 0 0 0 0 0 0 0 ...
##  $ caseID    : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ targetID  : int  1 2 3 4 5 6 7 8 9 10 ...</code></pre>
<pre class="r"><code>result.D &lt;- doMRMCaucORcluster(df.D2) </code></pre>
<pre><code>## [1] 1
## [1] 1
## [1] 1
## [1] 2
## [1] 1
## [1] 3
## [1] 1
## [1] 4
## [1] 1
## [1] 5
## [1] 1
## [1] 6
## [1] 1
## [1] 7
## [1] 1
## [1] 8
## [1] 1
## [1] 9
## [1] 1
## [1] 10
## [1] 2
## [1] 2
## [1] 2
## [1] 3
## [1] 2
## [1] 4
## [1] 2
## [1] 5
## [1] 2
## [1] 6
## [1] 2
## [1] 7
## [1] 2
## [1] 8
## [1] 2
## [1] 9
## [1] 2
## [1] 10
## [1] 3
## [1] 3
## [1] 3
## [1] 4
## [1] 3
## [1] 5
## [1] 3
## [1] 6
## [1] 3
## [1] 7
## [1] 3
## [1] 8
## [1] 3
## [1] 9
## [1] 3
## [1] 10
## [1] 4
## [1] 4
## [1] 4
## [1] 5
## [1] 4
## [1] 6
## [1] 4
## [1] 7
## [1] 4
## [1] 8
## [1] 4
## [1] 9
## [1] 4
## [1] 10
## [1] 5
## [1] 5
## [1] 5
## [1] 6
## [1] 5
## [1] 7
## [1] 5
## [1] 8
## [1] 5
## [1] 9
## [1] 5
## [1] 10
## [1] 6
## [1] 6
## [1] 6
## [1] 7
## [1] 6
## [1] 8
## [1] 6
## [1] 9
## [1] 6
## [1] 10
## [1] 7
## [1] 7
## [1] 7
## [1] 8
## [1] 7
## [1] 9
## [1] 7
## [1] 10
## [1] 8
## [1] 8
## [1] 8
## [1] 9
## [1] 8
## [1] 10
## [1] 9
## [1] 9
## [1] 9
## [1] 10
## [1] 10
## [1] 10
## $F_stat
## [1] 5.431004
## 
## $df.H
## [1] 5.562663
## 
## $p
## [1] 0.06195702
## 
## $theta.i
## [1] 0.7983317 0.7315315
## 
## $df.sgl
## [1] 26.308358  9.531879
## 
## $se.i
## [1] 0.02136731 0.03480640
## 
## $se.dif
## [1] 0.02866403
## 
## $covOR
##         varR        varTR         cov1         cov2         cov3         varE 
## 0.0006078024 0.0011754341 0.0003476034 0.0003526108 0.0002901614 0.0009764380 
##       varR.1       varR.2       cov2.1       cov2.2       varE.1       varE.2 
## 0.0002198574 0.0033466157 0.0002785362 0.0004266855 0.0009488074 0.0010040687 
## 
## $theta.hat
##           [,1]      [,2]      [,3]      [,4]      [,5]
## [1,] 0.7640974 0.7778612 0.8058058 0.8419253 0.8019686
## [2,] 0.7062062 0.6477311 0.8019686 0.7168001 0.7849516
## 
## $cov.hat
##               [,1]         [,2]         [,3]         [,4]         [,5]
##  [1,] 0.0008886398 0.0002800444 0.0003713059 0.0002753974 0.0003151405
##  [2,] 0.0002800444 0.0011171121 0.0002878674 0.0002193651 0.0001089266
##  [3,] 0.0003713059 0.0002878674 0.0008919458 0.0004000620 0.0003250308
##  [4,] 0.0002753974 0.0002193651 0.0004000620 0.0008527298 0.0002022214
##  [5,] 0.0003151405 0.0001089266 0.0003250308 0.0002022214 0.0009936096
##  [6,] 0.0003923912 0.0003276904 0.0003453996 0.0002238991 0.0004184716
##  [7,] 0.0002841690 0.0002758367 0.0002583828 0.0001111251 0.0002804586
##  [8,] 0.0001763031 0.0002400651 0.0003713099 0.0003796392 0.0002607563
##  [9,] 0.0002847034 0.0003269489 0.0003963217 0.0003398130 0.0002651090
## [10,] 0.0002867286 0.0002724075 0.0003423597 0.0003222891 0.0003586659
##               [,6]         [,7]         [,8]         [,9]        [,10]
##  [1,] 0.0003923912 0.0002841690 0.0001763031 0.0002847034 0.0002867286
##  [2,] 0.0003276904 0.0002758367 0.0002400651 0.0003269489 0.0002724075
##  [3,] 0.0003453996 0.0002583828 0.0003713099 0.0003963217 0.0003423597
##  [4,] 0.0002238991 0.0001111251 0.0003796392 0.0003398130 0.0003222891
##  [5,] 0.0004184716 0.0002804586 0.0002607563 0.0002651090 0.0003586659
##  [6,] 0.0010698737 0.0005462020 0.0003259133 0.0005499905 0.0004306152
##  [7,] 0.0005462020 0.0011718227 0.0001955992 0.0003736004 0.0004178551
##  [8,] 0.0003259133 0.0001955992 0.0009936096 0.0004888059 0.0003200857
##  [9,] 0.0005499905 0.0003736004 0.0004888059 0.0008842620 0.0006181875
## [10,] 0.0004306152 0.0004178551 0.0003200857 0.0006181875 0.0009007754
## 
## $botCI
## [1] 0.7544356 0.6534587
## 
## $topCI
## [1] 0.8422278 0.8096044</code></pre>
<pre class="r"><code>finish &lt;- proc.time()
print(finish - start)</code></pre>
<pre><code>##    user  system elapsed 
##   2.694   0.060   2.800</code></pre>
<pre class="r"><code>aucMRMCcluster &lt;- list(result.A, result.B, result.C, result.D)
names(aucMRMCcluster) &lt;- c(&quot;ScannerA&quot;, &quot;ScannerB&quot;, &quot;ScannerC&quot;, &quot;ScannerD&quot;)

print(aucMRMCcluster)</code></pre>
<pre><code>## $ScannerA
## $ScannerA$F_stat
## [1] 24.13445
## 
## $ScannerA$df.H
## [1] 8.30231
## 
## $ScannerA$p
## [1] 0.001054525
## 
## $ScannerA$theta.i
## [1] 0.7983317 0.7009676
## 
## $ScannerA$df.sgl
## [1]   26.30836 4815.92855
## 
## $ScannerA$se.i
## [1] 0.02136731 0.02137396
## 
## $ScannerA$se.dif
## [1] 0.01981891
## 
## $ScannerA$covOR
##          varR         varTR          cov1          cov2          cov3 
## -2.831249e-04  9.032934e-05  3.805371e-04  3.611080e-04  3.010335e-04 
##          varE        varR.1        varR.2        cov2.1        cov2.2 
##  1.031883e-03  2.198574e-04 -6.054485e-04  2.785362e-04  4.436798e-04 
##        varE.1        varE.2 
##  9.488074e-04  1.114959e-03 
## 
## $ScannerA$theta.hat
##           [,1]      [,2]      [,3]      [,4]      [,5]
## [1,] 0.7640974 0.7778612 0.8058058 0.8419253 0.8019686
## [2,] 0.7129630 0.6996997 0.7035369 0.6909409 0.6976977
## 
## $ScannerA$cov.hat
##               [,1]         [,2]         [,3]         [,4]         [,5]
##  [1,] 0.0008886398 0.0002800444 0.0003713059 0.0002753974 0.0003151405
##  [2,] 0.0002800444 0.0011171121 0.0002878674 0.0002193651 0.0001089266
##  [3,] 0.0003713059 0.0002878674 0.0008919458 0.0004000620 0.0003250308
##  [4,] 0.0002753974 0.0002193651 0.0004000620 0.0008527298 0.0002022214
##  [5,] 0.0003151405 0.0001089266 0.0003250308 0.0002022214 0.0009936096
##  [6,] 0.0004599338 0.0003022434 0.0004542182 0.0002345308 0.0003473709
##  [7,] 0.0001365180 0.0003616237 0.0003520178 0.0003133894 0.0001401003
##  [8,] 0.0004240792 0.0001780302 0.0004792379 0.0001640934 0.0003432123
##  [9,] 0.0003800614 0.0002882986 0.0004072226 0.0003118814 0.0003225292
## [10,] 0.0003550457 0.0002165725 0.0004234828 0.0002376537 0.0002900087
##               [,6]         [,7]         [,8]         [,9]        [,10]
##  [1,] 0.0004599338 0.0001365180 0.0004240792 0.0003800614 0.0003550457
##  [2,] 0.0003022434 0.0003616237 0.0001780302 0.0002882986 0.0002165725
##  [3,] 0.0004542182 0.0003520178 0.0004792379 0.0004072226 0.0004234828
##  [4,] 0.0002345308 0.0003133894 0.0001640934 0.0003118814 0.0002376537
##  [5,] 0.0003473709 0.0001401003 0.0003432123 0.0003225292 0.0002900087
##  [6,] 0.0010704991 0.0003187929 0.0005181985 0.0006470766 0.0005853567
##  [7,] 0.0003187929 0.0013456309 0.0002383782 0.0003243063 0.0003418173
##  [8,] 0.0005181985 0.0002383782 0.0012615782 0.0004552788 0.0004367112
##  [9,] 0.0006470766 0.0003243063 0.0004552788 0.0009451042 0.0005708816
## [10,] 0.0005853567 0.0003418173 0.0004367112 0.0005708816 0.0009519836
## 
## $ScannerA$botCI
## [1] 0.7544356 0.6590649
## 
## $ScannerA$topCI
## [1] 0.8422278 0.7428703
## 
## 
## $ScannerB
## $ScannerB$F_stat
## [1] 10.6736
## 
## $ScannerB$df.H
## [1] 9.855737
## 
## $ScannerB$p
## [1] 0.008633451
## 
## $ScannerB$theta.i
## [1] 0.7983317 0.7349349
## 
## $ScannerB$df.sgl
## [1]  26.30836 557.88604
## 
## $ScannerB$se.i
## [1] 0.02136731 0.02313488
## 
## $ScannerB$se.dif
## [1] 0.0194049
## 
## $ScannerB$covOR
##          varR         varTR          cov1          cov2          cov3 
## -1.105293e-04  4.397234e-05  3.850625e-04  3.842192e-04  3.158881e-04 
##          varE        varR.1        varR.2        cov2.1        cov2.2 
##  1.009141e-03  2.198574e-04 -3.529714e-04  2.785362e-04  4.899023e-04 
##        varE.1        varE.2 
##  9.488074e-04  1.069475e-03 
## 
## $ScannerB$theta.hat
##           [,1]      [,2]      [,3]      [,4]      [,5]
## [1,] 0.7640974 0.7778612 0.8058058 0.8419253 0.8019686
## [2,] 0.7429096 0.7147147 0.7376543 0.7258926 0.7535035
## 
## $ScannerB$cov.hat
##               [,1]         [,2]         [,3]         [,4]         [,5]
##  [1,] 0.0008886398 0.0002800444 0.0003713059 0.0002753974 0.0003151405
##  [2,] 0.0002800444 0.0011171121 0.0002878674 0.0002193651 0.0001089266
##  [3,] 0.0003713059 0.0002878674 0.0008919458 0.0004000620 0.0003250308
##  [4,] 0.0002753974 0.0002193651 0.0004000620 0.0008527298 0.0002022214
##  [5,] 0.0003151405 0.0001089266 0.0003250308 0.0002022214 0.0009936096
##  [6,] 0.0003635490 0.0003201075 0.0004931587 0.0002132884 0.0003056105
##  [7,] 0.0001983410 0.0003925291 0.0003135956 0.0002796060 0.0001458199
##  [8,] 0.0003692806 0.0002807318 0.0005062129 0.0003404255 0.0003163333
##  [9,] 0.0003891151 0.0003601488 0.0004719073 0.0003333560 0.0003109619
## [10,] 0.0002635600 0.0002422056 0.0004129422 0.0002906221 0.0003296657
##               [,6]         [,7]         [,8]         [,9]        [,10]
##  [1,] 0.0003635490 0.0001983410 0.0003692806 0.0003891151 0.0002635600
##  [2,] 0.0003201075 0.0003925291 0.0002807318 0.0003601488 0.0002422056
##  [3,] 0.0004931587 0.0003135956 0.0005062129 0.0004719073 0.0004129422
##  [4,] 0.0002132884 0.0002796060 0.0003404255 0.0003333560 0.0002906221
##  [5,] 0.0003056105 0.0001458199 0.0003163333 0.0003109619 0.0003296657
##  [6,] 0.0011722580 0.0004990524 0.0004523130 0.0004926864 0.0005210242
##  [7,] 0.0004990524 0.0011591778 0.0004542182 0.0003623643 0.0005106737
##  [8,] 0.0004523130 0.0004542182 0.0009314197 0.0005043077 0.0005727739
##  [9,] 0.0004926864 0.0003623643 0.0005043077 0.0010365327 0.0005296096
## [10,] 0.0005210242 0.0005106737 0.0005727739 0.0005296096 0.0010479849
## 
## $ScannerB$botCI
## [1] 0.7544356 0.6894928
## 
## $ScannerB$topCI
## [1] 0.8422278 0.7803770
## 
## 
## $ScannerC
## $ScannerC$F_stat
## [1] 25.87
## 
## $ScannerC$df.H
## [1] 8.5309
## 
## $ScannerC$p
## [1] 0.0007758205
## 
## $ScannerC$theta.i
## [1] 0.7983317 0.6939940
## 
## $ScannerC$df.sgl
## [1] 26.30836 23.85604
## 
## $ScannerC$se.i
## [1] 0.02136731 0.02815730
## 
## $ScannerC$se.dif
## [1] 0.02051365
## 
## $ScannerC$covOR
##         varR        varTR         cov1         cov2         cov3         varE 
## 0.0005038157 0.0001717699 0.0003395247 0.0003733608 0.0003070308 0.0009544594 
##       varR.1       varR.2       cov2.1       cov2.2       varE.1       varE.2 
## 0.0002198574 0.0011313140 0.0002785362 0.0004681854 0.0009488074 0.0009601115 
## 
## $ScannerC$theta.hat
##           [,1]      [,2]      [,3]      [,4]      [,5]
## [1,] 0.7640974 0.7778612 0.8058058 0.8419253 0.8019686
## [2,] 0.6847681 0.6312980 0.7173841 0.6988655 0.7376543
## 
## $ScannerC$cov.hat
##               [,1]         [,2]         [,3]         [,4]         [,5]
##  [1,] 0.0008886398 0.0002800444 0.0003713059 0.0002753974 0.0003151405
##  [2,] 0.0002800444 0.0011171121 0.0002878674 0.0002193651 0.0001089266
##  [3,] 0.0003713059 0.0002878674 0.0008919458 0.0004000620 0.0003250308
##  [4,] 0.0002753974 0.0002193651 0.0004000620 0.0008527298 0.0002022214
##  [5,] 0.0003151405 0.0001089266 0.0003250308 0.0002022214 0.0009936096
##  [6,] 0.0003795851 0.0003360848 0.0003595145 0.0002699671 0.0002261605
##  [7,] 0.0002302651 0.0003922809 0.0003831014 0.0002770705 0.0001646197
##  [8,] 0.0003054905 0.0002413320 0.0003648739 0.0003085304 0.0002287781
##  [9,] 0.0003040616 0.0003924751 0.0004068663 0.0002908292 0.0002559272
## [10,] 0.0002767223 0.0004195693 0.0004599338 0.0002941463 0.0002700541
##               [,6]         [,7]         [,8]         [,9]        [,10]
##  [1,] 0.0003795851 0.0002302651 0.0003054905 0.0003040616 0.0002767223
##  [2,] 0.0003360848 0.0003922809 0.0002413320 0.0003924751 0.0004195693
##  [3,] 0.0003595145 0.0003831014 0.0003648739 0.0004068663 0.0004599338
##  [4,] 0.0002699671 0.0002770705 0.0003085304 0.0002908292 0.0002941463
##  [5,] 0.0002261605 0.0001646197 0.0002287781 0.0002559272 0.0002700541
##  [6,] 0.0009803504 0.0003582057 0.0005402824 0.0005268679 0.0005746791
##  [7,] 0.0003582057 0.0009314665 0.0004027997 0.0004805146 0.0004580286
##  [8,] 0.0005402824 0.0004027997 0.0009257911 0.0003752824 0.0004146072
##  [9,] 0.0005268679 0.0004805146 0.0003752824 0.0010315296 0.0005505869
## [10,] 0.0005746791 0.0004580286 0.0004146072 0.0005505869 0.0009314197
## 
## $ScannerC$botCI
## [1] 0.7544356 0.6358616
## 
## $ScannerC$topCI
## [1] 0.8422278 0.7521264
## 
## 
## $ScannerD
## $ScannerD$F_stat
## [1] 5.431004
## 
## $ScannerD$df.H
## [1] 5.562663
## 
## $ScannerD$p
## [1] 0.06195702
## 
## $ScannerD$theta.i
## [1] 0.7983317 0.7315315
## 
## $ScannerD$df.sgl
## [1] 26.308358  9.531879
## 
## $ScannerD$se.i
## [1] 0.02136731 0.03480640
## 
## $ScannerD$se.dif
## [1] 0.02866403
## 
## $ScannerD$covOR
##         varR        varTR         cov1         cov2         cov3         varE 
## 0.0006078024 0.0011754341 0.0003476034 0.0003526108 0.0002901614 0.0009764380 
##       varR.1       varR.2       cov2.1       cov2.2       varE.1       varE.2 
## 0.0002198574 0.0033466157 0.0002785362 0.0004266855 0.0009488074 0.0010040687 
## 
## $ScannerD$theta.hat
##           [,1]      [,2]      [,3]      [,4]      [,5]
## [1,] 0.7640974 0.7778612 0.8058058 0.8419253 0.8019686
## [2,] 0.7062062 0.6477311 0.8019686 0.7168001 0.7849516
## 
## $ScannerD$cov.hat
##               [,1]         [,2]         [,3]         [,4]         [,5]
##  [1,] 0.0008886398 0.0002800444 0.0003713059 0.0002753974 0.0003151405
##  [2,] 0.0002800444 0.0011171121 0.0002878674 0.0002193651 0.0001089266
##  [3,] 0.0003713059 0.0002878674 0.0008919458 0.0004000620 0.0003250308
##  [4,] 0.0002753974 0.0002193651 0.0004000620 0.0008527298 0.0002022214
##  [5,] 0.0003151405 0.0001089266 0.0003250308 0.0002022214 0.0009936096
##  [6,] 0.0003923912 0.0003276904 0.0003453996 0.0002238991 0.0004184716
##  [7,] 0.0002841690 0.0002758367 0.0002583828 0.0001111251 0.0002804586
##  [8,] 0.0001763031 0.0002400651 0.0003713099 0.0003796392 0.0002607563
##  [9,] 0.0002847034 0.0003269489 0.0003963217 0.0003398130 0.0002651090
## [10,] 0.0002867286 0.0002724075 0.0003423597 0.0003222891 0.0003586659
##               [,6]         [,7]         [,8]         [,9]        [,10]
##  [1,] 0.0003923912 0.0002841690 0.0001763031 0.0002847034 0.0002867286
##  [2,] 0.0003276904 0.0002758367 0.0002400651 0.0003269489 0.0002724075
##  [3,] 0.0003453996 0.0002583828 0.0003713099 0.0003963217 0.0003423597
##  [4,] 0.0002238991 0.0001111251 0.0003796392 0.0003398130 0.0003222891
##  [5,] 0.0004184716 0.0002804586 0.0002607563 0.0002651090 0.0003586659
##  [6,] 0.0010698737 0.0005462020 0.0003259133 0.0005499905 0.0004306152
##  [7,] 0.0005462020 0.0011718227 0.0001955992 0.0003736004 0.0004178551
##  [8,] 0.0003259133 0.0001955992 0.0009936096 0.0004888059 0.0003200857
##  [9,] 0.0005499905 0.0003736004 0.0004888059 0.0008842620 0.0006181875
## [10,] 0.0004306152 0.0004178551 0.0003200857 0.0006181875 0.0009007754
## 
## $ScannerD$botCI
## [1] 0.7544356 0.6534587
## 
## $ScannerD$topCI
## [1] 0.8422278 0.8096044</code></pre>
</div>
<div id="bland-altman-analysis-for-tabata2019_diagn-pathol_v14p65"
class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Bland-Altman Analysis
for Tabata2019_Diagn-Pathol_v14p65</h2>
<ul>
<li><a
href="https://raw.githubusercontent.com/DIDSR/mitoticFigureCounts/master/inst/extra/docs/Tabata2019comparingScannersMFcounting-BlandAltman.Rmd"
class="uri">https://raw.githubusercontent.com/DIDSR/mitoticFigureCounts/master/inst/extra/docs/Tabata2019comparingScannersMFcounting-BlandAltman.Rmd</a></li>
<li><a
href="https://didsr.github.io/mitoticFigureCounts/inst/extra/docs/Tabata2019comparingScannersMFcounting-BlandAltman.pdf"
class="uri">https://didsr.github.io/mitoticFigureCounts/inst/extra/docs/Tabata2019comparingScannersMFcounting-BlandAltman.pdf</a></li>
</ul>
<pre class="r"><code># We know that the study has 5 participants and 157 candidate mitotic figures
nReaders &lt;- 5
readers &lt;- c(&quot;observer.1&quot;, &quot;observer.2&quot;, &quot;observer.3&quot;, &quot;observer.4&quot;, &quot;observer.5&quot;)
nCases &lt;- 155
cases &lt;- factor(1:155)
nModalities &lt;- 5
modalities &lt;- c(&quot;scanner.A&quot;, &quot;scanner.B&quot;, &quot;scanner.C&quot;, &quot;scanner.D&quot;, &quot;microscope&quot;)

# Create a list-mode data frame
inDFtype &lt;- &quot;matrixWithTruth&quot;
outDFtype &lt;- &quot;listWithTruth&quot;
nameTruth &lt;- &quot;truth&quot;
#dfCount.ListMode &lt;- convertDF(dfCountWSI, inDFtype, outDFtype, readers, nameTruth)
 
dfCountWSI20180627 &lt;- read.csv(&quot;./01_Datasets/dfCountWSI20180627.csv&quot;,
                    sep=&quot;,&quot;,
                    header=TRUE)

dfCount.ListMode &lt;- convertDF(dfCountWSI20180627, inDFtype, outDFtype, readers, nameTruth)
dfCount.ListMode$readerID &lt;- factor(dfCount.ListMode$readerID)
dfCount.ListMode &lt;- renameCol(dfCount.ListMode, &quot;wsiName&quot;, &quot;caseID&quot;)

# r Initialize Functions
getWRBM &lt;- function(mcsData, modality.X, modality.Y) {

  # Make an &quot;MRMClist&quot; data frame for modality.X
  df.x &lt;- mcsData[mcsData$modalityID == modality.X,
                  c(&quot;readerID&quot;, &quot;caseID&quot;, &quot;modalityID&quot;, &quot;score&quot;)]
  # Make an &quot;MRMClist&quot; data frame for modality.Y
  df.y &lt;- mcsData[mcsData$modalityID == modality.Y,
                  c(&quot;readerID&quot;, &quot;caseID&quot;, &quot;modalityID&quot;, &quot;score&quot;)]
  # Merge the two data frames
  df &lt;- merge(df.x, df.y, by = c(&quot;readerID&quot;, &quot;caseID&quot;), suffixes = c(&quot;.X&quot;,&quot;.Y&quot;))

  main &lt;- paste(&quot;Bland-Altman Plot:&quot;, modality.X, &quot;vs&quot;, modality.Y, &quot;\n N = &quot;, nrow(df))
  xlab &lt;- paste(&quot;Average of&quot;, modality.X, &quot;&amp;&quot;, modality.Y)
  ylab &lt;- paste(modality.X, &quot;minus&quot;, modality.Y)

  x &lt;- df$score.X
  y &lt;- df$score.Y
  df$xyDiff &lt;- df$score.X - df$score.Y
  df$xyAvg &lt;- (df$score.X + df$score.Y)/2

  return(df)

}

getBRBM &lt;- function(mcsData, modality.X, modality.Y) {

  # This data frame will hold all the paired observations
  df &lt;- data.frame()
  
  # Split the data by readers
  readers &lt;- levels(mcsData$readerID)
  nReaders &lt;- nlevels(mcsData$readerID)
  mcsData &lt;- split(mcsData, mcsData$readerID)
  for (reader.x in 1:(nReaders - 1)) {
    for (reader.y in (reader.x + 1):nReaders) {
      
      # Grab the data frame corresponding to the i&#39;th and j&#39;th readers
      mcsData.x &lt;- mcsData[[reader.x]]
      mcsData.y &lt;- mcsData[[reader.y]]
      
      if (nrow(mcsData.x) == 0) next()
      if (nrow(mcsData.y) == 0) next()
      
      # Make an &quot;MRMClist&quot; data frame for modality.X
      df.x &lt;- mcsData.x[mcsData.x$modalityID == modality.X,
                      c(&quot;readerID&quot;, &quot;caseID&quot;, &quot;modalityID&quot;, &quot;score&quot;)]
      # Make an &quot;MRMClist&quot; data frame for modality.Y
      df.y &lt;- mcsData.y[mcsData.y$modalityID == modality.Y,
                      c(&quot;readerID&quot;, &quot;caseID&quot;, &quot;modalityID&quot;, &quot;score&quot;)]
      # Merge the two data frames
      df.temp &lt;- merge(df.x, df.y, by = c(&quot;caseID&quot;), suffixes = c(&quot;.X&quot;,&quot;.Y&quot;))
      
      df &lt;- rbind(df, df.temp)
      
    }
      
  }

  return(df)

}

 

# Identify the mitotic figure candidates not found by the microscope.
# Then investigate to yield talking points for the paper
dfClassify &lt;- read.csv(&quot;./01_Datasets/dfClassify20180627.csv&quot;,
                    sep=&quot;,&quot;,
                    header=TRUE) 
dfClassify$sumObservers &lt;- rowSums(
  dfClassify[ , c(&quot;observer.1&quot;, &quot;observer.2&quot;, &quot;observer.3&quot;, &quot;observer.4&quot;, &quot;observer.5&quot;)]
)

dfClassify.ByModality &lt;- split(dfClassify, dfClassify$modalityID)
temp.micro &lt;- dfClassify.ByModality$microscope[dfClassify.ByModality$microscope$sumObservers == 0, ]
candidatesNotFoundByMicroscope &lt;- temp.micro$targetID

temp.A &lt;- dfClassify.ByModality$scanner.A[ candidatesNotFoundByMicroscope, ]
temp.B &lt;- dfClassify.ByModality$scanner.B[ candidatesNotFoundByMicroscope, ]
temp.C &lt;- dfClassify.ByModality$scanner.C[ candidatesNotFoundByMicroscope, ]
temp.D &lt;- dfClassify.ByModality$scanner.D[ candidatesNotFoundByMicroscope, ]</code></pre>
<div id="methods-bland-altman-analysis" class="section level3"
number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Methods:
Bland-Altman analysis</h3>
<p>Intra-observer agreement between the scanner and microscope data was
also analyzed with Bland-Altman plots and related summary statistics.
For each modality we plot the differences in log counts between the
paired scanner and microscope data for each pathologist against the
average of each pair (citation: Bland1999_Stat-Methods-Med-Res_v8p135).
The log transform stabilizes the variance in the count differences as a
function of the mean (citation: Veta2016_PloS-One_v11pe0161286). The
summary statistics include the mean differences in log counts and the
standard deviation of the log-count differences (uncertainty). Twice the
standard deviation of the log-count differences above and below the mean
give the limits of agreement (LA). LA are similar to but different from
confidence intervals, which typically quantify uncertainty in a mean.
For this analysis, we counted all the cells marked as MFs for each
reader in a WSI. This aligns with what is done in clinical practice
(citation: clinical ref?). Therefore, we have four counts for each
reader and modality. The uncertainties estimated in this Bland-Altman
analysis account for the variability from the pathologists and the
correlations that are arise when the pathologists evaluate the same
cases, a so-called multi-reader multi-case analysis (citation:
Gallas2007_J-Opt-Soc-Am-A_v24pB70).</p>
<ul>
<li>Bland1999_Stat-Methods-Med-Res_v8p135: Bland, J. M. &amp; Altman, D.
G. (1999), ‘Measuring Agreement in Method Comparison Studies’, <em>Stat
Methods Med Res</em> <strong>8</strong>(2), 135-160.</li>
<li>Veta2016_PloS-One_v11pe0161286: Veta, M.; van Diest, P. J.; Jiwa,
M.; Al-Janabi, S. &amp; Pluim, J. P. W. (2016), ‘Mitosis counting in
breast cancer: Object-level interobserver agreement and comparison to an
automatic method’, <em>PloS One</em> <strong>11</strong>(8),
e0161286.</li>
<li>Gallas2007_J-Opt-Soc-Am-A_v24pB70: Gallas, B. D.; Pennello, G. A.
&amp; Myers, K. J. (2007), ‘Multireader Multicase Variance Analysis for
Binary Data’, <em>J Opt Soc Am A, Special Issue on Image Quality</em>
<strong>24</strong>(12), B70-B80.</li>
</ul>
</div>
<div id="methods-accuracy" class="section level3" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Methods:
Accuracy</h3>
<p>Accuracy was analyzed using the average of sensitivity and
specificity, giving the 2 x 2 tables of true and false MFs vs. positive
and negative determinations of all candidate MFs. Sensitivity is defined
as the number of MFs detected by an observer divided by the number of
true MFs. Specificity is defined as one minus the false-positive
fraction, where the false-positive fraction is the number of false MFs
that were positively marked, divided by the total number of false MFs.
This average is equivalent to the area under the receiver operating
characteristic curve for binary scores and is proportional to Youden’s
index (28, 29); it is also correlated with Cohen’s Kappa (30). We
reported the accuracy for each reader and modality and then the average
over readers for each modality. We also performed a multiple-reader
multiple-case (MRMC) analysis of reader-averaged accuracy using the
Obuchowski-Rockette (OR) method (cite:
Obuchowski1995_Commun-Stat-Simulat_v24p285,
Hillis2014_Stat-Med_v33p330). This method takes as input the covariances
between the AUCs from all the reader by modality combinations (five
readers times five modalities). These covariances account for
within-slide correlation between measurements obtained on ROIs within
the same slide (cite: Obuchowski1997_Biometrics_v53p567,
Obuchowski1997_clusteredROC_software).</p>
<p>Hillis, S. L. (2014), ‘A marginal-mean ANOVA approach for analyzing
multireader multicase radiological imaging data.’, Stat Med 33(2),
330–360.</p>
<p>Obuchowski, N. A. &amp; Rockette, H. E. (1995), ‘Hypothesis Testing
of Diagnostic Accuracy for Multiple Readers and Multiple Tests: An ANOVA
Approach with Dependent Observations’, Commun Stat B-Simul 24(2),
285-308.</p>
<p>Obuchowski, N. A. (1997), ‘Nonparametric Analysis of Clustered ROC
Curve Data’, Biometrics 53(2), 567-578.</p>
<p>Obuchowski, N. (1997), ‘funcs_clusteredROC.R: Nonparametric Analysis
of Clustered ROC Curve Data’, Department of Quantitative Health
Sciences, Lerner Research Institute, Cleveland Clinic, Silver Spring,
MD. URL: <a
href="https://www.lerner.ccf.org/qhs/software/roc_analysis.php"
class="uri">https://www.lerner.ccf.org/qhs/software/roc_analysis.php</a>,
accessed 5/22/2019.</p>
<div style="page-break-after: always;"></div>
</div>
<div id="table" class="section level3" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Table</h3>
<pre class="r"><code>table4 &lt;- data.frame(
  &#39;ScannerA&#39; = aucMRMCcluster$`ScannerA`$theta.hat[2, ],
  &#39;ScannerB&#39; = aucMRMCcluster$`ScannerB`$theta.hat[2, ],
  &#39;ScannerC&#39; = aucMRMCcluster$`ScannerC`$theta.hat[2, ],
  &#39;ScannerD&#39; = aucMRMCcluster$`ScannerD`$theta.hat[2, ],
  
  &#39;Microscope&#39; = aucMRMCcluster$`ScannerA`$theta.hat[1, ]
)
table4[6, ] &lt;- c(
  aucMRMCcluster$`ScannerA`$theta.i[2],
  aucMRMCcluster$`ScannerB`$theta.i[2],
  aucMRMCcluster$`ScannerC`$theta.i[2],
  aucMRMCcluster$`ScannerD`$theta.i[2],
  
  aucMRMCcluster$`ScannerA`$theta.i[1]
)
table4[7, ] &lt;- c(
  aucMRMCcluster$`ScannerA`$se.i[2],
  aucMRMCcluster$`ScannerB`$se.i[2],
  aucMRMCcluster$`ScannerC`$se.i[2],
  aucMRMCcluster$`ScannerD`$se.i[2],
  
  aucMRMCcluster$`ScannerA`$se.i[1]
)
table4[8, ] &lt;- c(
  aucMRMCcluster$`ScannerA`$botCI[2],
  aucMRMCcluster$`ScannerB`$botCI[2],
  aucMRMCcluster$`ScannerC`$botCI[2],
  aucMRMCcluster$`ScannerD`$botCI[2],
  
  aucMRMCcluster$`ScannerA`$botCI[1]
)
table4[9, ] &lt;- c(
  aucMRMCcluster$`ScannerA`$topCI[2],
  aucMRMCcluster$`ScannerB`$topCI[2],
  aucMRMCcluster$`ScannerC`$topCI[2],
  aucMRMCcluster$`ScannerD`$topCI[2],
  
  aucMRMCcluster$`ScannerA`$topCI[1]
)
table4[10, ] &lt;- c(
  aucMRMCcluster$`ScannerA`$p,
  aucMRMCcluster$`ScannerB`$p,
  aucMRMCcluster$`ScannerC`$p,
  aucMRMCcluster$`ScannerD`$p,
  
  NA
)
rownames(table4) &lt;- c(
  &quot;Observer 1&quot;,
  &quot;Observer 2&quot;,
  &quot;Observer 3&quot;,
  &quot;Observer 4&quot;,
  &quot;Observer 5&quot;,
  &quot;Average&quot;,
  &quot;SE&quot;,
  &quot;botCI&quot;,
  &quot;topCI&quot;,
  &quot;p-value&quot;
)

print(round(table4, digits = 3))</code></pre>
<pre><code>##            ScannerA ScannerB ScannerC ScannerD Microscope
## Observer 1    0.713    0.743    0.685    0.706      0.764
## Observer 2    0.700    0.715    0.631    0.648      0.778
## Observer 3    0.704    0.738    0.717    0.802      0.806
## Observer 4    0.691    0.726    0.699    0.717      0.842
## Observer 5    0.698    0.754    0.738    0.785      0.802
## Average       0.701    0.735    0.694    0.732      0.798
## SE            0.021    0.023    0.028    0.035      0.021
## botCI         0.659    0.689    0.636    0.653      0.754
## topCI         0.743    0.780    0.752    0.810      0.842
## p-value       0.001    0.009    0.001    0.062         NA</code></pre>
<pre class="r"><code>#write.csv(table4, file.path(&quot;inst&quot;, &quot;docs&quot;, &quot;table4.csv&quot;))
# write.csv(table4, file.path(&quot;table4.csv&quot;))</code></pre>
</div>
</div>
</div>
<div id="package-mrmcaov" class="section level1" number="8">
<h1><span class="header-section-number">8</span> Package MRMCaov</h1>
<p>The <code>MRMCaov</code> package is designed for the analysis of
multi-reader, multi-case (MRMC) studies in medical imaging, evaluating
the diagnostic performance of imaging systems across different readers
and cases. Here’s a detailed description:</p>
<ul>
<li><p><strong>Purpose</strong>: <code>MRMCaov</code> is tailored for
statistical analysis in MRMC studies to assess the consistency and
reliability of imaging modalities across multiple readers and cases.
It’s particularly valuable in regulatory settings and research where
validating the diagnostic accuracy of imaging technologies is
essential.</p></li>
<li><p><strong>Functionality</strong>:</p>
<ul>
<li>Provides tools to compute reader and case effects using analysis of
variance (ANOVA) models, which help in understanding variance components
attributed to different sources in the study.</li>
<li>Supports the calculation of generalized estimating equations (GEE)
to account for the correlated data typical in MRMC designs.</li>
<li>Facilitates the estimation of the area under the ROC curve (AUC) and
its confidence intervals to measure diagnostic accuracy
comprehensively.</li>
</ul></li>
<li><p><strong>Differences</strong>:</p>
<ul>
<li>Unlike packages that focus on single-reader analyses,
<code>MRMCaov</code> explicitly handles the complexity of multiple
readers and cases, allowing for a more nuanced analysis of inter-reader
variability and case difficulty.</li>
<li>It integrates advanced statistical methodologies specific to MRMC
study designs, making it ideal for extensive and detailed analysis
required in high-stakes regulatory submissions or academic research
involving multiple imaging modalities and observer assessments.</li>
</ul></li>
</ul>
<div id="single-test-multi-case-analysis" class="section level2"
number="8.1">
<h2><span class="header-section-number">8.1</span> Single-Test
Multi-Case Analysis</h2>
<p>A single-test and single-reader multi-case (STMC) analysis involves a
single reader of multiple cases to estimate a reader performance metric
for one diagnostic test.</p>
<pre class="r"><code>data(VanDyke, package = &quot;MRMCaov&quot;)

## Subset VanDyke dataset by treatment 1 and reader 1
VanDyke11 &lt;- subset(VanDyke, treatment == &quot;1&quot; &amp; reader == &quot;1&quot;) %&gt;%
  select(-case2,-case3)

## Estimate ROC AUC for treatment 1 and reader 1
est &lt;- stmc(empirical_auc(truth, rating), case, data = VanDyke11) 

### Show STMC ROC Curve Parameters
print(parameters(est))</code></pre>
<pre><code>## # A tibble: 6 × 2
##      FPR   TPR
##    &lt;dbl&gt; &lt;dbl&gt;
## 1 1      1    
## 2 0.319  0.911
## 3 0.188  0.889
## 4 0.0435 0.844
## 5 0.0145 0.622
## 6 0      0</code></pre>
<pre class="r"><code>plot(est)</code></pre>
<p><img src="04-Design-Diagnostic-Study-MRMC_files/figure-html/unnamed-chunk-16-1.png" width="960" /></p>
<pre class="r"><code>summary(est)</code></pre>
<pre><code>## empirical_auc        StdErr      CI.Lower      CI.Upper 
##    0.91964573    0.03012552    0.86060081    0.97869066</code></pre>
</div>
<div id="multi-reader-multi-case-analysis" class="section level2"
number="8.2">
<h2><span class="header-section-number">8.2</span> Multi-Reader
Multi-Case Analysis</h2>
<pre class="r"><code>## Compare ROC AUC treatment means for the VanDyke example
est &lt;- mrmc(
  binormal_auc(truth, rating), treatment, reader, case, data = VanDyke
)

## Show MRMC Performance Metrics
print(est)</code></pre>
<pre><code>## Call:
## mrmc(response = binormal_auc(truth, rating), test = treatment, 
##     reader = reader, case = case, data = VanDyke)
## 
## Positive truth status: 1 
## 
## Response metric data:
## 
## # A tibble: 10 × 2
##        N data$binormal_auc $treatment $reader
##    &lt;dbl&gt;             &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;  
##  1   114             0.933 1          1      
##  2   114             0.890 1          2      
##  3   114             0.929 1          3      
##  4   114             0.970 1          4      
##  5   114             0.833 1          5      
##  6   114             0.951 2          1      
##  7   114             0.935 2          2      
##  8   114             0.928 2          3      
##  9   114             1     2          4      
## 10   114             0.945 2          5      
## 
## ANOVA Table:
## 
##                  Df    Sum Sq   Mean Sq
## treatment         1 0.0041142 0.0041142
## reader            4 0.0104325 0.0026081
## treatment:reader  4 0.0037916 0.0009479
## 
## 
## Obuchowski-Rockette error variance and covariance estimates:
## 
##           Estimate Correlation
## Error 0.0010790500          NA
## Cov1  0.0003125013   0.2896078
## Cov2  0.0003115986   0.2887713
## Cov3  0.0001937688   0.1795735</code></pre>
<pre class="r"><code>## Show MRMC Test Results
summary(est)</code></pre>
<pre><code>## Multi-Reader Multi-Case Analysis of Variance
## Data: VanDyke
## Factor types: Random Readers and Random Cases
## Covariance method: jackknife
## 
## Experimental design: factorial 
## 
## Obuchowski-Rockette variance component and covariance estimates:
## 
##                      Estimate Correlation
## reader           0.0007113799          NA
## treatment:reader 0.0002991713          NA
## Error            0.0010790500          NA
## Cov1             0.0003125013   0.2896078
## Cov2             0.0003115986   0.2887713
## Cov3             0.0001937688   0.1795735
## 
## 
## ANOVA global test of equal treatment binormal_auc:
##         MS(T)      MS(T:R)         Cov2         Cov3 Denominator        F df1
## 1 0.004114188 0.0009478901 0.0003115986 0.0001937688 0.001537039 2.676697   1
##        df2   p-value
## 1 10.51753 0.1313668
## 
## 
## 95% CIs and tests for treatment binormal_auc pairwise differences:
##   Comparison    Estimate     StdErr       df    CI.Lower    CI.Upper         t
## 1      1 - 2 -0.04056692 0.02479548 10.51753 -0.09544840  0.01431455 -1.636061
##     p-value
## 1 0.1313668
## 
## 
## 95% treatment binormal_auc CIs (each analysis based only on data for the
## specified treatment):
##    Estimate        MS(R)         Cov2     StdErr       df  CI.Lower  CI.Upper
## 1 0.9109867 0.0027417526 0.0004612201 0.03177374 13.55866 0.8426301 0.9793433
## 2 0.9515536 0.0008142524 0.0001619772 0.01802298 15.91432 0.9133299 0.9897774</code></pre>
<pre class="r"><code>plot(est)</code></pre>
<p><img src="04-Design-Diagnostic-Study-MRMC_files/figure-html/unnamed-chunk-17-1.png" width="960" /></p>
<pre class="r"><code>## Calculation of ROC curve true positive rate (TPR) and false positive rate (FPR) pairs for values of a numeric rating of a true binary response.
print(parameters(est))</code></pre>
<pre><code>## # A tibble: 10 × 3
##    Group$reader $treatment      a     b
##    &lt;fct&gt;        &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;
##  1 1            1            1.70 0.537
##  2 2            1            1.40 0.561
##  3 3            1            1.74 0.635
##  4 4            1            1.93 0.202
##  5 5            1            1.06 0.464
##  6 1            2            1.85 0.503
##  7 2            2            1.66 0.447
##  8 3            2            1.62 0.488
##  9 4            2          Inf    1    
## 10 5            2            1.73 0.422</code></pre>
</div>
</div>
<div id="package-rjafroc" class="section level1" number="9">
<h1><span class="header-section-number">9</span> Package RJafroc</h1>
<ul>
<li><strong>Purpose</strong>: This package is specifically designed for
analyzing reader performance in detecting and localizing lesions using
AFROC and FROC (Free-response ROC) methodologies. It is particularly
useful in studies where the detection and precise localization of
abnormalities are crucial.</li>
<li><strong>Functionality</strong>:
<ul>
<li>Supports the analysis of datasets where readers provide spatially
localized responses (lesions) along with confidence scores.</li>
<li>Calculates various performance metrics such as lesion localization
fraction (LLF) and false positive fraction (FPF), integral to AFROC
analysis.</li>
<li>Offers robust statistical tools for bootstrapping, jackknife
resampling, and fitting ROC curves specific to lesion detection and
localization tasks.</li>
</ul></li>
<li><strong>Differences</strong>:
<ul>
<li><code>RJafroc</code> is specialized for lesion detection and
localization analyses, directly aligning with the goals of AFROC
analyses.</li>
<li>Provides a more sophisticated handling of data specific to medical
imaging studies where localization is as important as detection,
including the management of multiple lesions per image and varied reader
confidence levels.</li>
</ul></li>
</ul>
<div id="jafroc-data-format" class="section level2" number="9.1">
<h2><span class="header-section-number">9.1</span> JAFROC data
format</h2>
<ul>
<li>In the ROC paradigm the observer assigns a rating to each image. A
rating is an ordered numeric label, and, in our convention, higher
values represent greater certainty or confidence level for presence of
disease. With human observers, a 5 (or 6) point rating scale is
typically used, with 1 representing highest confidence for absence of
disease and 5 (or 6) representing highest confidence for presence of
disease. Intermediate values represent intermediate confidence levels
for presence or absence of disease.</li>
<li>Note that location information, if applicable, associated with the
disease, is not collected.</li>
</ul>
<div id="truth-worksheet" class="section level3" number="9.1.1">
<h3><span class="header-section-number">9.1.1</span> Truth
worksheet</h3>
<ul>
<li>The Truth worksheet contains 6 columns: CaseID, LesionID, Weight,
ReaderID, ModalityID and Paradigm.</li>
<li>The first five columns contain as many rows as there are cases
(images) in the dataset.</li>
<li>CaseID: unique integers, one per case, representing the cases in the
dataset.</li>
<li>LesionID: integers 0 or 1, with each 0 representing a non-diseased
case and each 1 representing a diseased case.</li>
<li>In the current dataset, the non-diseased cases are labeled 1, 2 and
3, while the diseased cases are labeled 70, 71, 72, 73 and 74. The
values do not have to be consecutive integers; they need not be ordered;
the only requirement is that they be unique integers.</li>
<li>Weight: A floating point value, typically filled in with 0 or 1;
this field is not used for ROC data.</li>
<li>ReaderID: a comma-separated listing of reader labels, each
represented by a unique integer, that have interpreted the case. In the
example shown below each cell has the value 0, 1, 2, 3, 4 meaning that
each of these readers has interpreted all cases (hence the “factorial”
design).</li>
<li>ModalityID: a comma-separated listing of modalities, each
represented by a unique integer, that are applied to each case. In the
example each cell has the value 0, 1.</li>
</ul>
<p>ZB: <a
href="https://dpc10ster.github.io/RJafrocBook/quick-start-data-format.html"
class="uri">https://dpc10ster.github.io/RJafrocBook/quick-start-data-format.html</a></p>
<pre class="r"><code># rocCr1R &lt;- &quot;./01_Datasets/Template Data/rocCr.xlsx&quot;
rocCr1R &lt;- &quot;./01_Datasets/BDCA_wAFROC.xlsx&quot;
x &lt;- DfReadDataFile(rocCr1R, newExcelFileFormat = TRUE)
str(x)</code></pre>
<pre><code>## List of 3
##  $ ratings     :List of 3
##   ..$ NL   : num [1:2, 1:3, 1:295, 1] 0 0 0 1 0 1 0 0 0 0 ...
##   ..$ LL   : num [1:2, 1:3, 1:162, 1] 0 2 2 3 2 2 3 3 2 4 ...
##   ..$ LL_IL: logi NA
##  $ lesions     :List of 3
##   ..$ perCase: int [1:162] 1 1 1 1 1 1 1 1 1 1 ...
##   ..$ IDs    : num [1:162, 1] 1 1 1 1 1 1 1 1 1 1 ...
##   ..$ weights: num [1:162, 1] 1 1 1 1 1 1 1 1 1 1 ...
##  $ descriptions:List of 7
##   ..$ fileName     : chr &quot;BDCA_wAFROC&quot;
##   ..$ type         : chr &quot;FROC&quot;
##   ..$ name         : logi NA
##   ..$ truthTableStr: num [1:2, 1:3, 1:295, 1:2] 1 1 1 1 1 1 1 1 1 1 ...
##   ..$ design       : chr &quot;FCTRL&quot;
##   ..$ modalityID   : Named chr [1:2] &quot;0&quot; &quot;1&quot;
##   .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;0&quot; &quot;1&quot;
##   ..$ readerID     : Named chr [1:3] &quot;1&quot; &quot;2&quot; &quot;3&quot;
##   .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;1&quot; &quot;2&quot; &quot;3&quot;</code></pre>
<pre class="r"><code>lesDistr &lt;- UtilLesionDistrVector(x)
retFit &lt;- FitBinormalRoc(x, lesDistr)
retFit</code></pre>
<pre><code>## $a
## [1] 2.831333
## 
## $b
## [1] 1.513278
## 
## $zetas
##  zetaFwd1  zetaFwd2  zetaFwd3  zetaFwd4  zetaFwd5 
## 0.1177657 1.0150247 1.2716128 1.6856419 2.7901385 
## 
## $AUC
## [1] 0.9407336
## 
## $StdAUC
##            [,1]
## [1,] 0.01070916
## 
## $NLLIni
## [1] 598.4301
## 
## $NLLFin
## [1] 557.1962
## 
## $ChisqrFitStats
## $ChisqrFitStats$chisq
## [1] 185.508
## 
## $ChisqrFitStats$pVal
## [1] 5.217439e-41
## 
## $ChisqrFitStats$df
## [1] 2
## 
## 
## $covMat
##                  a            b        zeta1       zeta2        zeta3
## a      0.114377491  0.069671527 0.0069966201 0.012361082  0.008263010
## b      0.069671527  0.055541086 0.0021517787 0.001893051 -0.002539092
## zeta1  0.006996620  0.002151779 0.0053494961 0.003126691  0.002762980
## zeta2  0.012361082  0.001893051 0.0031266911 0.007423325  0.006749605
## zeta3  0.008263010 -0.002539092 0.0027629805 0.006749605  0.008420675
## zeta4 -0.003530933 -0.013431630 0.0022179493 0.006016674  0.008314316
## zeta5 -0.051211582 -0.053521143 0.0006855898 0.004627541  0.009943082
##              zeta4         zeta5
## a     -0.003530933 -0.0512115819
## b     -0.013431630 -0.0535211433
## zeta1  0.002217949  0.0006855898
## zeta2  0.006016674  0.0046275415
## zeta3  0.008314316  0.0099430816
## zeta4  0.013185668  0.0220169314
## zeta5  0.022016931  0.0692594103
## 
## $fittedPlot</code></pre>
<p><img src="04-Design-Diagnostic-Study-MRMC_files/figure-html/unnamed-chunk-18-1.png" width="960" /></p>
<pre class="r"><code># PlotEmpiricalOperatingCharacteristics(dataset = x, opChType=&quot;AFROC&quot;)$Plot</code></pre>
</div>
</div>
<div id="analysis" class="section level2" number="9.2">
<h2><span class="header-section-number">9.2</span> Analysis</h2>
<pre class="r"><code>str(dataset01)</code></pre>
<pre><code>## List of 3
##  $ ratings     :List of 3
##   ..$ NL   : num [1:2, 1:5, 1:185, 1:3] 3 -Inf 3 -Inf 4 ...
##   ..$ LL   : num [1:2, 1:5, 1:89, 1:2] 4 4 3 -Inf 3.5 ...
##   ..$ LL_IL: logi NA
##  $ lesions     :List of 3
##   ..$ perCase: int [1:89] 1 1 1 1 1 1 1 1 1 1 ...
##   ..$ IDs    : num [1:89, 1:2] 1 1 1 1 1 1 1 1 1 1 ...
##   ..$ weights: num [1:89, 1:2] 1 1 1 1 1 1 1 1 1 1 ...
##  $ descriptions:List of 7
##   ..$ fileName     : chr &quot;dataset01&quot;
##   ..$ type         : chr &quot;FROC&quot;
##   ..$ name         : chr &quot;TONY&quot;
##   ..$ truthTableStr: num [1:2, 1:5, 1:185, 1:4] 1 1 1 1 1 1 1 1 1 1 ...
##   ..$ design       : chr &quot;FCTRL&quot;
##   ..$ modalityID   : Named chr [1:2] &quot;BT&quot; &quot;DM&quot;
##   .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;BT&quot; &quot;DM&quot;
##   ..$ readerID     : Named chr [1:5] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...</code></pre>
<pre class="r"><code>RJafroc::PlotEmpiricalOperatingCharacteristics(dataset = dataset01, opChType = &quot;AFROC&quot;)$Plot</code></pre>
<p><img src="04-Design-Diagnostic-Study-MRMC_files/figure-html/unnamed-chunk-19-1.png" width="960" /></p>
<pre class="r"><code>### RSM ROC/AFROC/wAFROC AUC calculator
mu &lt;- 1;lambda &lt;- 1;nu &lt;- 0.9
lesDistr &lt;- c(0.9, 0.1) 
## i.e., 90% of dis. cases have one lesion, and 10% have two lesions
relWeights &lt;- c(0.05, 0.95)
## i.e., lesion 1 has weight 5 percent while lesion two has weight 95 percent

UtilAnalyticalAucsRSM(mu, lambda, nu, zeta1 = -Inf, lesDistr)</code></pre>
<pre><code>## $aucROC
## [1] 0.8147209
## 
## $aucAFROC
## [1] 0.7448364
## 
## $aucwAFROC
## [1] 0.7448364</code></pre>
<pre class="r"><code>UtilAnalyticalAucsRSM(mu, lambda, nu, zeta1 = -Inf, lesDistr, relWeights)</code></pre>
<pre><code>## $aucROC
## [1] 0.8147209
## 
## $aucAFROC
## [1] 0.7448364
## 
## $aucwAFROC
## [1] 0.785221</code></pre>
</div>
</div>
<div id="reference" class="section level1" number="10">
<h1><span class="header-section-number">10</span> Reference</h1>
<ul>
<li>Anthony D. Yao , Derrick L. Cheng, Ian Pan, Felipe Kitamura; Deep
Learning in Neuroradiology: A Systematic Review of Current Algorithms
and Approaches for the New Wave of Imaging Technology; Radiology:
Artificial Intelligence. Published Online:Mar 4 2020
doi.org/10.1148/ryai.2020190026</li>
<li>Genders, T. S. S., Spronk, S., Stijnen, T., Steyerberg, E. W.,
Lesaffre, E., &amp; Hunink, M. G. M. (2012). Methods for Calculating
Sensitivity and Specificity of Clustered Data: A Tutorial. Radiology,
265(3), 910–916. <a href="doi:10.1148/radiol.12120509"
class="uri">doi:10.1148/radiol.12120509</a></li>
<li>Observer Performance Methods for Diagnostic Imaging: Foundations,
Modeling, and Applications with r-Based Examples. Boca Raton, FL: CRC
Press.</li>
<li>Chakraborty, Dev, and Xuetong Zhai. 2022. RJafroc: Artificial
Intelligence Systems and Observer Performance. <a
href="https://dpc10ster.github.io/RJafroc/"
class="uri">https://dpc10ster.github.io/RJafroc/</a>.</li>
<li>Obuchowski NA, Beiden SV, Berbaum KS, Hillis SL, Ishwaran H, Song
HH, Wagner RF. Multireader, multicase receiver operating characteristic
analysis: an empirical comparison of five methods. Acad Radiol. 2004
Sep;11(9):980-95. doi: 10.1016/j.acra.2004.04.014. PMID: 15350579.</li>
<li>Gallas, Brandon D.; Bandos, Andriy; Samuelson, Frank W.; Wagner,
Robert F. (2009). A Framework for Random-Effects ROC Analysis: Biases
with the Bootstrap and Other Variance Estimators. Communications in
Statistics - Theory and Methods, 38(15), 2586–2603. <a
href="doi:10.1080/03610920802610084"
class="uri">doi:10.1080/03610920802610084</a></li>
<li><a href="https://dpc10ster.github.io/RJafrocRocBook/">Modeling ROC
data</a></li>
<li><a href="https://dpc10ster.github.io/RJafrocBook/">The RJafroc
Book</a></li>
<li><a
href="https://stats.stackexchange.com/questions/195006/is-the-dice-coefficient-the-same-as-accuracy"
class="uri">https://stats.stackexchange.com/questions/195006/is-the-dice-coefficient-the-same-as-accuracy</a></li>
<li><a
href="https://brian-j-smith.github.io/MRMCaov/using.html#5_Single-Reader_Multi-Case_Analysis">Using
MRMCaov for R</a></li>
</ul>
<p>Gallas BD, Bandos A, Samuelson FW, Wagner RF. A Framework for
Random-Effects ROC Analysis: Biases with the Bootstrap and Other
Variance Estimators. Communications in Statistics - Theory and Methods.
2009 Jul 23;38(15):2586–603. <a
href="https://www.tandfonline.com/doi/abs/10.1080/03610920802610084"
class="uri">https://www.tandfonline.com/doi/abs/10.1080/03610920802610084</a></p>
<p>Dorfman DD, Berbaum KS, Metz CE. Receiver operating characteristic
rating analysis. Generalization to the population of readers and
patients with the jackknife method. Invest Radiol. 1992
Sep;27(9):723–31.</p>
<p>Davidson, A. C., Hinkley, D. V. (1997). Bootstrap methods and their
applications. Cambridge University press.</p>
<p>Beiden SV, Wagner RF, Campbell G. Components-of-variance models and
multiple-bootstrap experiments: An alternative method for
random-effects, receiver operating characteristic analysis. Academic
Radiology. 2000 May;7(5):341–9. <a
href="https://www.academicradiology.org/article/S1076-6332(00)80008-2/pdf"
class="uri">https://www.academicradiology.org/article/S1076-6332(00)80008-2/pdf</a></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
