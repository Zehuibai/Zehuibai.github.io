---
title: |
  ![](logo.png){width=3in}  
  Meta Analysis 
output:
  html_document:
    df_print: paged
    number_sections: Yes
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
fontsize: 10pt
editor_options:
  chunk_output_type: console
colorlinks: yes
---
 
```{r setup, include=FALSE, echo = FALSE,message = FALSE, error = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 10, fig.height = 6)

# <!-- ---------------------------------------------------------------------- -->
# <!--                    1. load the required packages                       -->
# <!-- ---------------------------------------------------------------------- --> 

# if(!require(psych)){install.packages("psych")}
packages<-c("tidyverse","gdata",
            "meta","rmeta","metafor","gmeta",
            "kableExtra")

ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}
ipak(packages)

# get the wd path
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
getwd()


# source("./03_Functions/gmeta.r")
# source("./03_Functions/np.gmeta.r")
 


data1 <- read.csv("./01_Datasets/dataset01.csv", as.is=TRUE)
data2 <- read.csv("./01_Datasets/dataset02.csv", as.is=TRUE)
data3 <- read.csv("./01_Datasets/dataset03.csv", as.is=TRUE)
data4 <- read.csv("./01_Datasets/dataset04.csv", as.is=TRUE)
data5 <- read.csv("./01_Datasets/dataset05.csv", as.is=TRUE)
data6 <- read.csv("./01_Datasets/dataset06.csv", as.is=TRUE)
data7 <- read.csv("./01_Datasets/dataset07.csv", as.is=TRUE)
data8 <- read.csv("./01_Datasets/dataset08.csv", as.is=TRUE)
```


# Introduction
 
Meta-analysis is a statistical method used to systematically combine and synthesize results from multiple independent studies that address the same or closely related research question. Its primary purpose is to increase the overall power and precision of the findings, resolve uncertainty when individual studies disagree, and provide a more comprehensive understanding of the evidence base.

Unlike traditional narrative reviews, which rely on qualitative summaries and are often subject to author bias, meta-analysis follows a structured and quantitative approach. It typically involves extracting effect sizes or summary statistics from each study, assessing consistency among studies, and calculating an overall pooled estimate using appropriate statistical models.

Meta-analysis is especially valuable in fields such as medicine, psychology, education, and public health, where multiple studies may exist on a single intervention, treatment, or phenomenon. It allows researchers to answer questions with greater statistical confidence than any single study can provide, and it plays a critical role in evidence-based decision-making, such as developing clinical guidelines or informing public policy.

A well-conducted meta-analysis begins with a clearly defined research question and a written protocol. It includes systematic literature searches, predefined inclusion and exclusion criteria, careful data extraction, assessment of study quality, evaluation of heterogeneity among results, and application of suitable analytical techniques. When properly designed, a meta-analysis offers a rigorous and transparent synthesis of scientific evidence.

## Meta-Analysis Protocol

Writing a protocol is the **first and most critical step** in conducting a meta-analysis. It is comparable to designing a clinical trial: the protocol defines the research question, outlines inclusion and exclusion criteria, and specifies how data will be identified, abstracted, and synthesized.

Key benefits:

* **Minimizes bias** in study selection and analysis.
* **Enhances scientific rigor** and reproducibility.
* **Clarifies scope and eligibility** of studies for inclusion.

---

**[1] Defining the Research Objective**

A clearly defined objective is crucial for guiding all subsequent steps. For example:

> *"To assess the overall evidence of the effectiveness of calcium-channel blockers in treating mild-to-moderate hypertension."*

However, to operationalize this question, further specifications are required, such as:

* Definition of “mild-to-moderate” hypertension (blood pressure thresholds have evolved over time).
* Outcome measures (e.g., change in diastolic BP).
* Control group type (placebo or active comparator).
* Study design (e.g., parallel, crossover), randomization methods, and blinding.
* Patient characteristics (e.g., age, gender, comorbidities).
* Study duration.

These specifications form the foundation for establishing **eligibility criteria** for selecting studies.

---

**[2] Criteria for Identifying Eligible Studies**

 
To ensure consistency, validity, and scientific rigor in a meta-analysis, the following six criteria should be clearly defined in the research protocol:

**1. Clarifying the Disease or Condition Under Study**

* Definitions of disease or outcome must be consistent with contemporary clinical standards.
* Historical definitions may differ (e.g., older studies may define hypertension differently).

The specific medical condition or area of application must be precisely defined. For instance, if the focus is on "mild-to-moderate hypertension," the protocol must clarify what blood pressure ranges constitute this category. Clinical definitions may have changed over time—what was once considered mild may now be classified as moderate or even normal. Failing to standardize this definition could result in the inclusion of heterogeneous populations that compromise the validity of pooled estimates.

**2. Defining the Effectiveness Measure or Outcome**

* Define primary endpoints (e.g., change in diastolic blood pressure).
* Include methods of measurement (position, device).
* Ensure that chosen outcomes are consistently reported across studies.

Meta-analyses require a common outcome measure to aggregate data meaningfully. In hypertension trials, this might be the change in diastolic blood pressure (DBP) from baseline. However, outcomes may be reported differently across studies—mean change, percentage reaching normal levels, or alternative metrics like mean arterial pressure. The protocol must specify which outcomes are acceptable, how they are measured (e.g., sitting vs. standing BP), and whether they can be harmonized across studies.

**3. Specifying the Type of Control Group**

* Control types (placebo or active comparator) influence interpretation of efficacy.
* Meta-analysis should include studies with **comparable control groups**.

The type of comparator used in each study (e.g., placebo, active control, usual care) has major implications for the interpretation of efficacy. Placebo-controlled trials provide direct evidence, while active-controlled trials offer relative comparisons. The meta-analysis should either restrict inclusion to studies with the same type of control group or account for differences analytically (e.g., through subgroup or sensitivity analyses). The protocol should define acceptable control types and explain how variations will be handled.

**4. Outlining Acceptable Study Designs and Quality Characteristics**

* Design (parallel vs. crossover), randomization, stratification, blinding, and bias control.
* Design consistency helps maintain internal validity across synthesized studies.

Different experimental designs (parallel vs. crossover, randomized vs. non-randomized) can introduce variability in treatment effects. Additional factors such as blinding, allocation concealment, and stratification should be specified. The protocol must outline which study designs are acceptable and whether criteria like randomization method or blinding status are mandatory for inclusion. High-quality design features reduce bias and increase the reliability of synthesized results.

**5. Characterizing the Patient Population**

* Include demographic variables, inpatient vs. outpatient status, concurrent diseases, and concomitant medications.
* Abstracting subgroup-level summary statistics helps evaluate heterogeneity.

Eligible studies must include similar populations in terms of demographics and clinical characteristics. Age range, sex distribution, race or ethnicity, inpatient vs. outpatient setting, presence of comorbidities, and use of concomitant medications should be considered. If these factors vary widely, stratified analyses or subgroup extraction may be necessary. The protocol should state clearly which patient characteristics are required and how heterogeneity will be managed.

**6. Determining the Acceptable Length of Follow-Up or Treatment Duration**

* Treatment duration significantly affects clinical outcomes.
* Must be accounted for in both selection and interpretation.

The duration of each study must be taken into account, as it affects the stability and magnitude of treatment effects. A 4-week intervention may yield different results compared to a 6-month study. The protocol should define acceptable minimum and/or maximum follow-up durations, or describe how differences in length will be addressed in the analysis (e.g., through meta-regression).

---

**[3] Searching and Collecting Studies**

Numerous databases should be consulted based on the research field:

For Major Medical Databases:

* **PubMed/MEDLINE**
* **Embase**
* **Web of Science**
* **ClinicalTrials.gov**
* **Cochrane Central (CENTRAL)**

Inclusion/exclusion criteria are applied at the **study level** (unlike patient-level in clinical trials).

---

**[4] Data Abstraction and Extraction**

A **Data Extraction Form (DAF)** is essential for:

* Standardizing data capture from multiple studies.
* Ensuring clarity and reproducibility.
* Supporting quality assurance and future meta-analysis updates.
 

Data abstraction should be guided by:

* Disease definition
* Outcomes
* Control group type
* Study design
* Patient population
* Duration of follow-up


---

**[5] Meta-Analysis Methods**

The statistical methods must be pre-specified in the protocol:

* Determined by study design and type of outcome data.
* Must account for **heterogeneity** across studies.
* May need adjustment based on study characteristics identified during data abstraction.

---

**[6] Reporting the Meta-Analysis Results**

A comprehensive meta-analysis report should mirror the structure of the protocol and include:

* **Executive Summary / Abstract**
* **Objective**
* **Study Search and Inclusion Criteria**
* **Data Extraction and Methods**
* **Results (with figures, forest plots, heterogeneity analysis)**
* **Discussion and Conclusion**
* **Appendices**

This final report serves as:

* Documentation of the research process.
* A source for publications.
* A benchmark for transparency and methodological rigor.




## Fixed-Effects and Random-Effects

In meta-analysis, we aim to combine results from multiple independent studies to arrive at an overall conclusion about a treatment or effect. Two fundamental statistical models used in meta-analysis are the **fixed-effects model** and the **random-effects model**. 

The typical aim of a clinical trial is to compare the efficacy of a treatment (such as a new drug D) against a control (e.g., placebo P). This comparison is made using a treatment effect size, denoted by δ, which is a numerical summary of the difference in outcomes between the treatment and control groups.

* For **binary data** (e.g., death vs. survival), effect sizes may include:

  * Difference in proportions
  * Log odds ratio
  * Relative risk

* For **continuous data** (e.g., blood pressure, exercise capacity), effect sizes may include:

  * Difference in means
  * Standardized mean difference

The hypotheses are:

* Null hypothesis (H₀): δ = 0 (no difference between treatment and control)
* Alternative hypothesis (Hₐ): δ ≠ 0 or δ > 0 (depending on the test direction)

Each study provides an estimate of the treatment effect (denoted $\hat{\delta}_i$) and its variance $\hat{\sigma}_i^2$. Meta-analysis combines these estimates across studies to evaluate the overall effect.


### Fixed-Effects Model
 

**Assumption**: All studies estimate the **same true effect** δ. Any variation among study results is due to **random error** or **sampling variability**, not due to differences in the underlying populations or study conditions.

This model assumes that:

$$
\hat{\delta}_i = \delta + \varepsilon_i
$$

where $\varepsilon_i \sim N(0, \hat{\sigma}_i^2)$ represents the random error in study $i$.

Thus,

$$
\hat{\delta}_i \sim N(\delta, \hat{\sigma}_i^2)
$$

The objective is to compute a **weighted average** of the individual study estimates to derive an overall estimate $\hat{\delta}$ of δ:

$$
\hat{\delta} = \sum_{i=1}^K w_i \hat{\delta}_i
$$

where the weights $w_i$ are typically chosen to give more weight to more precise studies. A common choice is:

$$
w_i = \frac{1}{\hat{\sigma}_i^2}
$$

The variance of the pooled estimate is:

$$
\text{Var}(\hat{\delta}) = \sum_{i=1}^K w_i^2 \hat{\sigma}_i^2
$$

A 95% confidence interval for the overall effect size is:

$$
\hat{\delta} \pm 1.96 \times \sqrt{\text{Var}(\hat{\delta})}
$$

A test statistic to assess statistical significance of the combined estimate is:

$$
T = \frac{\hat{\delta} - 0}{\sqrt{\text{Var}(\hat{\delta})}}
$$

**Weighting Schemes** in fixed-effects meta-analysis:

1. Equal weights: $w_i = \frac{1}{K}$, where K is the number of studies.
2. By sample size: $w_i = \frac{N_i}{N}$, where $N_i$ is the sample size in study $i$ and $N$ is the total across studies.
3. By treatment group sizes: $w_i = \frac{N_{iD} \cdot N_{iP}}{N_{iD} + N_{iP}} \times \frac{1}{w}$
4. **Inverse-variance weighting** (most common): $w_i = \frac{1}{\hat{\sigma}_i^2}$

In fixed-effects meta-analysis for binary data, a special case is the **Mantel-Haenszel method**, which is used to combine odds ratios or risk ratios across 2x2 tables using the hypergeometric distribution.


### Random-Effects Model
 
**Assumption**: Each study estimates its **own true effect** $\delta_i$, which itself is a random draw from a distribution of true effects centered at the global mean effect δ. This accounts for **between-study variability**, such as differences in study design, populations, or measurement protocols.

The model assumes:

$$
\hat{\delta}_i = \delta_i + \varepsilon_i \quad \text{with} \quad \delta_i \sim N(\delta, \tau^2)
$$

and

$$
\varepsilon_i \sim N(0, \hat{\sigma}_i^2)
$$

So, the total variance in observed effects is:

$$
\text{Var}(\hat{\delta}_i) = \tau^2 + \hat{\sigma}_i^2
$$

Here, $\tau^2$ is the **between-study variance**, and it must be estimated (e.g., using the DerSimonian-Laird method).

The pooled estimate under the random-effects model is:

$$
\hat{\delta}_{RE} = \sum_{i=1}^K w_i^* \hat{\delta}_i \quad \text{with} \quad w_i^* = \frac{1}{\hat{\sigma}_i^2 + \tau^2}
$$

The confidence interval and hypothesis testing proceed in a similar way as with fixed-effects, but with the adjusted weights accounting for both within- and between-study variability.


### Compare


The decision between using a fixed-effects model or a random-effects model in meta-analysis is **not solely determined by statistical tests for heterogeneity**, such as the Cochran’s Q test or the I² statistic. Rather, the choice should depend on both **the scientific context** and **the objectives** of the meta-analysis. Analysts must make a judgment based on the **design of the studies**, **clinical and methodological similarities**, and **the rationale behind combining the studies in the first place**.


A **fixed-effects model** is appropriate when the **true treatment effect is assumed to be the same across all included studies**. The variation in observed results from one study to another is believed to be entirely due to **sampling error** or **random within-study variation**, not due to differences in populations, interventions, or study procedures.

To justify this model, the analyst should evaluate whether there is an **a priori reason** to believe all studies are estimating the **same underlying treatment effect**. For example, this is plausible when the studies:

* Use **identical protocols** or designs,
* Involve **similar patient populations**,
* Use **uniform interventions** and **outcomes**,
* Are conducted in **similar settings**.

* Use **fixed-effects** model when:

  * Studies are **homogeneous** (similar population, methods, settings)
  * There is **no evidence** of between-study heterogeneity (e.g., I² close to 0%)
  * You believe that all studies estimate the same true effect
 

A **random-effects model** is used when there is **no strong reason to assume that the treatment effect is identical across all studies**. This model assumes that each study estimates its **own effect size**, which itself is drawn from a **distribution of effect sizes** centered around a population mean. Thus, the model incorporates **both within-study variation** and **between-study heterogeneity**.

A random-effects model is more appropriate when:

* Studies differ in **designs, populations, treatment settings**, or **outcome definitions**,
* There is **clinical or methodological diversity** among the studies,
* The treatment might **interact with population characteristics**, such as age, gender, disease severity, geography, etc.

* Use **random-effects** model when:

  * Studies are **heterogeneous** in population, protocol, or design
  * You wish to generalize the results beyond the observed studies
  * The test for heterogeneity is significant (e.g., high I² or significant Q test)
  
  
| Feature                | Fixed-Effects Model              | Random-Effects Model                         |
| ---------------------- | -------------------------------- | -------------------------------------------- |
| True Effect Assumption | One common true effect           | Distribution of true effects                 |
| Heterogeneity Allowed  | No                               | Yes                                          |
| Weighting              | Inverse of within-study variance | Inverse of (within + between-study variance) |
| Suitable When          | Studies are very similar         | Studies differ (clinical/methodological)     |
| Confidence Intervals   | Narrower                         | Wider                                        |
| Example Use            | Repeated trials in same setting  | Multi-center studies across regions          |


In practice, many analysts compute **both fixed-effects and random-effects models**, even if they believe one is theoretically more appropriate. This dual approach provides:

* A **comparison** of effect sizes and confidence intervals,
* Insight into the **influence of between-study variability**,
* A **sensitivity analysis** for model assumptions.

The confidence intervals from the random-effects model are usually **wider** than those from the fixed-effects model because they account for **both within-study and between-study variance**. If there is **no heterogeneity** (i.e., τ² = 0), both models will produce **identical results**.

However, even when the heterogeneity tests (e.g., Q or I²) are not significant, if there is **clinical or contextual reason to expect variation**, the random-effects model is typically preferred for **generalizability** and **conservative inference**.

## Implementation in R

The **rmeta** package (by Thomas Lumley) and **meta** package (by Guido Schwarzer) provide functions for both models:

* For binary data:

  * Fixed-effects (Mantel-Haenszel): `meta.MH` in `rmeta`
  * Random-effects: `meta.DSL` (DerSimonian-Laird method)

* For continuous data:

  * `metacont` function in `meta` package allows analysis under both fixed and random models.

These tools help analysts perform meta-analyses and produce key outputs such as:

* Forest plots
* Funnel plots
* Estimates of heterogeneity (I², τ²)
* Confidence intervals for pooled effect size

### meta Package: Analysis for Different Data

* Meta-analysis of binary outcome data `metabin`
* Meta-analysis of continuous outcome data `metacont`
* Meta-analysis of correlations `metacor`
* Meta-analysis of incidence rates `metainc`
* Meta-regression `metareg`
* Meta-analysis of single proportions `metaprop`
* Meta-analysis of single means `metamean`
* Merge pooled results of two meta-analyses `metamerge`
* Combine and summarize meta-analysis objects `metabind`


Meta-Analysis with Survival Outcomes

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
mg1 <- metagen(logHR, selogHR,
               studlab=paste(author, year), data=data4,
               sm="HR")
print(mg1, digits=2)

## Forest plot
plot(mg1)

## To assess potential publication bias informally, we generate the funnel plot and visually assess whether it is symmetric. 
funnel(mg1)
```

 
Meta-Analysis of Cross-Over Trials

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
# meta-analysis of these cross-over trials
mg2 <- metagen(mean, SE, studlab=paste(author, year),
               data=data5, sm="MD")
print(summary(mg2), digits=2)

plot(mg2)
funnel(mg2)
```


## Combining p-Values in Meta-Analysis

**Combining p-values in meta-analysis** is a statistical approach used when studies report only p-values rather than full effect sizes or variance estimates. This situation often arises in systematic reviews where only minimal information is available from each individual study. The goal is to aggregate the evidence from several such studies to determine whether, collectively, they support or refute a common hypothesis.

One of the most widely used methods for this purpose is **Fisher's combined probability test**, proposed by Ronald Fisher. It allows researchers to combine the results of multiple independent hypothesis tests into a single test statistic.

---

**Theoretical Basis of Fisher’s Method**

Suppose you have **K independent studies**, each testing the same null hypothesis (**H₀**) and each reporting a p-value:
**p₁, p₂, ..., pₖ**

Under the null hypothesis, each individual p-value follows a **uniform distribution** on the interval \[0, 1]. Mathematically:
**pᵢ \~ U\[0, 1]**

If you take the **negative natural logarithm** of a uniform random variable, the result follows an **exponential distribution** with mean 1. Specifically:
**−ln(pᵢ) \~ Exp(1)**

Multiplying this value by 2 gives a value that follows a **chi-square distribution** with 2 degrees of freedom:
**−2ln(pᵢ) \~ χ²(2)**

If the p-values are independent, the sum of K such terms will follow a **chi-square distribution** with **2K degrees of freedom**:

**X² = −2 ∑ ln(pᵢ)**
This statistic follows a **χ² distribution with 2K degrees of freedom** under the null hypothesis.

A large value of X² suggests that at least some of the studies are showing significant results, and that the overall null hypothesis may be false.

---



**Step-by-Step R Implementation**

You can easily implement Fisher’s method in R without using any external packages. Here's how:

1. **Define a function** that takes a vector of p-values and calculates the combined p-value.
2. **Apply the chi-square distribution function** (`pchisq`) to obtain the p-value from the combined statistic.

Here is the full function:

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
# Function to compute Fisher's combined p-value
fishers.pvalue <- function(x) {
  # x: a numeric vector of p-values
  test_stat <- -2 * sum(log(x))
  pchisq(test_stat, df = 2 * length(x), lower.tail = FALSE)
}
```
 

**Worked Example**

Assume we have 4 independent studies on the effectiveness of statins. The reported p-values from these studies are:

* 0.106
* 0.0957
* 0.00166
* 0.0694

Let’s use the function above to combine them:

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
# Vector of p-values
x <- c(0.106, 0.0957, 0.00166, 0.0694)

# Apply Fisher's method
combined.pval <- fishers.pvalue(x)

# Print the result
print(combined.pval)
```


However, a few important points should be noted:

* This method **does not account for effect size heterogeneity**.
* It is **sensitive to very small p-values**, which can dominate the result.
* All included studies must test the **same hypothesis**, and the tests must be **independent**.

---

**Conclusion**

Fisher’s method is a powerful and simple tool to combine p-values when full meta-analytic data (like effect sizes and variances) are not available. It is particularly useful in early-phase systematic reviews, meta-analyses of hypothesis tests, or when only limited summary data are accessible.


# Continuous Outcomes using Fixed Effect Model

## Effect Measures  

Meta-analysis typically focuses on comparing two interventions, which we refer to as experimental and control. When the response is continuous (i.e. quantitative) typically the mean, standard deviation and sample size are reported for each group.
 
Suppose the goal is to evaluate whether the two groups differ in terms of their population means. Let:

* **µ₁** = true mean of the Treated group
* **µ₂** = true mean of the Control group
* **Δ = µ₁ − µ₂** = difference in population means (also called the **mean difference**)
* **δ = (µ₁ − µ₂)/σ** = standardized mean difference (or **effect size**), where **σ** is a standard deviation (either pooled or from the control group)

**Summary**

* Use raw **mean differences** (D) when outcome scales are consistent across studies
* Use **standardized mean differences** (δ, Cohen’s d, Hedges’ g) when outcome scales differ
* Hedges’ g is preferred in meta-analysis due to its bias correction
* Statistical inference (Z-tests, confidence intervals) depends on the variance of the effect size estimate
* Correction factors and variance formulas vary slightly in the literature but typically lead to very similar results for reasonably sized samples


### Estimating the Mean Difference (Δ)

When all studies in the meta-analysis report outcomes using the same scale or unit, we can directly compute and combine the raw **mean differences**.

For a single study:

* Let **X̄₁** and **X̄₂** be the sample means of the Treated and Control groups, respectively
* Let **S₁** and **S₂** be the corresponding sample standard deviations
* Let **n₁** and **n₂** be the sample sizes of the two groups

Then, the sample mean difference **D** is estimated by:

**D = X̄₁ − X̄₂**

To compute confidence intervals or perform hypothesis testing, we need the **variance** of D.

There are two cases:

**Case 1: Unequal Variances (Heteroscedasticity)**

Assume the two groups have different variances (σ₁² ≠ σ₂²). Then the variance of D is:

**Var(D) = (S₁² / n₁) + (S₂² / n₂)**

**Case 2: Equal Variances (Homoscedasticity)**

If variances are assumed equal (σ₁ = σ₂ = σ), a **pooled variance** is used:

**S²\_pooled = \[ (n₁ − 1)S₁² + (n₂ − 1)S₂² ] / (n₁ + n₂ − 2)**

Then, the variance of D is:

**Var(D) = (n₁ + n₂) / (n₁n₂) × S²\_pooled**

The **standard error** of D (SED) is simply the square root of its variance:

**SED = √Var(D)**

In meta-analysis, we combine the D estimates from multiple studies, weighting them by the inverse of their variances.


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
# use the metacont function to calculate mean difference and confidence interval
# sm="MD" (i.e. summary measure is the Mean Difference) as default setting.
data(Fleiss1993cont) 
m1_MD <- metacont(n.psyc, mean.psyc, sd.psyc, n.cont, mean.cont, sd.cont,
                  data = Fleiss1993cont, 
                  studlab=rownames(Fleiss1993cont),
                  sm = "MD")
m1_MD
plot(m1_MD)
```

 
### Estimating the Standardized Mean Difference (δ)

When different studies use **different measurement scales**, it's not meaningful to compare or pool raw mean differences directly. Instead, we use the **standardized mean difference (SMD)**, which removes scale effects.

SMD is defined as:

**δ = (µ₁ − µ₂) / σ**

where **σ** is a standard deviation, either from the control group or pooled from both.

---

**Two Common Estimators of δ:**

1. **Cohen's d** (proposed by Cohen, 1988)

This is calculated as:

**d = (X̄₁ − X̄₂) / S**

Where **S** is the pooled standard deviation:

**S² = \[ (n₁ − 1)S₁² + (n₂ − 1)S₂² ] / (n₁ + n₂)**

Then, **S = √S²**

Note: Cohen’s d slightly **overestimates** the true δ when sample sizes are small.

---

2. **Hedges’ g** (proposed by Hedges, 1982)

This is a corrected version of Cohen’s d for small sample bias:

**g = (X̄₁ − X̄₂) / S∗**

Where:

**S∗² = \[ (n₁ − 1)S₁² + (n₂ − 1)S₂² ] / (n₁ + n₂ − 2)**
This is the traditional **pooled sample variance**. Taking the square root gives **S∗**.

However, **g is biased**, and this bias can be corrected using a **correction factor J**:

**g∗ = J × g**, where **J = 1 − (3 / (4N − 9))** and **N = n₁ + n₂**

Then **g∗** is an approximately **unbiased estimate** of δ.

An approximate formula for the variance of g∗ is:

**Var(g∗) ≈ (1 / ñ) + (g∗² / (2(N − 3.94)))**

Where:

**ñ = (n₁ × n₂) / (n₁ + n₂)** is the **harmonic mean** of sample sizes

This variance formula is compatible with the **R `meta` package**, although other literature might use alternatives such as 2N, 2(N − 2), etc. These make very little difference unless **n₁ and n₂ are very small**.

**Hypothesis Testing for Effect Size δ**

To test **H₀: δ = 0** (no effect) versus **H₁: δ ≠ 0**, we use a **Z-statistic**:

**Z = g∗ / √Var(g∗)**

We reject the null hypothesis if |Z| exceeds the critical value from the standard normal distribution (typically zₐ/2 = 1.96 for a 95% confidence level).

A **confidence interval** for δ can be constructed as:

**CI = g∗ ± zₐ/2 × √Var(g∗)**

> There is a proportional relationship:

**d = (n₁ + n₂) / (n₁ + n₂ − 2) × g = (n₁ + n₂) / (n₁ + n₂ − 2) × g∗ / J**
 

 
* use the metacont function to calculate mean difference and confidence interval

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
# use the metacont function to calculate mean difference and confidence interval
# sm="MD" (i.e. summary measure is the Mean Difference) as default setting.
data(Fleiss1993cont) 
m1_SMD <- metacont(n.psyc, mean.psyc, sd.psyc, n.cont, mean.cont, sd.cont,
                   data = Fleiss1993cont, 
                   studlab=rownames(Fleiss1993cont),
                   sm = "SMD")
data.frame(
  Study      = m1_SMD$studlab,
  SMD        = round(m1_SMD$TE, 3),
  SE         = round(m1_SMD$seTE, 3),
  CI_lower   = round(m1_SMD$lower, 3),
  CI_upper   = round(m1_SMD$upper, 3)
)

m1_SMD

plot(m1_SMD)

# Use Cohen's d instead of Hedges' g as effect measure
update(m1_SMD, method.smd = "Cohen")
```


## Heterogeneity in Meta-Analysis


In meta-analysis, **heterogeneity** refers to the variation in study outcomes beyond what would be expected by random sampling alone. In other words, it captures how different the effect sizes are across studies. Understanding and quantifying heterogeneity is essential because it influences the choice of meta-analytic model (fixed-effect vs random-effects) and the interpretation of results.

When using the `meta` package in R (e.g., through functions like `metacont()` or `metabin()`), the output includes a section called **“Quantifying heterogeneity”** and **“Test of heterogeneity”**, which provides several statistics to assess this variability. 

**Summary of Interpretation**

* **Q statistic** tells you whether heterogeneity is present (yes/no)
* **τ²** tells you how much true heterogeneity exists (in raw variance units)
* **H** tells you how much more variability is observed than expected
* **I²** tells you the proportion of observed variance that is true heterogeneity

---

**Test of Heterogeneity: Cochran’s Q**

Cochran’s Q statistic is a classical test that evaluates whether the variability in observed effects across studies is greater than expected by chance.

It is defined as:

**Q = Σ wi × (δ̂i − δ̂)²**

Where:

* **δ̂i** is the effect estimate from the *i*th study
* **δ̂** is the overall pooled effect estimate
* **wi** is the weight assigned to the *i*th study, typically inverse variance

This formula calculates a **weighted sum of squared deviations** of the individual effect sizes from the overall effect.

Under the null hypothesis that all studies share the same true effect size (i.e., no heterogeneity), **Q follows a chi-squared distribution with (K − 1) degrees of freedom**, where K is the number of studies.

If the **p-value associated with Q** is less than a chosen significance level (e.g., 0.05), the null hypothesis is rejected, indicating **significant heterogeneity** across studies.

However, the Q test has limitations:

* It has **low power** when the number of studies is small, meaning it may **fail to detect real heterogeneity**.
* It has **excessive power** when the number of studies is large, meaning it may flag **trivial heterogeneity as significant**.

Because of these issues, researchers often use additional statistics to **quantify** the amount of heterogeneity, not just test for its presence.

Beyond the Q test, the `meta` package provides three additional indices to **describe the magnitude of heterogeneity**: τ² (tau-squared), H, and I².

---

**Tau-squared (τ²)**

Tau-squared estimates the **between-study variance**, i.e., the variance of the true effect sizes across studies. This value is crucial in **random-effects models**, where it directly influences the weights assigned to each study.

Tau-squared is estimated using the **DerSimonian and Laird method**:

**τ² = (Q − (K − 1)) / U**

Where:

* Q is Cochran’s Q statistic
* K is the number of studies
* U is a function of the study weights

If Q < K − 1 due to sampling error, τ² is set to zero. A larger τ² indicates more dispersion in the true effects.

---

**H Index**

The H index expresses heterogeneity as a **ratio** comparing the observed variability to what would be expected if all studies had the same true effect size.

It is defined as:

**H = √(Q / (K − 1))**

If **H = 1**, it implies perfect homogeneity (no excess variability). Values of H > 1 indicate increasing levels of heterogeneity.

A confidence interval for H can be constructed by assuming that **ln(H)** is approximately normally distributed. If the lower bound of this interval is above 1, it suggests statistically significant heterogeneity.

The formula for the standard error of ln(H) depends on the value of Q:

* If Q > K: SE is estimated using log transformations
* If Q ≤ K: SE uses a different approximation due to boundary issues

---

**I-squared (I²)**

I² quantifies the **percentage of total variation** in observed effects that is due to **real heterogeneity** rather than chance. It is arguably the most commonly reported measure of heterogeneity.

The formula is:

**I² = \[(Q − (K − 1)) / Q] × 100%**

This value ranges from 0% to 100%:

* **0–25%**: low heterogeneity
* **25–50%**: moderate heterogeneity
* **50–75%**: substantial heterogeneity
* **75–100%**: considerable heterogeneity

Negative values of I² are set to 0 by convention. If the **lower bound of the confidence interval for I² is greater than zero**, heterogeneity is considered statistically significant.

Alternatively, I² can be computed from H:

**I² = (H² − 1) / H² × 100%**

This formulation is used to derive confidence intervals for I² from those for H.
 

## Inverse Variance-Weighted Average Method

There are two methods for the fixed effects model meta-analysis: the IVW and weighted SZ. The fixed effects model assumes that all studies in a meta-analysis share a single true effect size. 


固定效应模型假设荟萃分析中成分研究 component studies 的估计效应来自单个同质群体 single homogeneous populatio。 因此，为了计算总体估算值，我们对每项研究的估算值取平均值，从而考虑到某些估算值比其他估算值更为精确的事实（来自较大的研究）

More formally, let $k=1, \ldots, K$ index study, $\hat{\theta}_{k}$ denote the intervention effect estimate from study $k$,
and $\theta$ denote the intervention effect in the population, which we wish to estimate. Denote by $\hat{\sigma}_{k}^{2}$ the sample estimate of $\operatorname{Var}\left(\hat{\theta}_{k}\right)$. The fixed effect model is
$$
\hat{\theta}_{k}=\theta+\sigma_{k} \epsilon_{k}, \quad \epsilon_{k}^{\mathrm{i} . \mathrm{i} . \mathrm{d} .} N(0,1)
$$
We now consider the fixed effect estimate of $\theta$, denoted by $\hat{\theta}_{F}$. Given estimates $\left(\hat{\theta}_{k}, \hat{\sigma}_{k}\right), k=1, \ldots, K$, the maximum-likelihood estimate under model (2.7) is
$$
\hat{\theta}_{F}=\frac{\sum_{k=1}^{K} \hat{\theta}_{k} / \hat{\sigma}_{k}^{2}}{\sum_{k=1}^{K} 1 / \hat{\sigma}_{k}^{2}}=\frac{\sum_{k=1}^{K} w_{k} \hat{\theta}_{k}}{\sum_{k=1}^{K} w_{k}}
$$
Accordingly, $\hat{\theta}_{F}$ is a weighted average of the individual effect estimates $\hat{\theta}_{k}$ with weights $w_{k}=1 / \hat{\sigma}_{k}^{2}$. Therefore, this method is called the **inverse variance method**. The variance of $\hat{\theta}_{F}$ is estimated by
$$
\widehat{\operatorname{Var}}\left(\hat{\theta}_{F}\right)=\frac{1}{\sum_{k=1}^{K} w_{k}}
$$
$(1-\alpha)$ confidence interval for $\hat{\theta}_{F}$ can be calculated by
$$\hat{\theta}_{F} \pm z_{1-\frac{\alpha}{2}} \text { SE }\left(\hat{\theta}_{F}\right)$$



```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
# The fixed effect estimate and its variance can be calculated using base R code

# 1. Calculate mean difference, variance and weights
MD <- with(data1, Me - Mc)
varMD <- with(data1, Se^2/Ne + Sc^2/Nc)
weight <- 1/varMD

# 2. Calculate the inverse variance estimator
# the standard weighted.mean function is used to calculate theta_F.
round(weighted.mean(MD, weight), 4)

# 3. Calculate the variance
round(1/sum(weight), 4)


# Alternative easier using the metacont function which yields identical results
mc1 <- metacont(Ne, Me, Se, Nc, Mc, Sc,
                data=data1,
                studlab=paste(author, year))
round(c(mc1$TE.fixed, mc1$seTE.fixed^2), 4)


# Forest Plot
# pdf(file="Schwarzer-Fig2.3.pdf", width=9.6) 
# uncomment line to generate PDF file
forest(mc1, comb.random=FALSE, xlab=
         "Difference in mean response (intervention - control)
units: maximum % fall in FEV1",
       xlim=c(-50,10), xlab.pos=-20, smlab.pos=-20)
# invisible(dev.off()) 
# uncomment line to save PDF file


```

## Generic inverse variance meta-analysis `metagen`

Fixed effect and random effects meta-analysis based on estimates (e.g. log hazard ratios) and their
standard errors. The inverse variance method is used for pooling.

* sm: A character string indicating underlying summary measure, e.g., "RD", "RR", "OR", "ASD", "HR", "MD", "SMD", or "ROM".
* Confidence intervals for individual studies: For the mean difference (argument sm = "MD"), the confidence interval for individual studies can be based on the
    + standard normal distribution (method.ci = "z"), or
    + t-distribution (method.ci = "t").
* Estimation of between-study variance:
    + `method.tau = "DL"` DerSimonian-Laird estimator (DerSimonian and Laird, 1986)
    + `method.tau = "PM"` Paule-Mandel estimator (Paule and Mandel, 1982)
    + `method.tau = "REML"` Restricted maximum-likelihood estimator (Viechtbauer, 2005)
    + `method.tau = "ML"` Maximum-likelihood estimator (Viechtbauer, 2005)
    + `method.tau = "HS"` Hunter-Schmidt estimator (Hunter and Schmidt, 2015)
    + `method.tau = "SJ"` Sidik-Jonkman estimator (Sidik and Jonkman, 2005)
    + `method.tau = "HE"` Hedges estimator (Hedges and Olkin, 1985)
    + `method.tau = "EB"` Empirical Bayes estimator (Morris, 1983)
* Confidence interval for the between-study variance:
    + `method.tau.ci = "J"` Method by Jackson (2013)
    + `method.tau.ci = "BJ"` Method by Biggerstaff and Jackson (2008)
    + `method.tau.ci = "QP"` Q-Profile method (Viechtbauer, 2007)
    + `method.tau.ci = "PL"` Profile-Likelihood method for three-level meta-analysis model (Van den Noortgate et al., 2013)


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
mc1.gen <- metagen(TE, seTE, data=mc1, sm="MD")
# Print results for fixed effect and random effects method
c(mc1$TE.fixed, mc1$TE.random)
c(mc1.gen$TE.fixed, mc1.gen$TE.random)
```


## Weighted Sum of Z-Scores

Another popular method for the fixed effects model meta-analysis is calculating the weighted SZ from the follows studies. Let $Z_{i}$ be the z-score from study $i$, which $N(0,1)$ under the null hypothesis of no effects. Then, the weighted SZ statistic is
$$
Z_{S Z}=\frac{\sum w_{S Z, i} Z_{i}}{\sqrt{\sum w_{S Z, i}{ }^{2}}}
$$
By the characteristic of a normal distribution, $\mathrm{Z}_{\mathrm{SZ}}$ also follows $N(0,1)$ under the null hypothesis. To combine z-scores from multiple studies, a per-study sample size was suggested as weights of each study, as follows:
$$
w_{S Z, i}=\sqrt{N_{i}}
$$


# Continuous Outcomes using Random Effects Model 

## Introduction

The random effects model seeks to account for the fact that the study effect estimates $\hat{\theta}_{k}$ are often
more variable than assumed in the fixed effect model. Under the random effects model,
$$
\hat{\theta}_{k}=\theta+u_{k}+\sigma_{k} \epsilon_{k}, \quad \epsilon_{k}^{\text {i.i.d. }} N(0,1) ; u_{k}^{\text {i.i.d. }} N\left(0, \tau^{2}\right)
$$
where the $u$ 's and $\epsilon$ 's are independent. Define
$$
Q=\sum_{k=1}^{K} w_{k}\left(\hat{\theta}_{k}-\hat{\theta}_{F}\right)^{2}
$$
the weighted sum of squares about the fixed effect estimate with $w_{k}=1 / \hat{\sigma}_{k}^{2}$. This is usually referred to as either the homogeneity test statistic or the heterogeneity statistic. Next define
$$
S=\sum_{k=1}^{K} w_{k}-\frac{\sum_{k=1}^{K} w_{k}^{2}}{\sum_{k=1}^{K} w_{k}}
$$
If $Q<(K-1)$, then $\hat{\tau}^{2}$ is set to 0 and the random effects estimate $\hat{\theta}_{R}$ is set equal to the fixed effect estimate $\hat{\theta}_{F}$. Otherwise, the **Dersimonian-Laird estimator of the between-study variance** is
defined as
$$
\hat{\tau}^{2}=\frac{Q-(K-1)}{S}
$$
and the random effects estimate and its variance are given by
$$
\begin{array}{c}
\hat{\theta}_{R}=\frac{\sum_{k=1}^{K} w_{k}^{*} \hat{\theta}_{k}}{\sum_{k=1}^{K} w_{k}^{*}} \\
\operatorname{Var}\left(\hat{\theta}_{R}\right)=\frac{1}{\sum_{k=1}^{K} w_{k}^{*}}
\end{array}
$$
with weights $w_{k}^{*}=1 /\left(\hat{\sigma}_{k}^{2}+\hat{\tau}^{2}\right)$. The random effects estimator $\hat{\theta}_{R}$ is a weighted average of the individual effect estimates $\hat{\theta}_{k}$ with weights $1 /\left(\hat{\sigma}_{k}^{2}+\hat{\tau}^{2}\right)$. Accordingly, this method is often called "**Inverse variance method**", too. A $(1-\alpha)$ confidence interval for $\hat{\theta}_{R}$ can be calculated by
$$\hat{\theta}_{R} \pm z_{1-\frac{\alpha}{2}} \text { S.E. }\left(\hat{\theta}_{R}\right)$$

## Implementation

**Estimation of Between-Study Variance**

* DerSimonian–Laird estimator (method.tau="DL") (default) 
* Paule–Mandel estimator (method.tau="PM") 
* Restricted maximum-likelihood estimator (method.tau="REML") 
* Maximum-likelihood estimator (method.tau="ML") 
* Hunter–Schmidt estimator (method.tau="HS") 
* Sidik–Jonkman estimator (method.tau="SJ") 
* Hedges estimator (method.tau="HE")
* Empirical Bayes estimator (method.tau="EB").

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
# 1. Conduct meta-analyses
# 1a. DerSimonian-Laird estimator (default)
mg1.DL <- metagen(TE, seTE, data=mc1)
# 1b. Paule-Mandel estimator
mg1.PM <- metagen(TE, seTE, data=mc1, method.tau="PM")
# 1c. Restricted maximum-likelihood estimator
mg1.RM <- metagen(TE, seTE, data=mc1, method.tau="REML")
# 1d. Maximum-likelihood estimator
mg1.ML <- metagen(TE, seTE, data=mc1, method.tau="ML")
# 1e. Hunter-Schmidt estimator
mg1.HS <- metagen(TE, seTE, data=mc1, method.tau="HS")
# 1f. Sidik-Jonkman estimator
mg1.SJ <- metagen(TE, seTE, data=mc1, method.tau="SJ")
# 1g. Hedges estimator
mg1.HE <- metagen(TE, seTE, data=mc1, method.tau="HE")
# 1h. Empirical Bayes estimator
mg1.EB <- metagen(TE, seTE, data=mc1, method.tau="EB")
# 2. Extract between-study variance tau-squared
tau2 <- data.frame(tau2=round(c(0,
                                mg1.DL$tau^2, mg1.PM$tau^2,
                                mg1.RM$tau^2, mg1.ML$tau^2,
                                mg1.HS$tau^2, mg1.SJ$tau^2,
                                mg1.HE$tau^2, mg1.EB$tau^2), 2),
                   row.names=c("FE", "DL", "PM", "REML", "ML",
                               "HS", "SJ", "HE", "EB"))
# 3. Print tau-squared values
t(tau2)
# 4. Create dataset with summaries
res <- data.frame(MD=c(mg1.DL$TE.fixed,
                       mg1.DL$TE.random, mg1.PM$TE.random,
                       mg1.RM$TE.random, mg1.ML$TE.random,
                       mg1.HS$TE.random, mg1.SJ$TE.random,
                       mg1.HE$TE.random, mg1.EB$TE.random),
                  seMD=c(mg1.DL$seTE.fixed,
                         mg1.DL$seTE.random, mg1.PM$seTE.random,
                         mg1.RM$seTE.random, mg1.ML$seTE.random,
                         mg1.HS$seTE.random, mg1.SJ$seTE.random,
                         mg1.HE$seTE.random, mg1.EB$seTE.random),
                  method=c("",
                           "DerSimonian-Laird",
                           "Paule-Mandel",
                           "Restricted maximum-likelihood",
                           "Maximum-likelihood",
                           "Hunter-Schmidt",
                           "Sidik-Jonkman",
                           "Hedges",
                           "Empirical Bayes"),
                  tau2=tau2,
                  model=c("Fixed-effect model",
                          rep("Random-effect model", 8)))
knitr::kable(res)

# 5. Do meta-analysis
m <- metagen(MD, seMD, data=res,
             studlab=method,
             sm="MD",
             comb.fixed=FALSE, comb.random=FALSE,
             byvar=model)
knitr::kable(m)

# 6. Do forest plot
# pdf(file="Schwarzer-Fig2.5.pdf", width=8.1, height=4.0) 
# uncomment line to generate PDF file
forest(m,
       xlim=c(-20, -12),
       hetstat=FALSE, smlab="",
       leftcols=c("studlab", "tau2"),
       leftlabs=c("Method", "Between-study\nheterogeneity"),
       print.byvar=FALSE)
# invisible(dev.off()) # uncomment line to save PDF file   
```

## Hartung-Knapp Adjustment

Hartung and Knapp (2001a,b) proposed an alternative method for random effects meta-analysis
based on a refined variance estimator for the treatment estimate. Simulation studies (Hartung and
Knapp, 2001a,b; IntHout et al., 2014; Langan et al., 2019) show **improved coverage probabilities**
compared to the classic random effects method. 


Hartung和Knapp 在随机效应模型中引入了一种基于改进方差估计器的新的会萃分析方法。与DerSimonianLaird方法相比，首选Hartung-Knapp方法。 代替使方差估计
$$
\widehat{\operatorname{Var}}\left(\hat{\theta}_{R}\right)=\frac{1}{\sum_{k=1}^{K} w_{k}^{*}}
$$
Instead of using the variance estimate given in Eq. (2.14), Hartung and Knapp propose to use the following variance estimator for $\hat{\theta}_{R}$ :
$$
\widehat{\operatorname{Var}}_{\mathrm{HK}}\left(\hat{\theta}_{R}\right)=\frac{1}{K-1} \sum_{k=1}^{K} \frac{w_{k}^{*}}{w^{*}}\left(\hat{\theta}_{k}-\hat{\theta}_{R}\right)^{2}
$$
with weights $w_{k}^{*}$ as given in above and $w=\sum k=1^{K} w k$. Hartung showed that
$$
\frac{\hat{\theta}_{R}-\theta}{\text { S.E. } \mathrm{HK}\left(\hat{\theta}_{R}\right)}
$$
with standard error s.E. $\mathrm{HK}\left(\hat{\theta}_{R}\right)=\sqrt{\widehat{\operatorname{Var}}_{\mathrm{HK}}\left(\hat{\theta}_{R}\right)}$ follows a $t$ -distribution with $K-1$ degrees of freedom. Accordingly, a $(1-\alpha)$ confidence interval for $\hat{\theta}_{R}$ based on the Hartung-Knapp method
can be calculated by
$$
\hat{\theta}_{R} \pm t_{K-1 ; 1-\frac{\alpha}{2}} \text { S.E. } \mathrm{HK}\left(\hat{\theta}_{R}\right)
$$


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
 # use the metacont function to conduct the Hartung–Knapp adjustment
mc2.hk <- metacont(Ne, Me, Se, Nc, Mc, Sc, sm="SMD",
                   data=data2, comb.fixed=FALSE,
                   hakn=TRUE)
```



However, in rare settings with very homogeneous
treatment estimates, the Hartung-Knapp (HK) variance estimate can be arbitrarily small resulting
in a very narrow confidence interval (Knapp and Hartung, 2003; Wiksten et al., 2016). In such
cases, an ad hoc variance correction has been proposed by utilising the variance estimate from
the classic random effects model with the HK method (Knapp and Hartung, 2003; IQWiQ, 2020).
An alternative approach is to use the wider confidence interval of classic fixed or random effects
meta-analysis and the HK method (Wiksten et al., 2016; Jackson et al., 2017).

* `adhoc.hakn = ""` Ad hoc method not used
* `adhoc.hakn = "se"` use variance correction if HK standard error is smaller than standard error from classic random effects meta-analysis (Knapp and Hartung, 2003)
* `adhoc.hakn = "iqwig6"` use variance correction if HK confidence interval is narrower than CI from classic random effects model with DerSimonian-Laird estimator (IQWiG, 2020)
* `adhoc.hakn = "ci"` use wider confidence interval of classic random effects and HK meta-analysis (Hybrid method 2 in Jackson et al., 2017)






# Meta-Regression

**Meta-regression** is an extension of meta-analysis designed to explain **residual heterogeneity** among study results by incorporating **study-level moderators** (also called covariates or independent variables). These moderators might include characteristics such as study year, mean age of participants, treatment dosage, study quality score, or geographic region.

In essence, meta-regression allows us to model how the **effect size** changes across studies depending on these moderators. This helps uncover patterns and improve understanding of why studies differ in their results.

## Basic Concept 


Just as a standard regression model evaluates how a dependent variable is influenced by one or more predictors, a meta-regression regresses the **effect sizes** reported in individual studies on one or more **study-level characteristics**. The key difference is that each effect size is estimated with a known variance (from the original studies), and that this variance should be incorporated into the estimation procedure.

The effect size from study *i* is denoted as **δ̂ᵢ**. These observed effect sizes vary due to both:

1. **Within-study sampling variance** (denoted σ̂²ᵢ), and
2. **Between-study variance** (denoted τ²), which is estimated if a **random-effects model** is used.

The general **meta-regression model** can be written as:

**δᵢ = β₀ + β₁xᵢ₁ + β₂xᵢ₂ + … + βₚxᵢₚ + νᵢ + εᵢ**

* δᵢ: true effect size for study *i*
* xᵢ₁, xᵢ₂, ..., xᵢₚ: study-level moderators
* β₀: intercept (mean effect when all covariates are 0)
* βⱼ: regression coefficient for moderator *j*
* νᵢ \~ N(0, τ²): between-study random effect (residual heterogeneity)
* εᵢ \~ N(0, σ̂²ᵢ): within-study error (sampling variance from each study)

---

**Fixed-Effects vs Random-Effects Meta-Regression**

In **fixed-effects meta-regression**, it is assumed that all residual variation in effect size across studies is due to within-study sampling error (τ² = 0). This is appropriate only if there is no heterogeneity beyond sampling error.

In contrast, **random-effects meta-regression** allows for additional variability in true effect sizes not explained by sampling error, acknowledging that effect sizes may truly differ across studies.

* Use fixed-effects meta-regression when you believe studies share a common true effect.
* Use random-effects meta-regression when studies likely differ in their true effects, even after accounting for moderators.

---

**Interpreting Meta-Regression**

The main goals are:

* To identify which moderators significantly **explain** heterogeneity.
* To understand the **direction** and **magnitude** of association between moderators and effect sizes.
* To quantify how much **residual heterogeneity** (unexplained variance) remains after including moderators.

Statistical output includes:

* Estimates of **β coefficients** (effect of each moderator on the outcome)
* Their **standard errors**, **z-values**, and **p-values**
* The estimate of **τ²**, which is the residual heterogeneity **after** accounting for moderators
* Measures like **QM** (analogous to the regression F-test), testing whether all β’s are jointly zero


## Implementing Meta-Regression in R with metafor

The `meta` and `rmeta` packages do not support meta-regression. Instead, the **`metafor`** package is used, which is designed for flexible meta-analysis and includes advanced features like meta-regression.

To perform **meta-analysis** using the `rma()` function in the **`metafor`** R package, you must specify the **method** used to estimate the **between-study variance τ²**, which is critical for random-effects models. The choice of method can impact the estimated τ² and, consequently, the weighting of studies and the overall meta-analytic result.


**When to Choose Which Method?**

* Use `"REML"` when you want **accurate τ² estimation** and plan to use meta-regression.
* Use `"DL"` when replicating older studies or when simplicity and speed are needed.
* Use `"SJ"` or `"EB"` if you're concerned about **underestimating** between-study heterogeneity.
* Use `"ML"` if you want to compare models using likelihood criteria (e.g., AIC).


**Fixed-Effects Model**

If you assume **no between-study heterogeneity** (i.e., τ² = 0), you are using a **fixed-effects model**. You can specify it explicitly with:

```r
method = "FE"
```

This method assumes that all studies estimate the same true effect size, and any observed differences are due to sampling error alone.

---

**Random-Effects Models and τ² Estimation Methods**

For **random-effects meta-analysis**, τ² must be estimated. The `rma()` function provides multiple estimators for τ², each with different theoretical backgrounds and properties.

Here are the main methods you can choose from:

1. **DerSimonian-Laird ("DL")**

   * Classic method used in most meta-analyses historically.
   * Based on method of moments.
   * **Quick and easy**, but known to **underestimate τ²**, especially with few studies or when heterogeneity is large.
   * Use:

     ```r
     rma(yi, vi, data = dat, method = "DL")
     ```

2. **Hunter-Schmidt ("HS")**

   * Used mostly in psychological and behavioral science meta-analyses.
   * Assumes **true score variance model**, often used with correlation coefficients.
   * Less common in medical or epidemiological contexts.
   * Use:

     ```r
     method = "HS"
     ```

3. **Hedges ("HE")**

   * Based on Hedges and Olkin’s method.
   * Also a **method-of-moments** estimator, more robust than DL under certain conditions.
   * Known to produce **less biased τ² estimates** when variances are unequal.
   * Use:

     ```r
     method = "HE"
     ```

4. **Sidik-Jonkman ("SJ")**

   * A newer estimator.
   * Tends to **overestimate τ²** in small samples but is **very conservative**.
   * Good when aiming for **wider confidence intervals** to avoid overconfidence.
   * Use:

     ```r
     method = "SJ"
     ```

5. **Empirical Bayes ("EB")**

   * Uses Bayesian principles to shrink individual study estimates toward the overall mean.
   * Especially useful when the number of studies is **very small**.
   * Use:

     ```r
     method = "EB"
     ```

6. **Maximum Likelihood ("ML")**

   * Optimizes the likelihood of observing the data given a model.
   * Biased in small samples (underestimates τ²), but still widely used.
   * Allows model comparisons (e.g., AIC).
   * Use:

     ```r
     method = "ML"
     ```

7. **Restricted Maximum Likelihood ("REML")**

   * Default method in `rma()`.
   * Adjusts for degrees of freedom used in estimating fixed effects.
   * **Asymptotically unbiased and efficient**.
   * Recommended for most applications.
   * Use:

     ```r
     method = "REML"
     ```
     
     
     
**To Do with Doing Meta-Analysis in R: A Hands-on Guide**

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
 
```

**Practical Notes**

* Use only study-level covariates (not individual participant-level variables).
* Be cautious when including many moderators in a model with a small number of studies.
* Avoid overfitting; meta-regression should be guided by theory or prior knowledge.
* Meta-regression is observational—**associations do not imply causation**.


## Meta-regression vs. Weighted Regression

**Meta-regression vs. Weighted Regression** is a subtle but important distinction in statistical modeling, especially in the context of meta-analysis.

At first glance, meta-regression appears very similar to ordinary **weighted linear regression**, where the **effect size estimates** (e.g., log odds ratios, SMDs, or risk differences from individual studies) are regressed on **study-level covariates** using a set of **weights**. However, while the **parameter estimates** (the regression coefficients) from both approaches will typically match, the **standard errors**, **confidence intervals**, and **p-values** can differ significantly. The key lies in the **assumed error structure** of the models.

In **meta-regression**, the goal is to estimate how **study-level moderators** (like geographic region, year, or sample size) explain variations in the effect sizes across studies. The correct meta-regression model assumes:

* Known and fixed variances for each study's effect size (provided as input)
* Residuals that have a **known variance structure**, namely:

  **Var(εᵢ) = σ̂ᵢ² + τ²**

This combined variance includes:

* **σ̂ᵢ²**, the within-study variance (from each study)
* **τ²**, the between-study variance (heterogeneity), estimated by `rma()` from the data

These two components make up the weights:

**wᵢ = 1 / (σ̂ᵢ² + τ²)**

---

**The Mistake with `lm()` and Weighted Regression**

The R function `lm()` can perform weighted linear regression via the `weights` argument, and it will indeed **give the correct point estimates** for the regression coefficients if you use **wᵢ** as the weights. However, it does **not make the correct distributional assumption** for the residuals.

Specifically:

* `lm()` assumes residuals εᵢ are from **N(0, σ² × wᵢ)**, where σ² is an **unknown common error variance** that is **estimated from the data**
* Meta-regression assumes εᵢ ∼ **N(0, 1 / wᵢ)** — i.e., the weights are already the known inverse variances, and no additional σ² scaling is needed

Because of this mismatch, `lm()` will inflate the standard errors by a factor of √(σ̂²), which results in **incorrect inference** (i.e., p-values and confidence intervals will be wrong).

---

**Conclusion**

* **Meta-regression** and **weighted regression** use the same formula for calculating coefficient estimates, but they differ in the assumed variance model of the residuals.
* **lm() incorrectly estimates an extra residual variance term (σ²)**, leading to inflated standard errors and incorrect inference.
* Always use `rma()` or manual calculations with fixed weights and no additional residual variance term when performing meta-regression.



 
# Binary Outcomes

## Effect Measures

In meta-analysis, especially when dealing with binary outcomes (such as "event occurred" vs. "event did not occur"), choosing an appropriate **effect measure** is crucial for meaningful interpretation and synthesis across studies. The most commonly used effect measures include:


**Study Summary Data (2×2 Table)**

In each study $k$ included in the meta-analysis (where $k = 1, ..., K$), the outcome data can be summarized in a 2×2 contingency table:

$$
\begin{array}{l|c|c|c}
\hline
 & \text{Event} & \text{No Event} & \text{Group Size} \\
\hline
\text{Experimental Group} & a_k & b_k & n_{ek} = a_k + b_k \\
\text{Control Group} & c_k & d_k & n_{ck} = c_k + d_k \\
\hline
\text{Totals} & a_k + c_k & b_k + d_k & n_k = n_{ek} + n_{ck} \\
\hline
\end{array}
$$

From this, the **estimated probabilities** of event occurrence in each group are calculated as:

$$
\hat{p}_{e k} = \frac{a_k}{n_{e k}} \quad \text{and} \quad \hat{p}_{c k} = \frac{c_k}{n_{c k}}
$$

These probabilities are then used to compute the chosen effect measure (OR, RR, RD, ASD) for each study $k$, which are subsequently pooled using either a **fixed-effects** or **random-effects** model, depending on heterogeneity and modeling assumptions.
 
 
---

**1. Odds Ratio (OR)**

The **odds ratio** compares the odds of the event occurring in the treatment group to the odds in the control group.

$$
\text{OR}_k = \frac{a_k / b_k}{c_k / d_k} = \frac{a_k d_k}{b_k c_k}
$$

* **Odds**: the ratio of the probability that the event occurs to the probability that it does not occur.
* Often used in **case-control studies** or where outcomes are **rare**.
* **Log(OR)** is approximately normally distributed, making it suitable for statistical modeling and meta-analysis.

---

**2. Risk Ratio (RR)**

Also called the **relative risk**, this measure compares the **probability** of the event between the two groups.

$$
\text{RR}_k = \frac{\hat{p}_{e k}}{\hat{p}_{c k}} = \frac{a_k / (a_k + b_k)}{c_k / (c_k + d_k)} = \frac{a_k / n_{e k}}{c_k / n_{c k}}
$$

* Indicates how many times more (or less) likely an event is in the treatment group compared to the control.
* Intuitive and widely used in **randomized controlled trials**.
* Less statistically stable than OR when event probabilities approach 0 or 1.

---

**3. Risk Difference (RD)**

The **risk difference** measures the absolute difference in event probabilities between groups.

$$
\text{RD}_k = \hat{p}_{e k} - \hat{p}_{c k}
$$

* Directly interpretable as the percentage point difference in risk.
* More sensitive to **baseline risk**; can vary substantially across populations.
* **Less stable** across studies than OR or RR, which are **relative** measures.

---

**4. Arcsine Difference (ASD)**

The **arcsine difference** is used less frequently but is especially useful for detecting **small-study effects** (e.g., publication bias) in meta-analysis.

$$
\text{ASD}_k = \arcsin\left(\sqrt{\hat{p}_{e k}}\right) - \arcsin\left(\sqrt{\hat{p}_{c k}}\right)
$$

* Stabilizes variance, especially in studies with small sample sizes or rare events.
* Used in certain **regression-based tests** for funnel plot asymmetry or bias detection.

---

**Why Are OR and RR Usually Preferred?**

The **odds ratio** and **risk ratio** are preferred in most meta-analyses for several reasons:

* They are **relative measures**, which tend to remain more **consistent** across studies with different baseline risks (a concept known as **effect measure homogeneity**).
* **Log(OR)** and **log(RR)** are approximately normally distributed, enabling the use of standard meta-analytic methods (e.g., inverse-variance weighting).
* They are less affected by the absolute event rate compared to risk difference, making them more **robust** in diverse clinical settings.

 

## Analysis with Risk-Ratio [Fixed Effect - Inverse Variance Method]

The risk ratio (RR), also known as relative risk, is a widely used measure of treatment effect in clinical trials with binary outcomes (e.g., event/no event). In a meta-analysis, RR quantifies how the probability of an event (e.g., death, MI) differs between a treatment group and a control group across multiple studies.
  
 

The risk ratio is defined as the ratio of the event probability (risk) in the experimental group to the event probability in the control group:

$$
\text{RR} = \frac{p_E}{p_C} = \frac{x_E / n_E}{x_C / n_C}
$$

* $p_E$: Risk (probability of event) in the **experimental** group
* $p_C$: Risk (probability of event) in the **control** group
* $x_E$: Number of events in the experimental group
* $n_E$: Total number of patients in the experimental group
* $x_C$: Number of events in the control group
* $n_C$: Total number of patients in the control group

Interpretation:

* RR = 1: No difference between the groups
* RR < 1: Lower risk in the experimental group (treatment is beneficial)
* RR > 1: Higher risk in the experimental group

For example, in a statin study, RR < 1 would indicate that high-dose statins reduce risk of death or MI compared to standard-dose statins.

---

**Confidence Interval for RR using Log Transformation**

Since the distribution of the RR is skewed, it's common to perform inference on its **logarithmic scale** using:

$$
\ln(\text{RR}) = \ln(p_E) - \ln(p_C)
$$

The variance of $\ln(\text{RR})$ is approximated by the delta method as:

$$
\text{Var}(\ln(\text{RR})) = \frac{1}{x_E} - \frac{1}{n_E} + \frac{1}{x_C} - \frac{1}{n_C}
$$

The **standard error** (SE) is the square root of the variance. A **95% confidence interval** on the log scale is:

$$
\ln(\text{RR}) \pm 1.96 \times SE
$$

Exponentiating the lower and upper limits gives the 95% CI for RR:

$$
RR = \exp(\ln(\text{RR})) \\
LRR = \exp(\ln(\text{RR}) - 1.96 \times SE) \\
URR = \exp(\ln(\text{RR}) + 1.96 \times SE)
$$

This CI helps determine statistical significance: if the interval **does not include 1**, the RR is statistically significant.

 
**Assessing Statistical Significance**

1. **Confidence Interval Method**
   Calculate the 95% CI using the log RR and its variance. Example results:

* Study 3 (TNT) 95% CI: \[0.697, 0.922] → significant
* Other studies’ CIs include 1 → not significant

2. **P-Value Method**
   Compute the Z-statistic:

$$
z = \frac{\ln(\text{RR})}{SE}
$$

Then compute p-value:

$$
p = 2(1 - \Phi(|z|))
$$

Example:

* TNT study p-value = 0.00166 → significant
* Others > 0.05 → not significant

3. **Post-Hoc Power Analysis**
   Using the `pwr` package in R, power calculations showed:

* TNT study: power ≈ 88.5% (high)
* Others: power ≈ 36–44% (low)

Low power explains why some studies didn’t show significance despite lower RR values.

---

**Risk Ratio Meta-Analysis Step-by-Step (Fixed-Effects Model)** Before considering a random-effects model, check for **between-study heterogeneity**.

1. Compute the Q statistic:

$$
Q = \sum w_i (\ln(\text{RR}_i))^2 - \frac{(\sum w_i \ln(\text{RR}_i))^2}{\sum w_i}
$$

2. Compare Q to a chi-square distribution with $K - 1$ degrees of freedom.

**Automated Meta-Analysis in R using `meta` package**

```
metabin(Ee, Ne, Ec, Nc, sm="RR", method="I",
        data=data7, subset=study=="Milpied")
```

## Analysis with Risk Difference [Fixed Effect - Inverse Variance Method]

While the **risk ratio (RR)** is widely used for summarizing binary outcome data in meta-analysis, the **risk difference (RD)** is also an important effect measure due to its **simplicity and intuitive interpretation**. It tells us the **absolute difference in event rates** between the treatment and control groups.

**Definition of Risk Difference (RD)**

The **risk difference** is calculated as the difference between the observed risks in the experimental and control groups:

$$
\text{RD} = \hat{p}_E - \hat{p}_C = \frac{x_E}{n_E} - \frac{x_C}{n_C}
$$

Where:

* $\hat{p}_E$: estimated risk (probability of event) in the **experimental** group
* $\hat{p}_C$: estimated risk in the **control** group
* $x_E$, $x_C$: number of events in the experimental and control groups
* $n_E$, $n_C$: total number of patients in each group

**Interpretation**:

* RD = 0: No difference between groups
* RD < 0: Lower risk in the experimental group → treatment **beneficial**
* RD > 0: Lower risk in the control group → treatment **harmful**

This measure is especially appealing to clinicians and policy-makers because it reflects the **actual difference in event rates**, which can be used to estimate metrics such as **number needed to treat (NNT = 1 / |RD|)**.

---

**Variance and Standard Error of RD**

To construct confidence intervals and conduct hypothesis tests, we need to estimate the **variance** of the risk difference. This is derived from the binomial distribution under the assumption that each group’s outcomes are independent:

$$
\text{Var(RD)} = \frac{\hat{p}_E(1 - \hat{p}_E)}{n_E} + \frac{\hat{p}_C(1 - \hat{p}_C)}{n_C}
$$

Then the **standard error** is:

$$
\text{SE(RD)} = \sqrt{\text{Var(RD)}}
$$

---

**Example: RD Calculation in Statin Trials**

Using the statin trial data, suppose we have:


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
# Estimated risks
pE = c(0.0700, 0.0905, 0.0669, 0.0926)
pC = c(0.0834, 0.1053, 0.0835, 0.1041)

# Risk differences
RD = pE - pC
# Result:
# [1] -0.0133 -0.0148 -0.0166 -0.0115
```

Now calculate the **variance and standard error** for each RD:

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
# Sample sizes (example values)
nE = c(2099, 2265, 4995, 4439)
nC = c(2063, 2232, 5006, 4449)

# Variance of RD
VarRD = pE*(1 - pE)/nE + pC*(1 - pC)/nC
# Standard Error
SERD = sqrt(VarRD)
SERD
```


This allows us to construct **95% confidence intervals** for each RD:

$$
\text{RD} \pm 1.96 \times \text{SE}
$$



---

**Meta-Analysis of Risk Differences (Fixed-Effects Model)**

To combine the RD values from all studies and improve precision, a meta-analysis is performed using a **fixed-effects model**. In this model, each study is **weighted by the inverse of the variance** of its RD:

$$
w_i = \frac{1}{\text{Var(RD)}_i}
$$

The combined (pooled) RD is then calculated as a **weighted average**:

$$
\text{RD}_{\text{pooled}} = \frac{\sum w_i \cdot \text{RD}_i}{\sum w_i}
$$

And the variance of the pooled RD:

$$
\text{Var}(\text{RD}_{\text{pooled}}) = \frac{1}{\sum w_i}
$$

The standard error and 95% confidence interval follow directly from this.

---

**Performing Meta-Analysis Using the R Package `meta`**

The `meta` package allows for straightforward implementation of RD meta-analysis with the function `metabin()`:


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
# Create the statin study data
dat <- data.frame(
  Study = c("Prove-It", "A-to-Z", "TNT", "IDEAL"),
  evhigh = c(147, 205, 334, 411),   # number of events in high-dose group
  nhigh  = c(2099, 2265, 4995, 4439),  # total in high-dose group
  evstd  = c(172, 235, 418, 463),   # number of events in standard-dose group
  nstd   = c(2063, 2232, 5006, 4449)   # total in standard-dose group
)
RD.Statin = metabin(evhigh, nhigh, evstd, nstd,
                    studlab = Study,
                    data = dat,
                    method = "Inverse",
                    sm = "RD")
summary(RD.Statin)
plot(RD.Statin)
```


```
metabin(Ee, Ne, Ec, Nc, sm="RD", method="I",
        data=data7, subset=study=="Milpied")
```



## Meta-Analysis with Odds Ratio 

### Fixed Effect - Inverse Variance Method

The odds ratio for study $k, \psi_{k}$, is defined as the ratio of the odds of an event in the experimental arm
to that in the control arm. That is
$$
\psi_{k}=\frac{\left(\frac{p_{e k}}{1-p_{e k}}\right)}{\left(\frac{p_{c k}}{1-p_{c k}}\right)}=\frac{p_{e k}\left(1-p_{c k}\right)}{p_{c k}\left(1-p_{e k}\right)}
$$
- If either of the two estimated event probabilities is zero the log odds ratio, $\log \psi_{k}$, is either
$-\infty$ or $+\infty$
- If both are zero, the log odds ratio is undefined.
The odds ratio from study $k$ is estimated by
$$
\hat{\psi}_{k}=\frac{a_{k} d_{k}}{b_{k} c_{k}}
$$
variance of the natural logarithm of the odds ratio is well approximated by
$$
\widehat{\operatorname{Var}}\left(\log \hat{\psi}_{k}\right)=\frac{1}{a_{k}}+\frac{1}{b_{k}}+\frac{1}{c_{k}}+\frac{1}{d_{k}}
$$
where the approximation improves as nek and $n_{c k}$ increase. Using the estimate of the log odds
ratio, and its estimated variance, an approximate two-sided $(1-\alpha)$ confidence interval for the
odds ratio is given by

$$\exp \left(\log \hat{\psi}_{k} \pm z_{1-\frac{\alpha}{2}} \text { S.E. }\left(\log \hat{\psi}_{k}\right)\right)$$

* The `metabin` function can smoothly realize the combination of effect sizes

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
data(Fleiss93)
# Can taking aspirin after a heart attack reduce mortality?
# 7 studies information
# event.e indicates the number of deaths in the treatment group
# n.e indicates the total number of people in the treatment group
# event.c indicates the total number of people in the control group
# n.c indicates the total number of people in the control group

metabin(event.e, n.e, event.c, n.c, data=Fleiss93, sm="OR")

# Meta-analysis forest plot is generally indispensable
# The OR value and 95% confidence interval, weight, combined effect size value and heterogeneity test results of each study are all in this figure
# comb.random=FALSE, the random effect model is not used for effect size combination, but the fixed effect model is used
metaresult<-metabin(event.e, n.e,event.c,n.c,data=Fleiss93,sm="OR",
                    studlab=paste(study, year),random=FALSE)
plot(metaresult)
```


**Unfortunately, for sparse data, the pooled estimates based on the inverse variance method are biased**


### Meta-Analysis using Mantel-Haenszel Method (Fixed Effect)

**Detailed Explanation of the Mantel-Haenszel (MH) Method in Meta-Analysis**

The **Mantel-Haenszel method** is a classical and widely used approach in meta-analysis for combining binary outcome data (e.g., event vs. non-event) across multiple studies. It estimates a **pooled odds ratio** under the assumption of a **fixed effect**, meaning that the true treatment effect (odds ratio) is assumed to be the same in all studies, and differences arise purely from sampling variability.

---

**Data Structure and Odds Ratio Definition**

For each study, data are typically organized in a 2-by-2 contingency table:

| Group        | Events $(A, C)$ | Non-Events $(B, D)$ | Total $(n_1, n_2)$ |
| ------------ | --------------- | ------------------- | ------------------ |
| Experimental | $A$             | $B$                 | $A + B$            |
| Control      | $C$             | $D$                 | $C + D$            |

The **odds** of the event in each group is calculated as:

* Experimental: $\text{Odds}_E = A / B$
* Control: $\text{Odds}_C = C / D$

The **odds ratio (OR)** for each study is:

$$
\text{OR}_i = \frac{A_i \cdot D_i}{B_i \cdot C_i}
$$

---

**Mantel-Haenszel Pooled Odds Ratio**

The **Mantel-Haenszel pooled OR** is calculated as:

$$
\text{OR}_{MH} = \frac{\sum_{i=1}^K \frac{A_i D_i}{N_i}}{\sum_{i=1}^K \frac{B_i C_i}{N_i}}
$$

where:

* $N_i = A_i + B_i + C_i + D_i$, the total sample size in the ith study.
* This estimator is **more robust** than inverse variance methods when individual studies have **small cell counts**.

It can be viewed as a **weighted average** of individual study odds ratios, where each weight is:

$$
w_i = \frac{B_i C_i}{N_i}
$$

This leads to a relative weight for each study:

$$
\text{relWeight}_i = \frac{w_i}{\sum w_i}
$$

and the pooled estimate becomes:

$$
\text{OR}_{MH} = \sum \text{relWeight}_i \cdot \text{OR}_i
$$

---

**Naïve Variance and Confidence Interval (Not Recommended)**

A simple variance estimate for $\text{OR}_{MH}$ is the inverse of the total weight:

$$
\text{Var}(\text{OR}_{MH}) = \frac{1}{\sum w_i}
$$

This is used to construct an approximate 95% confidence interval:

$$
\text{CI} = \text{OR}_{MH} \pm 1.96 \times \sqrt{\text{Var}}
$$

However, this method is not ideal because the **distribution of the odds ratio is skewed**, especially for small probabilities, and the variance of the OR itself does not follow a normal distribution.

---

**Improved Confidence Interval Using Emerson’s Approximation**

To improve accuracy, it is common to perform inference on the **log scale** and construct a confidence interval for the **log odds ratio**, then exponentiate the limits. Emerson’s method uses an advanced formula proposed by Robins et al. (1986) to estimate the **variance of the log odds ratio** more precisely.

The steps are:

1. For each study, define:

   * $T1_i = \frac{A_i + D_i}{N_i}$
   * $T2_i = \frac{B_i + C_i}{N_i}$
   * $T3_i = \frac{A_i D_i}{N_i}$
   * $T4_i = \frac{B_i C_i}{N_i}$

2. Sum across all studies:

   * $ST3 = \sum T3_i$
   * $ST4 = \sum T4_i$

3. Compute the **variance of the log odds ratio**:

$$
\text{Var}(\log(\text{OR}_{MH})) = \frac{1}{2} \sum \left[
\frac{T1_i T3_i}{ST3^2} +
\frac{T1_i T4_i + T2_i T3_i}{ST3 \cdot ST4} +
\frac{T2_i T4_i}{ST4^2}
\right]
$$

4. The confidence interval is then:

* Lower bound on log scale:

  $$
  \log(\text{OR}_{MH}) - 1.96 \cdot \sqrt{\text{Var}}
  $$

* Upper bound on log scale:

  $$
  \log(\text{OR}_{MH}) + 1.96 \cdot \sqrt{\text{Var}}
  $$

* Exponentiate both bounds to return to the original scale.

---
 


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
# Create the statin study dataset
dat <- data.frame(
  Study  = c("Prove-It", "A-to-Z", "TNT", "IDEAL"),
  evhigh = c(147, 205, 334, 411),    # events in high-dose group
  nhigh  = c(2099, 2265, 4995, 4439),# total in high-dose group
  evstd  = c(172, 235, 418, 463),    # events in standard-dose group
  nstd   = c(2063, 2232, 5006, 4449) # total in standard-dose group
)

# Odds ratios from individual studies
OR <- c(0.828, 0.846, 0.787, 0.878)

# Step 1: compute raw Mantel-Haenszel weights
w0 <- (dat$nhigh - dat$evhigh) * dat$evstd / (dat$nhigh + dat$nstd)

# Step 2: total weight and relative weights
TotWeight <- sum(w0)
relWt <- w0 / TotWeight

# Step 3: compute pooled MH OR
OR.MH <- sum(relWt * OR)

# Step 4: naive variance
Var.ORMH1 <- 1 / TotWeight
lowCI_naive <- OR.MH - 1.96 * sqrt(Var.ORMH1)
upCI_naive <- OR.MH + 1.96 * sqrt(Var.ORMH1)

print(paste0("MH estimate = ",round(OR.MH,3), 
             ", 95% CI (naive) = [",
             round(lowCI_naive,3),",",
             round(upCI_naive,3),"]"))
```

**R Implementation: Emerson's Improved Method**


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
A <- dat$evhigh
B <- dat$nhigh - dat$evhigh
C <- dat$evstd
D <- dat$nstd - dat$evstd
N <- A + B + C + D

T1 <- (A + D) / N
T2 <- (B + C) / N
T3 <- A * D / N
T4 <- B * C / N

ST3 <- sum(T3)
ST4 <- sum(T4)

Var.lnOR <- 0.5 * (
  (T1 * T3) / ST3^2 +
  (T1 * T4 + T2 * T3) / (ST3 * ST4) +
  (T2 * T4) / ST4^2
)

Var.lnMH <- sum(Var.lnOR)

# 95% CI on log scale
lowCI_lnMH <- log(OR.MH) - 1.96 * sqrt(Var.lnMH)
upCI_lnMH <- log(OR.MH) + 1.96 * sqrt(Var.lnMH)

# Convert to OR scale
lowCI_MH <- exp(lowCI_lnMH)
upCI_MH <- exp(upCI_lnMH)

print(paste0("MH estimate = ",round(OR.MH,3), 
             ", 95% CI (Emerson) = [",
             round(lowCI_MH,3),",",
             round(upCI_MH,3),"]"))
```

This confidence interval matches the output from the `meta` package.


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
ORMH.Statin <- metabin(
  event.e = evhigh, n.e = nhigh,
  event.c = evstd,  n.c = nstd,
  studlab = Study,
  data = dat,
  method = "MH",   # Mantel-Haenszel
  sm = "OR"
)

summary(ORMH.Statin)
plot(ORMH.Statin)
```

The **Mantel-Haenszel method** is a reliable and efficient approach to pooling odds ratios across studies, especially when:

* The event is rare
* Some cell counts are small
* The number of studies is moderate to large


### Estimation in Sparse Data - Continuity correction

 
**Sparse data** refers to cases where the number of events is very low—sometimes **zero**—in one or more groups of a 2×2 contingency table:

|              | Event (Yes) | Event (No) | Total                |
| ------------ | ----------- | ---------- | -------------------- |
| Experimental | $a_k$       | $b_k$      | $n_{ek} = a_k + b_k$ |
| Control      | $c_k$       | $d_k$      | $n_{ck} = c_k + d_k$ |

If **either $a_k = 0$ or $c_k = 0$** (i.e. no events in the treatment or control group), then:

* The **odds ratio (OR)** becomes either 0 or ∞ (undefined on the log scale).
* The **risk ratio (RR)** becomes either 0 or 1, but the **variance estimate becomes unstable or unreliable**.

This results in high **statistical uncertainty** and **biased estimates**, especially when you try to combine such studies in a meta-analysis.

---

To address this problem, researchers typically consider two approaches:

**1. Exclude the study**:
Omitting studies with zero events may avoid the distortion of summary statistics, but it **discards potentially valuable data** and can introduce **selection bias**.

**2. Apply continuity correction**:
This involves **adding a small constant** to each cell of the 2×2 table to avoid zeros and make OR and RR computable. This is the more commonly used method.

The **standard correction** is to add **0.5** to every cell of the table:

$$
\hat{\psi}_{k}^{\text{mod}} = \frac{(a_k + 0.5)(d_k + 0.5)}{(b_k + 0.5)(c_k + 0.5)}
$$

This adjusted odds ratio becomes computable even if some original counts were zero.

The variance of the **log-transformed adjusted odds ratio** is:

$$
\widehat{\operatorname{Var}}(\log \hat{\psi}_{k}^{\text{mod}}) = \frac{1}{a_k + 0.5} + \frac{1}{b_k + 0.5} + \frac{1}{c_k + 0.5} + \frac{1}{d_k + 0.5}
$$

This method is easy to implement and prevents division by zero. However, it is **sensitive to the choice of increment**, especially in small studies.

* Using 0.5 may **artificially shrink** or **inflate** estimates.
* To assess this, one can try other small constants like 0.01 or 0.1 to check **sensitivity**.

---

Sweeting et al. (2004) proposed a **more refined continuity correction** that adjusts for **imbalance in sample sizes** between treatment groups.

Define:

* $n_e$: sample size in the **experimental** group
* $n_c$: sample size in the **control** group

Then the continuity corrections for each arm are:

$$
\text{incr}_e = \frac{n_e}{n_e + n_c}, \quad \text{incr}_c = \frac{n_c}{n_e + n_c}
$$

**Key points:**

* The sum of $\text{incr}_e + \text{incr}_c = 1$
* If sample sizes are equal, both are 0.5 (same as the standard method)
* If **sample sizes are unbalanced**, the group with the smaller size gets a **smaller correction**, leading to **less bias**

This approach helps produce more accurate estimates of ORs in **asymmetric studies** where one group is much larger than the other.

---

**Summary of Continuity Correction Techniques**

| Method                       | Increment Type       | When Used                        | Advantage                            |
| ---------------------------- | -------------------- | -------------------------------- | ------------------------------------ |
| **Standard (0.5 rule)**      | 0.5 to each cell     | Common, default in many packages | Simple, avoids zero counts           |
| **Smaller fixed increments** | e.g. 0.01, 0.1       | Sensitivity analysis             | Tests robustness of results          |
| **Sweeting’s method**        | Based on group sizes | Unbalanced study arms            | Reduces bias in odds ratio estimates |

---

**Practical Recommendation**

When working with sparse data in meta-analysis:

1. **Do not blindly discard studies** with zero events.
2. Use a **continuity correction**, especially if using OR or RR.
3. Prefer **Sweeting’s method** when arms are highly unbalanced.
4. Always perform a **sensitivity analysis** using different corrections to ensure robustness of conclusions.

These approaches are implemented in most statistical software packages (e.g., R `meta`, `metafor`) via options for continuity correction (e.g., `add=0.5` or `add="tacc"` for treatment-arm continuity correction).


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
# With 0.5 continuity correction for sparse data
metabin(Ee, Ne, Ec, Nc, sm="OR", method="I",
        data=data8, subset=study=="Australian")

# With 0.1 continuity correction for sparse data
metabin(Ee, Ne, Ec, Nc, sm="OR", method="I",
        data=data8, subset=study=="Australian",
        incr=0.1)

# conduct an analysis based on the treatment arm continuity correction
# using argument incr="TACC"
metabin(Ee, Ne, Ec, Nc, sm="OR", method="I",
        data=data8, subset=study=="Australian",
        incr="TACC")        
```

**Calculating the risk ratio with sparse data, (Pettigrew et al)**

$$\hat{\phi}_{k}^{\mathrm{mod}}=\frac{a_{k}+0.5}{a_{k}+b_{k}+0.5} / \frac{c_{k}+0.5}{c_{k}+d_{k}+0.5}$$
$$\widehat{\operatorname{Var}}\left(\log \hat{\phi}_{k}^{\bmod }\right)=\frac{1}{a_{k}+0.5}+\frac{1}{c_{k}+0.5}-\frac{1}{a_{k}+b_{k}+0.5}-\frac{1}{c_{k}+d_{k}+0.5} \cdot$$

### Peto Odds Ratio (Fixed Effect)

The **Peto method** is another fixed-effect approach to meta-analyzing binary outcomes, particularly when effect sizes are small and events are relatively rare. This method provides an approximation of the **pooled odds ratio (OR)** without requiring the individual odds ratios from each study to be calculated directly. It's especially suited for **unbalanced designs** (i.e., differing group sizes) or **sparse data**, although it can become biased if the treatment and control group sizes are very unequal or if the treatment effect is large.


---

**1. Peto's Odds Ratio: Concept**

The **Peto odds ratio** for each study is based on the **difference between observed and expected event counts** under the null hypothesis (no effect), and uses this difference to construct the log odds ratio.

For study $i$, define:

* $O_i$: Observed number of events in the **experimental** group (i.e., $A_i$)
* $E_i$: Expected number of events in the experimental group under the null:

  $$
  E_i = \frac{(A_i + B_i)(A_i + C_i)}{N_i}
  $$
* $V_i$: Variance of $O_i - E_i$:

  $$
  V_i = \frac{(A_i + B_i)(C_i + D_i)(A_i + C_i)(B_i + D_i)}{N_i^2(N_i - 1)}
  $$

Using these, the **Peto odds ratio** for each study is:

$$
\hat{\Psi}_i = \exp\left(\frac{O_i - E_i}{V_i}\right)
$$

And the **95% confidence interval** for this estimate is:

$$
\text{CI}_i = \exp\left(\frac{O_i - E_i \pm 1.96 \cdot \sqrt{V_i}}{V_i}\right)
$$

---

**2. Pooled Peto’s Odds Ratio Across Studies**

The **pooled odds ratio** is calculated as:

$$
\hat{\Psi}_{\text{pooled}} = \exp\left(\frac{\sum_{i=1}^K (O_i - E_i)}{\sum_{i=1}^K V_i}\right)
$$

And its **95% confidence interval** is:

$$
\exp\left( \frac{\sum (O_i - E_i) \pm 1.96 \cdot \sqrt{\sum V_i}}{\sum V_i} \right)
$$

This method avoids direct computation of individual study ORs and variances, focusing instead on the **expected vs. observed** framework.

---


**3. Manual Calculation in R (Step-by-Step)**

 

```{r ,echo = T,message = FALSE, error = FALSE, warning = FALSE}
dat <- data.frame(
  Study  = c("Prove-It", "A-to-Z", "TNT", "IDEAL"),
  evhigh = c(147, 205, 334, 411),
  nhigh  = c(2099, 2265, 4995, 4439),
  evstd  = c(172, 235, 418, 463),
  nstd   = c(2063, 2232, 5006, 4449)
)

## First, define the 2×2 table components:

A <- dat$evhigh
B <- dat$nhigh - A
C <- dat$evstd
D <- dat$nstd - C
N <- A + B + C + D
 
# Now compute the Peto components for each study:
# Observed
Oi <- A

# Expected under null
Ei <- (A + B) * (A + C) / N

# Variance
Vi <- (A + B) * (C + D) * (A + C) * (B + D) / (N^2 * (N - 1))

# Peto’s OR
psii <- exp((Oi - Ei) / Vi)
print(psii)


## Compute confidence intervals for each study:
lowCIi <- exp((Oi - Ei - 1.96 * sqrt(Vi)) / Vi)
upCIi  <- exp((Oi - Ei + 1.96 * sqrt(Vi)) / Vi)

print(lowCIi)
print(upCIi)

print("Interpretation: Only the third study may have a CI that excludes 1 → statistically significant")


## Now compute the **pooled Peto’s OR**:
psi <- exp(sum(Oi - Ei) / sum(Vi))
lowCI <- exp((sum(Oi - Ei) - 1.96 * sqrt(sum(Vi))) / sum(Vi))
upCI  <- exp((sum(Oi - Ei) + 1.96 * sqrt(sum(Vi))) / sum(Vi))

cat("Pooled Peto OR =", round(psi, 3), "\n")
cat("95% CI = [", round(lowCI, 3), ",", round(upCI, 3), "]\n")
```

 
---

**4. Using the `meta` R Package for Peto’s Method**

The `meta` package automates this calculation. Here’s how to perform Peto’s OR meta-analysis using `metabin()`:


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}

ORPeto.Statin <- metabin(
  event.e = evhigh, n.e = nhigh,
  event.c = evstd,  n.c = nstd,
  studlab = Study,
  data = dat,
  method = "Peto",  # use Peto's method
  sm = "OR"
)

summary(ORPeto.Statin)
plot(ORPeto.Statin)
```
 

**When to Use Peto’s Method**

Peto’s method is most appropriate when:

* Treatment effects are small (i.e., odds ratios close to 1)
* Event rates are low (rare outcomes)
* Treatment and control group sizes are approximately balanced

However, **Peto’s method can be biased** if:

* There are large treatment effects
* Treatment and control groups are imbalanced
* There are studies with zero events in one arm

For general applications with more robust statistical properties, the **Mantel-Haenszel method** or **inverse-variance method** is typically preferred.


 
 
## Random Effect Model

In real-world meta-analysis of binary outcomes (e.g., treatment success/failure, survival/death), the differences between study results often cannot be explained by random sampling error alone. These differences may come from:

* Varying populations across studies
* Differences in study design or quality
* Variations in intervention delivery or control conditions

The **random effects model** explicitly models this extra variability between studies.

---

**Model Specification**



Suppose each study $k$ provides an estimate $\hat{\theta}_k$, such as the **log odds ratio** or **log risk ratio**. The random effects model assumes:

$$
\hat{\theta}_{k} = \theta + u_k + \sigma_k \epsilon_k
$$

Where:

* $\hat{\theta}_k$: Observed treatment effect (e.g., log OR) in study $k$
* $\theta$: The **overall average effect size** across all studies (the pooled estimate)
* $u_k \sim N(0, \tau^2)$: Random **between-study deviation**, capturing heterogeneity
* $\sigma_k \epsilon_k \sim N(0, \sigma_k^2)$: Within-study error due to sampling variability
* $\tau^2$: The **between-study variance**, a key quantity that reflects the extent of heterogeneity
* $\epsilon_k$: Standard normal error, i.i.d. $N(0,1)$

In words, this model says that each study’s effect estimate deviates from the overall effect due to:

* **Between-study variability** ($u_k$) → heterogeneity
* **Within-study sampling error** ($\sigma_k \epsilon_k$)

Both sources of error are assumed to be independent.

---

**Key Assumption: Distribution of Effects**

Instead of assuming all studies estimate the **same true effect** ($\theta$), we assume that the **true effect for each study** is **drawn from a distribution** centered at $\theta$, with variability $\tau^2$:

$$
\theta_k^{\text{true}} = \theta + u_k, \quad \text{with } u_k \sim N(0, \tau^2)
$$

So, the treatment effect itself is **random across studies**, and the observed effect $\hat{\theta}_k$ is a noisy version of that random effect.

Given the above, the **total variance** of the observed effect in each study is:

$$
\operatorname{Var}(\hat{\theta}_k) = \sigma_k^2 + \tau^2
$$

Where:

* $\sigma_k^2$: **Within-study variance**, often derived from 2×2 tables (as in log OR variance)
* $\tau^2$: **Between-study variance**, to be estimated

---



**Estimating Between-Study Variance: DerSimonian–Laird Method**

The **DerSimonian–Laird (DL) method** is a widely used approach for estimating $\tau^2$.

**Step 1: Compute Q Statistic (Cochran’s Q)**

$$
Q = \sum_{k=1}^{K} w_k (\hat{\theta}_k - \hat{\theta}_F)^2
$$

* $\hat{\theta}_F$: Fixed-effect estimate (weighted average of study estimates)
* $w_k = \frac{1}{\widehat{\operatorname{Var}}(\hat{\theta}_k)}$: Inverse variance weights

**Step 2: Compute the DL estimator of $\tau^2$:**

$$
\hat{\tau}^2 = \frac{Q - (K - 1)}{\sum_{k=1}^{K} w_k - \frac{\sum_{k=1}^{K} w_k^2}{\sum_{k=1}^{K} w_k}}
$$

* If $Q < K - 1$, then $\hat{\tau}^2$ is **set to 0** (i.e., no detected heterogeneity)

**Interpretation:**

* $Q > K - 1$ → significant heterogeneity → $\tau^2 > 0$
* $Q \approx K - 1$ → little/no heterogeneity → $\tau^2 \approx 0$

---

**Pooled Effect and Weighting**

Once $\tau^2$ is estimated, the weights used for the random effects pooled estimate become:

$$
w_k^{\text{RE}} = \frac{1}{\sigma_k^2 + \hat{\tau}^2}
$$

Then the random effects pooled estimate is:

$$
\hat{\theta}_{\text{RE}} = \frac{\sum w_k^{\text{RE}} \hat{\theta}_k}{\sum w_k^{\text{RE}}}
$$

With variance:

$$
\widehat{\operatorname{Var}}(\hat{\theta}_{\text{RE}}) = \frac{1}{\sum w_k^{\text{RE}}}
$$

---

| Concept                | Random Effects Model                                        |
| ---------------------- | ----------------------------------------------------------- |
| Treatment Effect       | Varies across studies: $\theta_k = \theta + u_k$            |
| Between-Study Variance | $\tau^2$ (heterogeneity across true effects)                |
| Within-Study Variance  | $\sigma_k^2$ (sampling error)                               |
| Pooled Estimate        | Weighted average using total variance $\sigma_k^2 + \tau^2$ |
| Weight Formula         | $w_k^{RE} = \frac{1}{\sigma_k^2 + \tau^2}$                  |
| Estimator of $\tau^2$  | DerSimonian–Laird (DL)                                      |
| Best for               | Heterogeneous studies, moderate to large number of studies  |

# Meta-Analysis for Rare Events





# Reference (APA Style)

## Book

Harrer, M., Cuijpers, P., Furukawa, T.A., & Ebert, D.D. (2021). _Doing Meta-Analysis with R: A Hands-On Guide_. Boca Raton, FL and London: Chapmann & Hall/CRC Press. ISBN 978-0-367-61007-4.
https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/

Chen, D.-G. (Din), & Peace, K. E. (2021). Applied Meta-Analysis with R and Stata (2nd ed.). Chapman & Hall/CRC Press. https://doi.org/10.1201/9780429061240

Schwarzer, G., Carpenter, J. R., & Rücker, G. (2015). Meta-Analysis with R. Springer. https://doi.org/10.1007/978-3-319-21416-0

## Meta Analysis

Berkey, C. S., D. C. Hoaglin, A. Antezak-Bouckoms, F. Mosteller, and G. A.
Colditz (1998). Meta-analysis of multiple outcomes by regression with random effects. Statistics in Medicine 17, 2537–2550.

Berkey, C. S., D. C. Hoaglin, F. Mosteller, and G. A. Colditz (1995).
A random-effects regression model for meta-analysis. Statistics in
Medicine 14(4), 395–411.

Berman, N. G. and R. A. Parker (2002). Meta-analysis: Neither quick nor
easy. BMC Medical Research Methodology 2(10), 1–9.

Bhaumik, D. K., A. Amatya, S. T. Normand, J. Greenhouse, E. Kaizar,
B. Neelon, and R. D. Gibbons (2012). Meta-analysis of rare binary adverse event data. Journal of the American Statistical Association 107(498),
555–567.

Bohning, D., E. Dietz, P. Schlattmann, C. Viwatwonkasem, A. Biggeri, and
J. Bock (2002). Some general points in estimating heterogeneity variance
with the dersimonianUlaird estimator. ˝ Biostatistics 3, 445–457.

Borenstein, M., L. V. Hedges, J. P. T. Higgins, and H. R. Rothstein (2009).
Introduction to Meta-Analysis. West Sussex, United Kingdom: Wiley.

Cai, T., L. Parast, and L. Ryan (2010). Meta-analysis for rare events. Statistics
in Medicine 29(20), 2078–2089.

Chen, H., A. K. Manning, and J. Dupuis (2012). A method of moments
estimator for random effect multivariate meta-analysis. Biometrics, (Epub
May 2, 2012).

Cooper, H., L. V. Hedges, and J. C. Valentine (2009). The Handbook of
Research Synthesis and Meta-Analysis (2nd edition). New York: Russell
Sage Foundation.

DerSimonian, R. and N. Laird (1986). Meta-analysis in clinical trials. Controlled Clinical Trials 7, 177–188.

Hartung, J., G. Knapp, and B. K. Sinha (2008). Statistical Meta-Analysis
with Applications. Hoboken, New Jersey: John Wiley & Sons, Inc.


