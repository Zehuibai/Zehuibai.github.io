<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title> Meta Analysis</title>

<script src="site_libs/header-attrs-2.29/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<link href="site_libs/font-awesome-6.5.2/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Zehui Bai</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
    Home
  </a>
</li>
<li>
  <a href="02-Clinical_Experience.html">
    <span class="fa fa-user"></span>
     
    Clinical Experience
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-sliders"></span>
     
    Sample Size
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="03-SSC-Everything-to-Know-About-Sample-Size-Determination.html">Everything to Know About Sample Size Determination</a>
    </li>
    <li>
      <a href="03-SSC-Choosing-the-Effect-Size-for-Sample-Size-Calculations.html">Choosing the Effect Size</a>
    </li>
    <li>
      <a href="03-SSC-Biosimilar-Trials.html">Statistical Considerations for the Design and Analysis of Biosimilar Trials</a>
    </li>
    <li>
      <a href="03-SSC-Sample-Size-and-Power-for-Non-Parametric-Analysis.html">Sample Size and Power for Non-Parametric Analysis</a>
    </li>
    <li>
      <a href="03-SSC-Power-for-Complex-Hypotheses.html">Power for Complex Hypotheses</a>
    </li>
    <li>
      <a href="03-SSC-Alternatives-to-Power.html">Bayesian methods - Alternatives to Power</a>
    </li>
    <li>
      <a href="03-SSC-Sample-Size-for-Pilot-Studies.html">Sample Size for Pilot Studies</a>
    </li>
    <li>
      <a href="03-SSC-Case-Continuous-Endpoint.html">Sample Size Determination for Continuous Endpoint</a>
    </li>
    <li>
      <a href="03-SSC-Case-Categorical-Endpoint.html">Sample Size Determination for Categorical Endpoint</a>
    </li>
    <li>
      <a href="03-SSC-Sample-Size-Determination-for-Counts-and-Rates.html">Sample Size Determination for Counts and Rates</a>
    </li>
    <li>
      <a href="03-SSC-Case-Survival-Endpoint.html">Sample Size Determination for Survival Endpoint</a>
    </li>
    <li>
      <a href="03-SSC-Case-Repeated-Measures.html">Sample Size Determination for Repeated Measures</a>
    </li>
    <li>
      <a href="03-SSC-IA-Sequential-Design.html">Statistical Considerations for Group Sequential Design</a>
    </li>
    <li>
      <a href="03-SSC-IA-Adaptive-Design.html">Statistical Considerations for Adaptive Design</a>
    </li>
    <li>
      <a href="03-SSC-Multiple-Test.html">Sample Size for Multiple Test</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-pencil-square-o"></span>
     
    Study Design
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="04-Design-Estimands-Framework.html">Estimands Framework</a>
    </li>
    <li>
      <a href="04-Design-Estimands-Practice.html">Estimands and Sensitivity Analyses</a>
    </li>
    <li>
      <a href="04-Design-Phase-I-Trials---Design-Considerations.html">Phase I Trials - Design Considerations</a>
    </li>
    <li>
      <a href="04-Design-Phase-II-Trials---Design-Considerations.html">Phase II Trials - Design Considerations</a>
    </li>
    <li>
      <a href="04-Design-Phase-III-Trials---Design-Considerations.html">Phase III Trials - Design Considerations</a>
    </li>
    <li>
      <a href="04-Design-Phase-IV-Trials---Design-Considerations.html">Phase IV Trials - Design Considerations for Post Marketing Surveillance</a>
    </li>
    <li>
      <a href="04-Design-Complex-Sequential-Trials.html">Complex Sequential Analysis Trials</a>
    </li>
    <li>
      <a href="04-Design-Adaptive-Clinical-Trials.html">Adaptive Clinical Trials</a>
    </li>
    <li>
      <a href="04-Design-Bayesian-Clinical-Trials.html">Bayesian Clinical Trials</a>
    </li>
    <li>
      <a href="04-Design-Dose-Escalation-and-Stratification-Designs.html">Dose Escalation and Stratification Designs in Early Oncology Development</a>
    </li>
    <li>
      <a href="04-Design-Single-Arm-Clinical-Trials.html">Single Arm Clinical Trials</a>
    </li>
    <li>
      <a href="04-Design-Diagnostic-Study-Design-and-Evaluation.html">Diagnostic Study-Design and Evaluation</a>
    </li>
    <li>
      <a href="04-Design-Diagnostic-Study-MRMC.html">Diagnostic Study-Multireader Multicase (MRMC)</a>
    </li>
    <li>
      <a href="04-Design-Vaccine-Design.html">Vaccine Trials</a>
    </li>
    <li>
      <a href="04-Design-Hierarchical-composite-endpoints.html">Hierarchical Composite Endpoints</a>
    </li>
    <li>
      <a href="04-Design-Externally-Controlled-Trials.html">Considerations for the Design and Conduct of Externally Controlled Trials</a>
    </li>
    <li>
      <a href="04-Design-Noninferiority-Trials.html">Noninferiority Trials</a>
    </li>
    <li>
      <a href="04-Design-Bioequivalence-and-Biosimilar-Trials.html">Bioequivalence and Biosimilar Trials</a>
    </li>
    <li>
      <a href="04-Design-Exploring-Survival-Analysis-Designs-for-Clinical-Trials.html">Exploring Survival Analysis Designs for Clinical Trials</a>
    </li>
    <li>
      <a href="04-Design-Projecting-How-Long-Your-Trial-Will-Take.html">Projecting How Long Your Trial Will Take</a>
    </li>
    <li>
      <a href="04-Design-Regulatory-Submission.html">Regulatory Submission from Stats Perspective</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-line-chart"></span>
     
    Data Visualization
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="05-Plot-Time-To-Event.html">Time to Event Data Visualization</a>
    </li>
    <li>
      <a href="05-Plot-PRO-Data.html">Patient Reported Outcome Data Visualization</a>
    </li>
    <li>
      <a href="05-Plot-SSC-and-Power-Calculation.html">Sample Size and Power Calculations Visualization</a>
    </li>
    <li>
      <a href="05-Plot-Correlation.html">Correlation Visualization</a>
    </li>
    <li>
      <a href="05-Plot-Model-Table.html">Clinical Data and Model visualization</a>
    </li>
    <li>
      <a href="05-Plot-ScatterPlot.html">Scatter and Line Plot</a>
    </li>
    <li>
      <a href="05-Plot-BarPlot.html">Bar Chart</a>
    </li>
    <li>
      <a href="05-Plot-PieChart.html">Pie Chart</a>
    </li>
    <li>
      <a href="05-Plot-BoxPlot.html">Box Plot</a>
    </li>
    <li>
      <a href="05-Plot-Histogram.html">Histogram</a>
    </li>
    <li>
      <a href="05-Plot-Forest-Plot.html">Forest Plot</a>
    </li>
    <li>
      <a href="05-Plot-Flow-Chart.html">Flow Chart</a>
    </li>
    <li>
      <a href="05-Plot-Some-Interesting.html">Some Interesting Plots</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-calculator"></span>
     
    Statistical Analysis
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="06-Analysis-Linear-Regression.html">Linear Regression</a>
    </li>
    <li>
      <a href="06-Analysis-Logistic-Regression.html">Logistic Regression</a>
    </li>
    <li>
      <a href="06-Analysis-Mixed-Model.html">Mixed Model</a>
    </li>
    <li>
      <a href="06-Analysis-MMRM.html">Mixed Model Repeated Measures</a>
    </li>
    <li>
      <a href="06-Analysis-GEE.html">Generalized Estimating Equation</a>
    </li>
    <li>
      <a href="06-Analysis-ANOVA.html">Analysis of Variance</a>
    </li>
    <li>
      <a href="06-Analysis-Survival-Analysis.html">Survival Analysis</a>
    </li>
    <li>
      <a href="06-Analysis-Survival-Competing-Risk.html">Survival Analysis - Competing Risk</a>
    </li>
    <li>
      <a href="06-Analysis-Missing-Data.html">Missing Data Analysis</a>
    </li>
    <li>
      <a href="06-Analysis-MI-Implementation.html">Multiple Imputation Analysis</a>
    </li>
    <li>
      <a href="06-Analysis-SSD.html">Safety Signal Detection and Evaluation</a>
    </li>
    <li>
      <a href="06-Analysis-Meta-Analysis.html">Meta Analysis</a>
    </li>
    <li>
      <a href="06-Analysis-PK-and-PD.html">PK and PD Analysis</a>
    </li>
    <li>
      <a href="06-Analysis-Time-Series-Analysis.html">Time Series Analysis</a>
    </li>
    <li>
      <a href="06-Analysis-SEM.html">Structural Equation Modeling</a>
    </li>
    <li>
      <a href="06-Analysis-Factor-Analysis.html">Factor Analysis</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-bar-chart"></span>
     
    Machine Learning
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="07-ML-Bayesian-Theory.html">Bayesian Theory</a>
    </li>
    <li>
      <a href="07-ML-Bayesian-Analysis.html">Bayesian Analysis</a>
    </li>
    <li>
      <a href="07-ML-Regularization-Penalized-Regression.html">Regularization Penalized Regression</a>
    </li>
    <li>
      <a href="07-ML-Loss-Regression.html">Loss Functions in Machine Learning</a>
    </li>
    <li>
      <a href="07-ML-PCA.html">Principal Component Analysis</a>
    </li>
    <li>
      <a href="07-ML-KNN.html">K-Nearest Neighbors</a>
    </li>
    <li>
      <a href="07-ML-SVM.html">Support Vector Machine</a>
    </li>
    <li>
      <a href="07-ML-Tree-Models.html">Tree Models</a>
    </li>
    <li>
      <a href="07-ML-LDA.html">Linear Discriminant Analysis</a>
    </li>
    <li>
      <a href="07-ML-Cluster-Analysis.html">Cluster Analysis</a>
    </li>
    <li>
      <a href="07-ML-Neural-Networks.html">Neural Network</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-stethoscope"></span>
     
    Clinical Analysis (ADaM)
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="08-Clinical-Disposition-Baseline.html">Disposition and Baseline</a>
    </li>
    <li>
      <a href="08-Clinical-Efficacy-Continuous.html">Efficacy Continuous Analysis</a>
    </li>
    <li>
      <a href="08-Clinical-Efficacy-Responder.html">Efficacy Responder Analysis</a>
    </li>
    <li>
      <a href="08-Clinical-Survival-Analysis.html">Survival Analysis</a>
    </li>
    <li>
      <a href="08-Clinical-Pharmacokinetics-Analysis.html">Pharmacokinetics Analysis</a>
    </li>
    <li>
      <a href="08-Clinical-Adverse-Event.html">Adverse Event Visualization</a>
    </li>
    <li>
      <a href="08-Clinical-Laboratory-Data.html">Laboratory Data Visualization</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="08-CV.html">
    <span class="fa fa-file-pdf-o"></span>
     
    CV
  </a>
</li>
<li>
  <a href="mailto:zehuibai@outlook.com">
    <span class="fa fa-envelope-o"></span>
     
    Contact me
  </a>
</li>
<li>
  <a href="https://github.com/Zehuibai">
    <span class="fa fa-github"></span>
     
    GitHub
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore"><p><img src="logo.png"
style="width:3in" /><br />
Meta Analysis</p></h1>

</div>


<div id="introduction" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>Meta-analysis is a statistical method used to systematically combine
and synthesize results from multiple independent studies that address
the same or closely related research question. Its primary purpose is to
increase the overall power and precision of the findings, resolve
uncertainty when individual studies disagree, and provide a more
comprehensive understanding of the evidence base.</p>
<p>Unlike traditional narrative reviews, which rely on qualitative
summaries and are often subject to author bias, meta-analysis follows a
structured and quantitative approach. It typically involves extracting
effect sizes or summary statistics from each study, assessing
consistency among studies, and calculating an overall pooled estimate
using appropriate statistical models.</p>
<p>Meta-analysis is especially valuable in fields such as medicine,
psychology, education, and public health, where multiple studies may
exist on a single intervention, treatment, or phenomenon. It allows
researchers to answer questions with greater statistical confidence than
any single study can provide, and it plays a critical role in
evidence-based decision-making, such as developing clinical guidelines
or informing public policy.</p>
<p>A well-conducted meta-analysis begins with a clearly defined research
question and a written protocol. It includes systematic literature
searches, predefined inclusion and exclusion criteria, careful data
extraction, assessment of study quality, evaluation of heterogeneity
among results, and application of suitable analytical techniques. When
properly designed, a meta-analysis offers a rigorous and transparent
synthesis of scientific evidence.</p>
<div id="what-are-meta-analyses" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> What Are
Meta-Analyses</h2>
<p><strong>Definition and Purpose</strong> Meta-analysis is a method
used to statistically combine and interpret the results of multiple
independent studies addressing the same research question. Coined by
Gene V. Glass in 1976 as an “analysis of analyses,” it shifts the unit
of analysis from individual participants or observations to entire
studies. The aim is to create a comprehensive, quantitative summary of
the available evidence in a specific research area.</p>
<p><strong>Types of Evidence Synthesis</strong> There are several
approaches to synthesizing results from multiple studies. These methods
vary in terms of structure, objectivity, and the type of conclusions
they allow.</p>
<p><strong>1. Narrative Reviews</strong></p>
<ul>
<li>Narrative reviews are qualitative summaries of a research
field.</li>
<li>They are typically written by experts based on their knowledge and
experience.</li>
<li>There are no formal rules for how studies are selected or how
evidence is interpreted.</li>
<li>Because of this flexibility, narrative reviews can be biased and
reflect the personal opinions of the author.</li>
<li>However, when done carefully, they can provide valuable overviews
and help identify key themes and questions in a field.</li>
</ul>
<p><strong>2. Systematic Reviews</strong></p>
<ul>
<li>Systematic reviews use predefined methods to locate, assess, and
synthesize all relevant studies on a specific topic.</li>
<li>The review process is transparent, reproducible, and designed to
minimize bias.</li>
<li>Inclusion and exclusion criteria are clearly specified
beforehand.</li>
<li>Study quality is assessed using objective criteria.</li>
<li>Results are summarized systematically, though not necessarily in a
quantitative form.</li>
</ul>
<p><strong>3. Meta-Analyses</strong></p>
<ul>
<li>Meta-analyses are typically conducted as part of a systematic review
but go further by statistically combining study results.</li>
<li>The process starts with a clearly defined research question and
selection criteria.</li>
<li>Only studies with quantitative data are included.</li>
<li>The outcome is a single numerical estimate that reflects the overall
effect size, prevalence, or correlation derived from the selected
studies.</li>
<li>Because of this quantitative focus, meta-analyses often require that
studies be relatively homogeneous in design, interventions, and
measurement methods.</li>
</ul>
<p><strong>Key Distinction</strong> While both systematic reviews and
meta-analyses involve structured, transparent methods, the hallmark of
meta-analysis is the quantitative integration of results. This allows
for more precise estimates and statistical evaluation of consistency
across studies.</p>
<p><strong>4. Individual Participant Data (IPD)
Meta-Analysis</strong></p>
<ul>
<li>IPD meta-analysis involves collecting raw, individual-level data
from each study, rather than relying on published summary
statistics.</li>
<li>The combined dataset allows for more flexible and detailed
analyses.</li>
<li>Advantages include the ability to apply consistent statistical
methods across studies, handle missing data uniformly, and explore
participant-level moderators (e.g., age, gender) that cannot be examined
using aggregated data.</li>
<li>Despite its advantages, IPD meta-analysis is still rare due to the
difficulty of obtaining individual-level data from all relevant
studies.</li>
<li>In practice, many IPD meta-analyses are limited by incomplete data
availability. For example, a review found that IPD could be obtained
from only about 64% of eligible studies, with most analyses using fewer
studies than initially intended.</li>
</ul>
</div>
<div id="meta-analysis-protocol" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Meta-Analysis
Protocol</h2>
<p>Writing a protocol is the <strong>first and most critical
step</strong> in conducting a meta-analysis. It is comparable to
designing a clinical trial: the protocol defines the research question,
outlines inclusion and exclusion criteria, and specifies how data will
be identified, abstracted, and synthesized.</p>
<p>Key benefits:</p>
<ul>
<li><strong>Minimizes bias</strong> in study selection and
analysis.</li>
<li><strong>Enhances scientific rigor</strong> and reproducibility.</li>
<li><strong>Clarifies scope and eligibility</strong> of studies for
inclusion.</li>
</ul>
<hr />
<p><strong>[1] Defining the Research Objective</strong></p>
<p>A clearly defined objective is crucial for guiding all subsequent
steps. For example:</p>
<blockquote>
<p><em>“To assess the overall evidence of the effectiveness of
calcium-channel blockers in treating mild-to-moderate
hypertension.”</em></p>
</blockquote>
<p>However, to operationalize this question, further specifications are
required, such as:</p>
<ul>
<li>Definition of “mild-to-moderate” hypertension (blood pressure
thresholds have evolved over time).</li>
<li>Outcome measures (e.g., change in diastolic BP).</li>
<li>Control group type (placebo or active comparator).</li>
<li>Study design (e.g., parallel, crossover), randomization methods, and
blinding.</li>
<li>Patient characteristics (e.g., age, gender, comorbidities).</li>
<li>Study duration.</li>
</ul>
<p>These specifications form the foundation for establishing
<strong>eligibility criteria</strong> for selecting studies.</p>
<hr />
<p><strong>[2] Criteria for Identifying Eligible Studies</strong></p>
<p>To ensure consistency, validity, and scientific rigor in a
meta-analysis, the following six criteria should be clearly defined in
the research protocol:</p>
<p><strong>1. Clarifying the Disease or Condition Under
Study</strong></p>
<ul>
<li>Definitions of disease or outcome must be consistent with
contemporary clinical standards.</li>
<li>Historical definitions may differ (e.g., older studies may define
hypertension differently).</li>
</ul>
<p>The specific medical condition or area of application must be
precisely defined. For instance, if the focus is on “mild-to-moderate
hypertension,” the protocol must clarify what blood pressure ranges
constitute this category. Clinical definitions may have changed over
time—what was once considered mild may now be classified as moderate or
even normal. Failing to standardize this definition could result in the
inclusion of heterogeneous populations that compromise the validity of
pooled estimates.</p>
<p><strong>2. Defining the Effectiveness Measure or Outcome</strong></p>
<ul>
<li>Define primary endpoints (e.g., change in diastolic blood
pressure).</li>
<li>Include methods of measurement (position, device).</li>
<li>Ensure that chosen outcomes are consistently reported across
studies.</li>
</ul>
<p>Meta-analyses require a common outcome measure to aggregate data
meaningfully. In hypertension trials, this might be the change in
diastolic blood pressure (DBP) from baseline. However, outcomes may be
reported differently across studies—mean change, percentage reaching
normal levels, or alternative metrics like mean arterial pressure. The
protocol must specify which outcomes are acceptable, how they are
measured (e.g., sitting vs. standing BP), and whether they can be
harmonized across studies.</p>
<p><strong>3. Specifying the Type of Control Group</strong></p>
<ul>
<li>Control types (placebo or active comparator) influence
interpretation of efficacy.</li>
<li>Meta-analysis should include studies with <strong>comparable control
groups</strong>.</li>
</ul>
<p>The type of comparator used in each study (e.g., placebo, active
control, usual care) has major implications for the interpretation of
efficacy. Placebo-controlled trials provide direct evidence, while
active-controlled trials offer relative comparisons. The meta-analysis
should either restrict inclusion to studies with the same type of
control group or account for differences analytically (e.g., through
subgroup or sensitivity analyses). The protocol should define acceptable
control types and explain how variations will be handled.</p>
<p><strong>4. Outlining Acceptable Study Designs and Quality
Characteristics</strong></p>
<ul>
<li>Design (parallel vs. crossover), randomization, stratification,
blinding, and bias control.</li>
<li>Design consistency helps maintain internal validity across
synthesized studies.</li>
</ul>
<p>Different experimental designs (parallel vs. crossover, randomized
vs. non-randomized) can introduce variability in treatment effects.
Additional factors such as blinding, allocation concealment, and
stratification should be specified. The protocol must outline which
study designs are acceptable and whether criteria like randomization
method or blinding status are mandatory for inclusion. High-quality
design features reduce bias and increase the reliability of synthesized
results.</p>
<p><strong>5. Characterizing the Patient Population</strong></p>
<ul>
<li>Include demographic variables, inpatient vs. outpatient status,
concurrent diseases, and concomitant medications.</li>
<li>Abstracting subgroup-level summary statistics helps evaluate
heterogeneity.</li>
</ul>
<p>Eligible studies must include similar populations in terms of
demographics and clinical characteristics. Age range, sex distribution,
race or ethnicity, inpatient vs. outpatient setting, presence of
comorbidities, and use of concomitant medications should be considered.
If these factors vary widely, stratified analyses or subgroup extraction
may be necessary. The protocol should state clearly which patient
characteristics are required and how heterogeneity will be managed.</p>
<p><strong>6. Determining the Acceptable Length of Follow-Up or
Treatment Duration</strong></p>
<ul>
<li>Treatment duration significantly affects clinical outcomes.</li>
<li>Must be accounted for in both selection and interpretation.</li>
</ul>
<p>The duration of each study must be taken into account, as it affects
the stability and magnitude of treatment effects. A 4-week intervention
may yield different results compared to a 6-month study. The protocol
should define acceptable minimum and/or maximum follow-up durations, or
describe how differences in length will be addressed in the analysis
(e.g., through meta-regression).</p>
<hr />
<p><strong>[3] Searching and Collecting Studies</strong></p>
<p>Numerous databases should be consulted based on the research
field:</p>
<p>For Major Medical Databases:</p>
<ul>
<li><strong>PubMed/MEDLINE</strong></li>
<li><strong>Embase</strong></li>
<li><strong>Web of Science</strong></li>
<li><strong>ClinicalTrials.gov</strong></li>
<li><strong>Cochrane Central (CENTRAL)</strong></li>
</ul>
<p>Inclusion/exclusion criteria are applied at the <strong>study
level</strong> (unlike patient-level in clinical trials).</p>
<hr />
<p><strong>[4] Data Abstraction and Extraction</strong></p>
<p>A <strong>Data Extraction Form (DAF)</strong> is essential for:</p>
<ul>
<li>Standardizing data capture from multiple studies.</li>
<li>Ensuring clarity and reproducibility.</li>
<li>Supporting quality assurance and future meta-analysis updates.</li>
</ul>
<p>Data abstraction should be guided by:</p>
<ul>
<li>Disease definition</li>
<li>Outcomes</li>
<li>Control group type</li>
<li>Study design</li>
<li>Patient population</li>
<li>Duration of follow-up</li>
</ul>
<hr />
<p><strong>[5] Meta-Analysis Methods</strong></p>
<p>The statistical methods must be pre-specified in the protocol:</p>
<ul>
<li>Determined by study design and type of outcome data.</li>
<li>Must account for <strong>heterogeneity</strong> across studies.</li>
<li>May need adjustment based on study characteristics identified during
data abstraction.</li>
</ul>
<hr />
<p><strong>[6] Reporting the Meta-Analysis Results</strong></p>
<p>A comprehensive meta-analysis report should mirror the structure of
the protocol and include:</p>
<ul>
<li><strong>Executive Summary / Abstract</strong></li>
<li><strong>Objective</strong></li>
<li><strong>Study Search and Inclusion Criteria</strong></li>
<li><strong>Data Extraction and Methods</strong></li>
<li><strong>Results (with figures, forest plots, heterogeneity
analysis)</strong></li>
<li><strong>Discussion and Conclusion</strong></li>
<li><strong>Appendices</strong></li>
</ul>
<p>This final report serves as:</p>
<ul>
<li>Documentation of the research process.</li>
<li>A source for publications.</li>
<li>A benchmark for transparency and methodological rigor.</li>
</ul>
</div>
<div id="data-extraction-and-coding-in-meta-analysis"
class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Data Extraction and
Coding in Meta-Analysis</h2>
<p>Once the selection of studies for a meta-analysis is finalized, the
next essential step is data extraction. This step forms the foundation
for all subsequent analyses and must be conducted systematically and
thoroughly. According to best practices, there are three major types of
information that should be extracted from each study:</p>
<ol style="list-style-type: decimal">
<li>Characteristics of the studies</li>
<li>Data necessary to calculate effect sizes</li>
<li>Study quality or risk of bias information</li>
</ol>
<p>Each category serves a distinct purpose in ensuring transparency,
reproducibility, and validity in the meta-analytic process.</p>
<p><strong>1. Characteristics of the Studies</strong></p>
<p>A high-quality meta-analysis typically presents a table summarizing
the key features of the included studies. While the exact content may
vary depending on the research field and specific question, the
following information should always be included:</p>
<ul>
<li>First author’s name</li>
<li>Year of publication</li>
<li>Sample size</li>
</ul>
<p>In addition, it is common to include characteristics that correspond
to the PICO framework (Population, Intervention, Comparison, Outcome),
which helps define the scope and context of the analysis:</p>
<ul>
<li>Country where the study was conducted</li>
<li>Mean or median age of participants</li>
<li>Proportion of female and male participants</li>
<li>Description of the intervention or exposure</li>
<li>Type of control group or comparator, if applicable</li>
<li>Primary and secondary outcomes assessed</li>
</ul>
<p>If certain details are not available in a study, it is important to
clearly indicate that the information is missing or not reported.</p>
<p><strong>2. Data for Calculating Effect Sizes</strong></p>
<p>In addition to descriptive characteristics, numerical data must be
extracted to compute effect sizes or outcome measures. This could
include means and standard deviations, proportions, odds ratios, or
correlation coefficients, depending on the metric chosen for
synthesis.</p>
<p>If subgroup analyses or meta-regressions are planned, any relevant
variables that could act as moderators or covariates must also be
collected. These may include intervention duration, dosage levels, study
setting, or publication year. Structuring this data properly in a
spreadsheet or database is critical for efficient analysis and minimizes
the risk of error.</p>
<p><strong>3. Assessment of Study Quality or Risk of Bias</strong></p>
<p>Assessing the quality or risk of bias of the primary studies is an
essential component of meta-analysis. The specific approach depends on
the study design:</p>
<ul>
<li><p>For randomized controlled trials, the <strong>Cochrane Risk of
Bias Tool</strong> is the standard. This tool evaluates various domains
where bias could occur, such as random sequence generation, allocation
concealment, blinding, incomplete outcome data, and selective reporting.
Each domain is rated as “low risk,” “high risk,” or “some
concerns.”</p></li>
<li><p>The focus of the risk of bias tool is not on the overall
methodological quality of the study but on whether specific aspects of
the design or conduct increase the likelihood of systematic errors in
the findings. A study might follow all standard practices in its field
and still have a high risk of bias due to subtle flaws in execution or
reporting.</p></li>
<li><p>For non-randomized studies, the <strong>ROBINS-I tool</strong>
(Risk Of Bias In Non-randomized Studies - of Interventions) is commonly
used. It adapts the risk of bias framework for studies where
participants are not randomly assigned to conditions.</p></li>
</ul>
<p>Visual summaries of risk of bias assessments are often created to
clearly communicate the findings. These summaries allow readers to
quickly understand the potential weaknesses in the included studies and
judge how much confidence to place in the results.</p>
<p>In some disciplines, particularly outside of medicine, standardized
assessments of study quality or bias are less common. For example, in
fields like psychology, quality assessments may be inconsistent or
entirely absent. In such cases, researchers can try to adapt existing
tools to fit their needs or refer to high-quality meta-analyses in
related areas to identify practical strategies for evaluating study
credibility.</p>
<p><strong>Distinguishing Study Quality from Risk of Bias</strong>
Although the terms are sometimes used interchangeably, study quality and
risk of bias refer to different concepts:</p>
<ul>
<li><strong>Study quality</strong> generally refers to the extent to
which a study follows accepted methodological standards and reporting
practices.</li>
<li><strong>Risk of bias</strong> focuses specifically on whether the
results of the study may be distorted due to systematic errors in the
design, conduct, or reporting.</li>
</ul>
<p>A study might be considered high quality based on standard criteria
yet still be at risk of bias in ways that could affect the
trustworthiness of its results. Therefore, evaluating risk of bias
directly is essential to determine whether the findings are credible and
usable in a meta-analysis.</p>
</div>
<div id="simpsons-paradox-and-visualization" class="section level2"
number="1.4">
<h2><span class="header-section-number">1.4</span> Simpson’s Paradox and
Visualization</h2>
<p><strong>What Is Simpson’s Paradox?</strong></p>
<p>Simpson’s paradox, sometimes called the <em>ecological fallacy</em>,
occurs when the relationship between two variables reverses when a third
variable is taken into account. In other words, a trend present within
multiple subgroups can disappear or even reverse when the data are
aggregated.</p>
<p>In meta-analysis, this paradox often arises when the size of
treatment arms is imbalanced across studies. For example, in the
rosiglitazone meta-analysis, individual trials suggested that the
treatment increased myocardial infarction (MI) risk, yet when data were
pooled across all trials without stratification, the overall effect
appeared reversed or null.</p>
<p><strong>Why Does It Happen?</strong></p>
<p>The paradox occurs because a confounding variable (like study size or
allocation imbalance) distorts the aggregated association. This
confounder may not be random but instead structured—such as when trials
with more rosiglitazone patients also have shorter follow-ups and fewer
events, giving misleading overall results.</p>
<p><strong>How to Perform Stratification</strong></p>
<p>To avoid Simpson’s paradox, one must stratify the analysis by the
confounding variable—in meta-analysis, this is usually the study
(trial). Rather than pooling all 2×2 tables directly, meta-analytic
methods like the Mantel-Haenszel or inverse-variance weighting approach
stratify by trial and then combine effect estimates.</p>
<p><strong>How to Visualize Simpson’s Paradox</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Scatter Plot</strong></p>
<ul>
<li>X-axis: Proportion of patients receiving active treatment in each
trial</li>
<li>Y-axis: Event rate (e.g., MI incidence)</li>
<li>Each point represents a trial. If there’s a negative trend, it may
misleadingly suggest treatment reduces risk—hence potential for
Simpson’s paradox.</li>
</ul></li>
<li><p><strong>Line Plot</strong></p>
<ul>
<li><p>X-axis: Treatment (0 = control, 1 = active)</p></li>
<li><p>Y-axis: Event rate for each arm within a trial</p></li>
<li><p>Each line shows the within-trial risk difference.</p></li>
<li><p>Additional lines include:</p>
<ul>
<li>Green: unweighted average risk difference</li>
<li>Blue: precision-weighted meta-analytic estimate</li>
<li>Red: naive pooled result (without stratification), which may show a
reversed effect</li>
</ul></li>
</ul>
<p>Diamond symbols can indicate the size of treatment/control arms to
reflect influence.</p></li>
<li><p><strong>Overlay Plot</strong></p>
<ul>
<li>Combines scatter and line plots</li>
<li>Each line represents a trial (with risk difference slope), and a
point on the line indicates the overall event rate for that trial, based
on its treatment allocation.</li>
<li>Useful for visualizing how varying treatment proportions influence
trial outcomes and contribute to the paradox.</li>
</ul></li>
</ol>
<pre class="r"><code>df &lt;-  read.csv(&quot;./01_Datasets/Simpson_Meta-Analysis_Data.csv&quot;)  

# 1. Scatterplot: Treatment Proportion vs Overall Event Rate
ggplot(df, aes(x = Treatment_Prop, y = Overall_Rate)) +
  geom_point(size = 3) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE, linetype = &quot;dashed&quot;, color = &quot;blue&quot;) +
  ggtitle(&quot;Scatterplot of Event Rate vs Treatment Proportion&quot;) +
  xlab(&quot;Proportion Treated&quot;) +
  ylab(&quot;Overall Event Rate&quot;)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="r"><code># 2. Line Plot: Within-Trial Risk Difference
line_data &lt;- df %&gt;%
  select(Trial, Treated_Rate, Control_Rate) %&gt;%
  pivot_longer(cols = c(&quot;Treated_Rate&quot;, &quot;Control_Rate&quot;),
               names_to = &quot;Group&quot;,
               values_to = &quot;Event_Rate&quot;) %&gt;%
  mutate(Group = ifelse(Group == &quot;Control_Rate&quot;, 0, 1))

ggplot(line_data, aes(x = Group, y = Event_Rate, group = Trial)) +
  geom_line(alpha = 0.6) +
  geom_point() +
  stat_summary(fun = mean, geom = &quot;line&quot;, aes(group = 1), color = &quot;green&quot;, linetype = &quot;dashed&quot;, size = 1) +
  ggtitle(&quot;Line Plot of Event Rate by Treatment Group (Per Trial)&quot;) +
  xlab(&quot;Treatment Group (0 = Control, 1 = Treated)&quot;) +
  ylab(&quot;Event Rate&quot;)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-1-2.png" width="672" /></p>
<pre class="r"><code># 3. Overlay Plot: Combine Scatter and Line Plot
overlay_data &lt;- line_data %&gt;%
  group_by(Trial) %&gt;%
  summarise(slope = diff(Event_Rate),
            intercept = first(Event_Rate)) %&gt;%
  left_join(df %&gt;% select(Trial, Treatment_Prop, Overall_Rate), by = &quot;Trial&quot;)

ggplot() +
  geom_abline(data = overlay_data,
              aes(intercept = intercept, slope = slope, group = Trial),
              alpha = 0.5) +
  geom_point(data = overlay_data,
             aes(x = Treatment_Prop, y = Overall_Rate),
             size = 3, color = &quot;red&quot;) +
  ggtitle(&quot;Overlay Plot Showing Simpson’s Paradox&quot;) +
  xlab(&quot;Proportion Treated&quot;) +
  ylab(&quot;Event Rate&quot;)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-1-3.png" width="672" /></p>
</div>
<div id="fixed-effects-and-random-effects" class="section level2"
number="1.5">
<h2><span class="header-section-number">1.5</span> Fixed-Effects and
Random-Effects</h2>
<p>In meta-analysis, we aim to combine results from multiple independent
studies to arrive at an overall conclusion about a treatment or effect.
Two fundamental statistical models used in meta-analysis are the
<strong>fixed-effects model</strong> and the <strong>random-effects
model</strong>.</p>
<p>The typical aim of a clinical trial is to compare the efficacy of a
treatment (such as a new drug D) against a control (e.g., placebo P).
This comparison is made using a treatment effect size, denoted by δ,
which is a numerical summary of the difference in outcomes between the
treatment and control groups.</p>
<ul>
<li><p>For <strong>binary data</strong> (e.g., death vs. survival),
effect sizes may include:</p>
<ul>
<li>Difference in proportions</li>
<li>Log odds ratio</li>
<li>Relative risk</li>
</ul></li>
<li><p>For <strong>continuous data</strong> (e.g., blood pressure,
exercise capacity), effect sizes may include:</p>
<ul>
<li>Difference in means</li>
<li>Standardized mean difference</li>
</ul></li>
</ul>
<p>The hypotheses are:</p>
<ul>
<li>Null hypothesis (H₀): δ = 0 (no difference between treatment and
control)</li>
<li>Alternative hypothesis (Hₐ): δ ≠ 0 or δ &gt; 0 (depending on the
test direction)</li>
</ul>
<p>Each study provides an estimate of the treatment effect (denoted
<span class="math inline">\(\hat{\delta}_i\)</span>) and its variance
<span class="math inline">\(\hat{\sigma}_i^2\)</span>. Meta-analysis
combines these estimates across studies to evaluate the overall
effect.</p>
<p>In the context of meta-analysis, the model’s role is to explain the
distribution of effect sizes across studies. Even though each study
reports a slightly different effect size, we typically believe that
there is some <em>true</em> underlying effect, and that differences
between studies arise due to random sampling error—or possibly due to
other systematic factors like study design, sample characteristics, or
measurement methods.</p>
<p>So, the purpose of a meta-analytic model is twofold:</p>
<ol style="list-style-type: decimal">
<li><strong>To estimate the overall (true) effect size</strong> across
all studies.</li>
<li><strong>To account for and explain the variation</strong> in effect
sizes between studies.</li>
</ol>
<p>This means our model must not only compute an average effect but also
describe how and why the results of individual studies deviate from this
average.</p>
<p><strong>Two Primary Models in Meta-Analysis</strong></p>
<p>There are two main statistical models used in meta-analysis, each
offering a different explanation for between-study variability:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Fixed-Effect Model</strong></p>
<ul>
<li>Assumes that all studies share one true effect size.</li>
<li>Differences in observed effect sizes are solely due to sampling
error.</li>
<li>Appropriate when studies are highly similar in design, population,
and measurement.</li>
<li><strong>The idea behind the fixed-effect model is that observed
effect sizes may vary from study to study, but this is only because of
the sampling error. In reality, their true effect sizes are all the
same: they are fixed. For this reason, the fixed-effect model is
sometimes also referred to as the “equal effects” or “common effect”
model.</strong></li>
</ul></li>
<li><p><strong>Random-Effects Model</strong></p>
<ul>
<li>Assumes that the true effect size may vary from study to study.</li>
<li>Observed differences reflect both sampling error and real
differences in underlying effects.</li>
<li>More appropriate when studies differ in methods, populations, or
interventions.</li>
<li><strong>The random-effects model assumes that there is not only one
true effect size but a distribution of true effect sizes. The goal of
the random-effects model is therefore not to estimate the one true
effect size of all studies, but the mean of the distribution of true
effects.</strong></li>
</ul></li>
</ol>
<p>Although these models are based on different assumptions, they are
conceptually linked. Both aim to estimate a central tendency of effect
sizes, but they do so in different ways depending on their view of
heterogeneity—the degree to which effect sizes genuinely differ across
studies.</p>
<div id="fixed-effects-model" class="section level3" number="1.5.1">
<h3><span class="header-section-number">1.5.1</span> Fixed-Effects
Model</h3>
<p><strong>Assumption</strong>: All studies estimate the <strong>same
true effect</strong> δ. Any variation among study results is due to
<strong>random error</strong> or <strong>sampling variability</strong>,
not due to differences in the underlying populations or study
conditions.</p>
<p>This model assumes that:</p>
<p><span class="math display">\[
\hat{\delta}_i = \delta + \varepsilon_i
\]</span></p>
<p>where <span class="math inline">\(\varepsilon_i \sim N(0,
\hat{\sigma}_i^2)\)</span> represents the random error in study <span
class="math inline">\(i\)</span>.</p>
<p>Thus,</p>
<p><span class="math display">\[
\hat{\delta}_i \sim N(\delta, \hat{\sigma}_i^2)
\]</span></p>
<p>The objective is to compute a <strong>weighted average</strong> of
the individual study estimates to derive an overall estimate <span
class="math inline">\(\hat{\delta}\)</span> of δ:</p>
<p><span class="math display">\[
\hat{\delta} = \sum_{i=1}^K w_i \hat{\delta}_i
\]</span></p>
<p>where the weights <span class="math inline">\(w_i\)</span> are
typically chosen to give more weight to more precise studies. A common
choice is:</p>
<p><span class="math display">\[
w_i = \frac{1}{\hat{\sigma}_i^2}
\]</span></p>
<p>The variance of the pooled estimate is:</p>
<p><span class="math display">\[
\text{Var}(\hat{\delta}) = \sum_{i=1}^K w_i^2 \hat{\sigma}_i^2
\]</span></p>
<p>A 95% confidence interval for the overall effect size is:</p>
<p><span class="math display">\[
\hat{\delta} \pm 1.96 \times \sqrt{\text{Var}(\hat{\delta})}
\]</span></p>
<p>A test statistic to assess statistical significance of the combined
estimate is:</p>
<p><span class="math display">\[
T = \frac{\hat{\delta} - 0}{\sqrt{\text{Var}(\hat{\delta})}}
\]</span></p>
<p><strong>Weighting Schemes</strong> in fixed-effects
meta-analysis:</p>
<ol style="list-style-type: decimal">
<li>Equal weights: <span class="math inline">\(w_i =
\frac{1}{K}\)</span>, where K is the number of studies.</li>
<li>By sample size: <span class="math inline">\(w_i =
\frac{N_i}{N}\)</span>, where <span class="math inline">\(N_i\)</span>
is the sample size in study <span class="math inline">\(i\)</span> and
<span class="math inline">\(N\)</span> is the total across studies.</li>
<li>By treatment group sizes: <span class="math inline">\(w_i =
\frac{N_{iD} \cdot N_{iP}}{N_{iD} + N_{iP}} \times
\frac{1}{w}\)</span></li>
<li><strong>Inverse-variance weighting</strong> (most common): <span
class="math inline">\(w_i = \frac{1}{\hat{\sigma}_i^2}\)</span></li>
</ol>
<p>In fixed-effects meta-analysis for binary data, a special case is the
<strong>Mantel-Haenszel method</strong>, which is used to combine odds
ratios or risk ratios across 2x2 tables using the hypergeometric
distribution.</p>
</div>
<div id="random-effects-model" class="section level3" number="1.5.2">
<h3><span class="header-section-number">1.5.2</span> Random-Effects
Model</h3>
<p><strong>Assumption</strong>: Each study estimates its <strong>own
true effect</strong> <span class="math inline">\(\delta_i\)</span>,
which itself is a random draw from a distribution of true effects
centered at the global mean effect δ. This accounts for
<strong>between-study variability</strong>, such as differences in study
design, populations, or measurement protocols.</p>
<p>The model assumes:</p>
<p><span class="math display">\[
\hat{\delta}_i = \delta_i + \varepsilon_i \quad \text{with} \quad
\delta_i \sim N(\delta, \tau^2)
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\varepsilon_i \sim N(0, \hat{\sigma}_i^2)
\]</span></p>
<p>So, the total variance in observed effects is:</p>
<p><span class="math display">\[
\text{Var}(\hat{\delta}_i) = \tau^2 + \hat{\sigma}_i^2
\]</span></p>
<p>Here, <span class="math inline">\(\tau^2\)</span> is the
<strong>between-study variance</strong>, and it must be estimated (e.g.,
using the DerSimonian-Laird method).</p>
<p>The pooled estimate under the random-effects model is:</p>
<p><span class="math display">\[
\hat{\delta}_{RE} = \sum_{i=1}^K w_i^* \hat{\delta}_i \quad \text{with}
\quad w_i^* = \frac{1}{\hat{\sigma}_i^2 + \tau^2}
\]</span></p>
<p>The confidence interval and hypothesis testing proceed in a similar
way as with fixed-effects, but with the adjusted weights accounting for
both within- and between-study variability.</p>
</div>
<div id="compare" class="section level3" number="1.5.3">
<h3><span class="header-section-number">1.5.3</span> Compare</h3>
<p>The decision between using a fixed-effects model or a random-effects
model in meta-analysis is <strong>not solely determined by statistical
tests for heterogeneity</strong>, such as the Cochran’s Q test or the I²
statistic. Rather, the choice should depend on both <strong>the
scientific context</strong> and <strong>the objectives</strong> of the
meta-analysis. Analysts must make a judgment based on the <strong>design
of the studies</strong>, <strong>clinical and methodological
similarities</strong>, and <strong>the rationale behind combining the
studies in the first place</strong>.</p>
<p>A <strong>fixed-effects model</strong> is appropriate when the
<strong>true treatment effect is assumed to be the same across all
included studies</strong>. The variation in observed results from one
study to another is believed to be entirely due to <strong>sampling
error</strong> or <strong>random within-study variation</strong>, not
due to differences in populations, interventions, or study
procedures.</p>
<p>To justify this model, the analyst should evaluate whether there is
an <strong>a priori reason</strong> to believe all studies are
estimating the <strong>same underlying treatment effect</strong>. For
example, this is plausible when the studies:</p>
<ul>
<li><p>Use <strong>identical protocols</strong> or designs,</p></li>
<li><p>Involve <strong>similar patient populations</strong>,</p></li>
<li><p>Use <strong>uniform interventions</strong> and
<strong>outcomes</strong>,</p></li>
<li><p>Are conducted in <strong>similar settings</strong>.</p></li>
<li><p>Use <strong>fixed-effects</strong> model when:</p>
<ul>
<li>Studies are <strong>homogeneous</strong> (similar population,
methods, settings)</li>
<li>There is <strong>no evidence</strong> of between-study heterogeneity
(e.g., I² close to 0%)</li>
<li>You believe that all studies estimate the same true effect</li>
</ul></li>
</ul>
<p>A <strong>random-effects model</strong> is used when there is
<strong>no strong reason to assume that the treatment effect is
identical across all studies</strong>. This model assumes that each
study estimates its <strong>own effect size</strong>, which itself is
drawn from a <strong>distribution of effect sizes</strong> centered
around a population mean. Thus, the model incorporates <strong>both
within-study variation</strong> and <strong>between-study
heterogeneity</strong>.</p>
<p>A random-effects model is more appropriate when:</p>
<ul>
<li><p>Studies differ in <strong>designs, populations, treatment
settings</strong>, or <strong>outcome definitions</strong>,</p></li>
<li><p>There is <strong>clinical or methodological diversity</strong>
among the studies,</p></li>
<li><p>The treatment might <strong>interact with population
characteristics</strong>, such as age, gender, disease severity,
geography, etc.</p></li>
<li><p>Use <strong>random-effects</strong> model when:</p>
<ul>
<li>Studies are <strong>heterogeneous</strong> in population, protocol,
or design</li>
<li>You wish to generalize the results beyond the observed studies</li>
<li>The test for heterogeneity is significant (e.g., high I² or
significant Q test)</li>
</ul></li>
</ul>
<table>
<colgroup>
<col width="22%" />
<col width="32%" />
<col width="44%" />
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Fixed-Effects Model</th>
<th>Random-Effects Model</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>True Effect Assumption</td>
<td>One common true effect</td>
<td>Distribution of true effects</td>
</tr>
<tr class="even">
<td>Heterogeneity Allowed</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td>Weighting</td>
<td>Inverse of within-study variance</td>
<td>Inverse of (within + between-study variance)</td>
</tr>
<tr class="even">
<td>Suitable When</td>
<td>Studies are very similar</td>
<td>Studies differ (clinical/methodological)</td>
</tr>
<tr class="odd">
<td>Confidence Intervals</td>
<td>Narrower</td>
<td>Wider</td>
</tr>
<tr class="even">
<td>Example Use</td>
<td>Repeated trials in same setting</td>
<td>Multi-center studies across regions</td>
</tr>
</tbody>
</table>
<p>In practice, many analysts compute <strong>both fixed-effects and
random-effects models</strong>, even if they believe one is
theoretically more appropriate. This dual approach provides:</p>
<ul>
<li>A <strong>comparison</strong> of effect sizes and confidence
intervals,</li>
<li>Insight into the <strong>influence of between-study
variability</strong>,</li>
<li>A <strong>sensitivity analysis</strong> for model assumptions.</li>
</ul>
<p>The confidence intervals from the random-effects model are usually
<strong>wider</strong> than those from the fixed-effects model because
they account for <strong>both within-study and between-study
variance</strong>. If there is <strong>no heterogeneity</strong> (i.e.,
τ² = 0), both models will produce <strong>identical
results</strong>.</p>
<p>However, even when the heterogeneity tests (e.g., Q or I²) are not
significant, if there is <strong>clinical or contextual reason to expect
variation</strong>, the random-effects model is typically preferred for
<strong>generalizability</strong> and <strong>conservative
inference</strong>.</p>
</div>
</div>
<div id="implementation-in-r" class="section level2" number="1.6">
<h2><span class="header-section-number">1.6</span> Implementation in
R</h2>
<p>The <strong>rmeta</strong> package (by Thomas Lumley) and
<strong>meta</strong> package (by Guido Schwarzer) provide functions for
both models:</p>
<ul>
<li><p>For binary data:</p>
<ul>
<li>Fixed-effects (Mantel-Haenszel): <code>meta.MH</code> in
<code>rmeta</code></li>
<li>Random-effects: <code>meta.DSL</code> (DerSimonian-Laird
method)</li>
</ul></li>
<li><p>For continuous data:</p>
<ul>
<li><code>metacont</code> function in <code>meta</code> package allows
analysis under both fixed and random models.</li>
</ul></li>
</ul>
<p>These tools help analysts perform meta-analyses and produce key
outputs such as:</p>
<ul>
<li>Forest plots</li>
<li>Funnel plots</li>
<li>Estimates of heterogeneity (I², τ²)</li>
<li>Confidence intervals for pooled effect size</li>
</ul>
<div id="meta-package-analysis-for-different-data"
class="section level3" number="1.6.1">
<h3><span class="header-section-number">1.6.1</span> meta Package:
Analysis for Different Data</h3>
<ul>
<li>Meta-analysis of binary outcome data <code>metabin</code></li>
<li>Meta-analysis of continuous outcome data <code>metacont</code></li>
<li>Meta-analysis of correlations <code>metacor</code></li>
<li>Meta-analysis of incidence rates <code>metainc</code></li>
<li>Meta-regression <code>metareg</code></li>
<li>Meta-analysis of single proportions <code>metaprop</code></li>
<li>Meta-analysis of single means <code>metamean</code></li>
<li>Merge pooled results of two meta-analyses
<code>metamerge</code></li>
<li>Combine and summarize meta-analysis objects
<code>metabind</code></li>
</ul>
<p>Meta-Analysis with Survival Outcomes</p>
<pre class="r"><code>mg1 &lt;- metagen(logHR, selogHR,
               studlab=paste(author, year), data=data4,
               sm=&quot;HR&quot;)
print(mg1, digits=2)</code></pre>
<pre><code>## Number of studies: k = 4
## 
##                        HR       95%-CI     z p-value
## Common effect model  0.89 [0.78; 1.01] -1.82  0.0688
## Random effects model 0.89 [0.78; 1.01] -1.82  0.0688
## 
## Quantifying heterogeneity (with 95%-CIs):
##  tau^2 &lt; 0.0001 [0.0000; 1.2885]; tau = 0.0011 [0.0000; 1.1351]
##  I^2 = 17.2% [0.0%; 87.3%]; H = 1.10 [1.00; 2.81]
## 
## Test of heterogeneity:
##     Q d.f. p-value
##  3.62    3  0.3049
## 
## Details of meta-analysis methods:
## - Inverse variance method
## - Restricted maximum-likelihood estimator for tau^2
## - Q-Profile method for confidence interval of tau^2 and tau
## - Calculation of I^2 based on Q</code></pre>
<pre class="r"><code>## Forest plot
plot(mg1)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>## To assess potential publication bias informally, we generate the funnel plot and visually assess whether it is symmetric. 
funnel(mg1)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
<p>Meta-Analysis of Cross-Over Trials</p>
<pre class="r"><code># meta-analysis of these cross-over trials
mg2 &lt;- metagen(mean, SE, studlab=paste(author, year),
               data=data5, sm=&quot;MD&quot;)
print(summary(mg2), digits=2)</code></pre>
<pre><code>##                               MD           95%-CI %W(common) %W(random)
## Skrabal et al. 1981a       -4.50 [ -8.62;  -0.38]        2.2        4.6
## Skrabal et al. 1981b       -0.50 [ -3.83;   2.83]        3.3        5.0
## MacGregor et al. 1982      -4.00 [ -7.72;  -0.28]        2.6        4.8
## Khaw and Thom 1982         -2.40 [ -4.56;  -0.24]        7.9        5.6
## Richards et al. 1984       -1.00 [ -7.66;   5.66]        0.8        3.3
## Smith et al. 1985           0.00 [ -3.72;   3.72]        2.6        4.8
## Kaplan et al. 1985         -5.80 [ -8.94;  -2.66]        3.7        5.1
## Zoccali et al. 1985        -3.00 [ -8.88;   2.88]        1.1        3.6
## Matlou et al. 1986         -3.00 [ -5.94;  -0.06]        4.2        5.2
## Barden et al. 1986         -1.50 [ -4.24;   1.24]        4.9        5.3
## Poulter and Sever 1986      2.00 [ -2.31;   6.31]        2.0        4.5
## Grobbee et al. 1987        -0.30 [ -3.24;   2.64]        4.2        5.2
## Krishna et al. 1989        -8.00 [-12.31;  -3.69]        2.0        4.5
## Mullen and O&#39;Connor 1990a   3.00 [ -0.92;   6.92]        2.4        4.7
## Mullen and O&#39;Connor 1990b   1.40 [ -2.52;   5.32]        2.4        4.7
## Patki et al. 1990         -13.10 [-14.47; -11.73]       19.5        5.9
## Valdes et al. 1991         -3.00 [ -6.92;   0.92]        2.4        4.7
## Barden et al. 1991         -0.60 [ -1.78;   0.58]       26.5        5.9
## Overlack et al. 1991        3.00 [ -0.92;   6.92]        2.4        4.7
## Smith et al. 1992          -1.70 [ -6.60;   3.20]        1.5        4.1
## Fotherby and Potter 1992   -6.00 [-10.90;  -1.10]        1.5        4.1
## 
## Number of studies: k = 21
## 
##                         MD         95%-CI      z  p-value
## Common effect model  -3.71 [-4.32; -3.11] -12.03 &lt; 0.0001
## Random effects model -2.43 [-4.19; -0.66]  -2.69   0.0071
## 
## Quantifying heterogeneity (with 95%-CIs):
##  tau^2 = 13.3645 [6.2066; 27.6896]; tau = 3.6558 [2.4913; 5.2621]
##  I^2 = 92.5% [89.9%; 94.5%]; H = 3.66 [3.14; 4.25]
## 
## Test of heterogeneity:
##       Q d.f.  p-value
##  267.24   20 &lt; 0.0001
## 
## Details of meta-analysis methods:
## - Inverse variance method
## - Restricted maximum-likelihood estimator for tau^2
## - Q-Profile method for confidence interval of tau^2 and tau
## - Calculation of I^2 based on Q</code></pre>
<pre class="r"><code>plot(mg2)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>funnel(mg2)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-3-2.png" width="672" /></p>
</div>
</div>
<div id="combining-p-values-in-meta-analysis" class="section level2"
number="1.7">
<h2><span class="header-section-number">1.7</span> Combining p-Values in
Meta-Analysis</h2>
<p><strong>Combining p-values in meta-analysis</strong> is a statistical
approach used when studies report only p-values rather than full effect
sizes or variance estimates. This situation often arises in systematic
reviews where only minimal information is available from each individual
study. The goal is to aggregate the evidence from several such studies
to determine whether, collectively, they support or refute a common
hypothesis.</p>
<p>One of the most widely used methods for this purpose is
<strong>Fisher’s combined probability test</strong>, proposed by Ronald
Fisher. It allows researchers to combine the results of multiple
independent hypothesis tests into a single test statistic.</p>
<hr />
<p><strong>Theoretical Basis of Fisher’s Method</strong></p>
<p>Suppose you have <strong>K independent studies</strong>, each testing
the same null hypothesis (<strong>H₀</strong>) and each reporting a
p-value: <strong>p₁, p₂, …, pₖ</strong></p>
<p>Under the null hypothesis, each individual p-value follows a
<strong>uniform distribution</strong> on the interval [0, 1].
Mathematically: <strong>pᵢ ~ U[0, 1]</strong></p>
<p>If you take the <strong>negative natural logarithm</strong> of a
uniform random variable, the result follows an <strong>exponential
distribution</strong> with mean 1. Specifically: <strong>−ln(pᵢ) ~
Exp(1)</strong></p>
<p>Multiplying this value by 2 gives a value that follows a
<strong>chi-square distribution</strong> with 2 degrees of freedom:
<strong>−2ln(pᵢ) ~ χ²(2)</strong></p>
<p>If the p-values are independent, the sum of K such terms will follow
a <strong>chi-square distribution</strong> with <strong>2K degrees of
freedom</strong>:</p>
<p><strong>X² = −2 ∑ ln(pᵢ)</strong> This statistic follows a <strong>χ²
distribution with 2K degrees of freedom</strong> under the null
hypothesis.</p>
<p>A large value of X² suggests that at least some of the studies are
showing significant results, and that the overall null hypothesis may be
false.</p>
<hr />
<p><strong>Step-by-Step R Implementation</strong></p>
<p>You can easily implement Fisher’s method in R without using any
external packages. Here’s how:</p>
<ol style="list-style-type: decimal">
<li><strong>Define a function</strong> that takes a vector of p-values
and calculates the combined p-value.</li>
<li><strong>Apply the chi-square distribution function</strong>
(<code>pchisq</code>) to obtain the p-value from the combined
statistic.</li>
</ol>
<p>Here is the full function:</p>
<pre class="r"><code># Function to compute Fisher&#39;s combined p-value
fishers.pvalue &lt;- function(x) {
  # x: a numeric vector of p-values
  test_stat &lt;- -2 * sum(log(x))
  pchisq(test_stat, df = 2 * length(x), lower.tail = FALSE)
}</code></pre>
<p><strong>Worked Example</strong></p>
<p>Assume we have 4 independent studies on the effectiveness of statins.
The reported p-values from these studies are:</p>
<ul>
<li>0.106</li>
<li>0.0957</li>
<li>0.00166</li>
<li>0.0694</li>
</ul>
<p>Let’s use the function above to combine them:</p>
<pre class="r"><code># Vector of p-values
x &lt;- c(0.106, 0.0957, 0.00166, 0.0694)

# Apply Fisher&#39;s method
combined.pval &lt;- fishers.pvalue(x)

# Print the result
print(combined.pval)</code></pre>
<pre><code>## [1] 0.0006225844</code></pre>
<p>However, a few important points should be noted:</p>
<ul>
<li>This method <strong>does not account for effect size
heterogeneity</strong>.</li>
<li>It is <strong>sensitive to very small p-values</strong>, which can
dominate the result.</li>
<li>All included studies must test the <strong>same hypothesis</strong>,
and the tests must be <strong>independent</strong>.</li>
</ul>
<hr />
<p><strong>Conclusion</strong></p>
<p>Fisher’s method is a powerful and simple tool to combine p-values
when full meta-analytic data (like effect sizes and variances) are not
available. It is particularly useful in early-phase systematic reviews,
meta-analyses of hypothesis tests, or when only limited summary data are
accessible.</p>
</div>
<div id="effect-size-correction" class="section level2" number="1.8">
<h2><span class="header-section-number">1.8</span> Effect Size
Correction</h2>
<p>Effect sizes estimated from individual studies do not always
perfectly reflect the true population effect. These deviations may be
due to two types of errors:</p>
<ul>
<li><strong>Random (sampling) error</strong>: inherent variability due
to sample size</li>
<li><strong>Systematic error (bias)</strong>: distortions caused by
small sample sizes, measurement error, or limited range of data</li>
</ul>
<p>To improve the accuracy of meta-analytic estimates, specific
correction techniques can be applied. These include adjustments for
small sample bias, unreliability in measurement, and range
restriction.</p>
<hr />
<p><strong>1. Small Sample Bias Correction – Hedges’ g</strong></p>
<p>When using standardized mean differences (SMDs), studies with small
sample sizes (especially n ≤ 20) tend to overestimate the true effect
size. Hedges’ g corrects for this upward bias.</p>
<ul>
<li><p><strong>Correction Formula</strong>: Hedges’ g = SMD × (1 − 3 /
(4n − 9)) where <em>n</em> is the total sample size</p></li>
<li><p><strong>R Implementation</strong>:</p>
<pre class="r"><code>library(esc)
SMD &lt;- 0.5
n &lt;- 30
g &lt;- hedges_g(SMD, n)</code></pre></li>
<li><p><strong>Key Point</strong>: Hedges’ g is always less than or
equal to the uncorrected SMD. The smaller the sample size, the larger
the correction.</p></li>
</ul>
<hr />
<p><strong>2. Correction for Unreliability (Measurement
Error)</strong></p>
<p>Instruments used in studies often introduce error, reducing the
reliability of the measurements. This unreliability attenuates
correlations or SMDs, underestimating the true effect.</p>
<ul>
<li><p><strong>Key Terms</strong>: Reliability coefficient (rₓₓ): ranges
from 0 (unreliable) to 1 (perfectly reliable)</p></li>
<li><p><strong>Correction for SMD</strong>: Corrected SMD = SMD /
√rₓₓ</p></li>
<li><p><strong>Correction for Correlation</strong>:</p>
<ul>
<li>If only x has unreliability: rₓy(corrected) = rₓy / √rₓₓ</li>
<li>If both x and y have unreliability: rₓy(corrected) = rₓy / (√rₓₓ ×
√ryy)</li>
</ul></li>
<li><p><strong>Correction for Standard Error</strong>: Adjust SEs using
the same logic:</p>
<ul>
<li>SE(corrected) = SE / √rₓₓ (or divided by both reliabilities if
correcting both variables)</li>
</ul></li>
<li><p><strong>R Example</strong>:</p>
<pre class="r"><code>r_xy &lt;- 0.34
se_r_xy &lt;- 0.09
smd &lt;- 0.65
se_smd &lt;- 0.18
r_xx &lt;- 0.8
r_yy &lt;- 0.7

smd_c &lt;- smd / sqrt(r_xx)
se_c_smd &lt;- se_smd / sqrt(r_xx)
r_xy_c &lt;- r_xy / (sqrt(r_xx) * sqrt(r_yy))
se_c_r &lt;- se_r_xy / (sqrt(r_xx) * sqrt(r_yy))</code></pre></li>
<li><p><strong>Important Note</strong>: This correction can only be used
if the reliability coefficients are available for all studies, or if a
reasonable and justifiable estimate is used across all.</p></li>
</ul>
<hr />
<p><strong>3. Correction for Range Restriction</strong></p>
<p>Range restriction occurs when the variability of a variable in a
study is smaller than in the population, leading to attenuated effect
sizes.</p>
<ul>
<li><p><strong>Example</strong>: A study looking at age and cognition
but only including participants aged 65–69 would likely report a weak
correlation due to limited age variation.</p></li>
<li><p><strong>Correction involves calculating a ratio U</strong>:</p>
<ul>
<li>U = SD(unrestricted population) / SD(restricted sample)</li>
</ul></li>
<li><p><strong>Correction Formula for Correlation</strong>:
rₓy(corrected) = (U × rₓy) / √[(U² − 1) × rₓy² + 1]</p></li>
<li><p><strong>Correction Formula for SMD</strong>: SMD(corrected) = (U
× SMD) / √[(U² − 1) × SMD² + 1]</p></li>
<li><p><strong>Standard Error Correction</strong>:</p>
<ul>
<li>SE(corrected) = (corrected effect / original effect) ×
SE(original)</li>
</ul></li>
<li><p><strong>R Example</strong>:</p>
<pre class="r"><code>r_xy &lt;- 0.34
se_r_xy &lt;- 0.09
sd_restricted &lt;- 11
sd_unrestricted &lt;- 18
U &lt;- sd_unrestricted / sd_restricted

r_xy_c &lt;- (U * r_xy) / sqrt((U^2 - 1) * r_xy^2 + 1)
se_r_xy_c &lt;- (r_xy_c / r_xy) * se_r_xy</code></pre></li>
<li><p><strong>Usage Caveat</strong>: Like reliability corrections, this
adjustment should be applied consistently across all studies. It is
especially useful when range restriction is severe and clearly
documented.</p></li>
</ul>
<hr />
<p><strong>Summary of Key Corrections</strong></p>
<table>
<colgroup>
<col width="18%" />
<col width="22%" />
<col width="34%" />
<col width="24%" />
</colgroup>
<thead>
<tr class="header">
<th>Correction Type</th>
<th>Bias Addressed</th>
<th>Input Needed</th>
<th>Applies To</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Hedges’ g</td>
<td>Small sample size</td>
<td>Total sample size (n)</td>
<td>Standardized Mean Differences</td>
</tr>
<tr class="even">
<td>Attenuation Correction</td>
<td>Measurement unreliability</td>
<td>Reliability coefficients (rₓₓ, ryy)</td>
<td>Correlations and SMDs</td>
</tr>
<tr class="odd">
<td>Range Restriction</td>
<td>Limited variable variation</td>
<td>SD of restricted and unrestricted samples</td>
<td>Correlations and SMDs</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="between-study-heterogeneity" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Between-Study
Heterogeneity</h1>
<p>In meta-analysis, <strong>heterogeneity</strong> refers to the
variation in study outcomes beyond what would be expected by random
sampling alone. In other words, it captures how different the effect
sizes are across studies. Understanding and quantifying heterogeneity is
essential because it influences the choice of meta-analytic model
(fixed-effect vs random-effects) and the interpretation of results.</p>
<p>When using the <code>meta</code> package in R (e.g., through
functions like <code>metacont()</code> or <code>metabin()</code>), the
output includes a section called <strong>“Quantifying
heterogeneity”</strong> and <strong>“Test of heterogeneity”</strong>,
which provides several statistics to assess this variability.</p>
<p><strong>Summary of Interpretation</strong></p>
<ul>
<li><strong>Q statistic</strong> tells you whether heterogeneity is
present (yes/no)</li>
<li><strong>τ²</strong> tells you how much true heterogeneity exists (in
raw variance units)</li>
<li><strong>H</strong> tells you how much more variability is observed
than expected</li>
<li><strong>I²</strong> tells you the proportion of observed variance
that is true heterogeneity</li>
</ul>
<div id="cochrans-q" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Cochran’s Q</h2>
<p>In meta-analysis, one of the key questions is whether the variability
among the observed study effect sizes can be explained by sampling error
alone or whether there is true variability in the underlying effects
across studies. Cochran’s Q statistic is one of the oldest and most
widely used tools to assess this question.</p>
<p><strong>Purpose of Cochran’s Q</strong></p>
<p>Cochran’s Q is used to test the <strong>null hypothesis</strong> that
all studies in a meta-analysis share a <strong>common true effect
size</strong>. In other words, it tests whether the differences we see
between studies are due to chance (sampling error) or if there is
<strong>real, underlying heterogeneity</strong> between study
effects.</p>
<p>In reality, two sources of variation influence the observed effect
sizes:</p>
<ul>
<li><strong>Sampling error</strong> (εₖ): random variation due to finite
sample size.</li>
<li><strong>Between-study heterogeneity</strong> (ζₖ): true differences
in effect sizes caused by study-level differences (e.g., population,
intervention method).</li>
</ul>
<p>Cochran’s Q tries to disentangle these by testing if the observed
dispersion in effect sizes exceeds what would be expected from sampling
error alone.</p>
<p><strong>The Formula for Cochran’s Q</strong></p>
<p>The statistic is calculated as a <strong>weighted sum of squared
deviations</strong> of each study’s effect estimate (θ̂ₖ) from the
overall summary effect (θ̂):</p>
<p>  <strong>Q = ∑ₖ wₖ (θ̂ₖ − θ̂)²</strong></p>
<p>Where:</p>
<ul>
<li><strong>θ̂ₖ</strong> is the observed effect size from study
<em>k</em>.</li>
<li><strong>θ̂</strong> is the pooled effect size under the fixed-effect
model.</li>
<li><strong>wₖ</strong> is the inverse-variance weight for study
<em>k</em> (usually 1 / variance of θ̂ₖ).</li>
</ul>
<p>Because the weights depend on the precision of the study, studies
with smaller standard errors (i.e. larger sample sizes) contribute more
to Q.</p>
<p><strong>Interpreting Q</strong></p>
<p>Under the null hypothesis of homogeneity (i.e. no between-study
heterogeneity), Cochran’s Q follows a <strong>chi-squared
distribution</strong> with <strong>K−1 degrees of freedom</strong>,
where K is the number of studies. If Q is significantly larger than this
expected distribution, it suggests <strong>excess variation</strong>—a
sign of <strong>true heterogeneity</strong> among studies.</p>
<p><strong>Limitations and Cautions</strong></p>
<p>Despite its popularity, Cochran’s Q has several limitations:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Sensitivity to Study Count (K)</strong>: Q increases with
the number of studies. With many studies, even small differences can
lead to a significant Q, possibly <strong>overstating
heterogeneity</strong>.</p></li>
<li><p><strong>Sensitivity to Study Precision</strong>: High-precision
studies (e.g., with large samples) contribute more to Q. Even small
deviations from the mean can yield large Q values, which may
<strong>inflate the signal</strong> of heterogeneity.</p></li>
<li><p><strong>Interpretation Is Not Binary</strong>: It’s not
sufficient to simply rely on the <strong>p-value of the Q-test</strong>
to decide whether to use a fixed-effect or random-effects model. A
significant Q does not always mean true heterogeneity is practically
important.</p></li>
<li><p><strong>Chi-Squared Distribution May Be Misleading</strong>: The
actual distribution of Q in real-world meta-analyses may differ from the
theoretical chi-squared distribution. As noted by Hoaglin (2016), this
misfit can lead to biases, especially in methods like DerSimonian-Laird
which rely on Q.</p></li>
</ol>
<p><strong>Conclusion and Best Practice</strong></p>
<p>Cochran’s Q is a foundational tool for assessing heterogeneity in
meta-analysis. However, its interpretation requires caution:</p>
<ul>
<li>Do not use Q alone to determine model choice.</li>
<li>Consider the <strong>magnitude</strong> and
<strong>consistency</strong> of heterogeneity.</li>
<li>Complement Q with <strong>other heterogeneity statistics</strong>,
such as I² or τ², which quantify heterogeneity rather than simply test
for its presence.</li>
</ul>
</div>
<div id="tau-squared-τ²" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Tau-squared (τ²)</h2>
<p>Tau-squared estimates the <strong>between-study variance</strong>,
i.e., the variance of the true effect sizes across studies. This value
is crucial in <strong>random-effects models</strong>, where it directly
influences the weights assigned to each study.</p>
<p>Tau-squared is estimated using the <strong>DerSimonian and Laird
method</strong>:</p>
<p><strong>τ² = (Q − (K − 1)) / U</strong></p>
<p>Where:</p>
<ul>
<li>Q is Cochran’s Q statistic</li>
<li>K is the number of studies</li>
<li>U is a function of the study weights</li>
</ul>
<p>If Q &lt; K − 1 due to sampling error, τ² is set to zero. A larger τ²
indicates more dispersion in the true effects.</p>
</div>
<div id="h²-statistic-ratio-based-measure-of-heterogeneity"
class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> H² Statistic:
Ratio-Based Measure of Heterogeneity</h2>
<p>The H² statistic, also introduced by Higgins and Thompson (2002), is
another measure of between-study heterogeneity in meta-analyses. Like
I², it is based on Cochran’s Q statistic but offers a slightly different
interpretation.</p>
<p>H² quantifies the ratio between the observed variability (as measured
by Q) and the amount of variability expected from sampling error alone
(i.e., under the null hypothesis of homogeneity). It is calculated
as:</p>
<p><strong>H² = Q / (K − 1)</strong></p>
<p>Where:</p>
<ul>
<li><strong>Q</strong> is Cochran’s Q statistic.</li>
<li><strong>K</strong> is the number of studies.</li>
<li><strong>K − 1</strong> represents the degrees of freedom under the
null hypothesis.</li>
</ul>
<p><strong>Interpretation</strong></p>
<ul>
<li><strong>H² = 1</strong>: All variation is due to sampling error
(i.e., no between-study heterogeneity).</li>
<li><strong>H² &gt; 1</strong>: Suggests that additional variability
exists beyond what would be expected by chance, indicating the presence
of heterogeneity.</li>
</ul>
<p>Unlike I², there is no need to adjust H² values if Q is smaller than
(K − 1). H² values are always ≥ 1.</p>
<p>A confidence interval for H can be constructed by assuming that
<strong>ln(H)</strong> is approximately normally distributed. If the
lower bound of this interval is above 1, it suggests statistically
significant heterogeneity.</p>
<p>The formula for the standard error of ln(H) depends on the value of
Q:</p>
<ul>
<li>If Q &gt; K: SE is estimated using log transformations</li>
<li>If Q ≤ K: SE uses a different approximation due to boundary
issues</li>
</ul>
</div>
<div id="higgins-thompsons-i²-statistic" class="section level2"
number="2.4">
<h2><span class="header-section-number">2.4</span> Higgins &amp;
Thompson’s I² Statistic</h2>
<p>The I² statistic, introduced by Higgins and Thompson in 2002,
provides a clear way to quantify the proportion of variability in effect
sizes that is due to true heterogeneity, rather than random sampling
error. It is derived from Cochran’s Q statistic and is expressed as a
percentage.</p>
<p><strong>Purpose and Interpretation</strong></p>
<p>I² indicates the proportion of observed variation across studies that
is real and not due to chance:</p>
<ul>
<li>An I² value of 0% suggests all variability is due to random
error.</li>
<li>An I² value of 100% suggests all variability is due to actual
differences between studies.</li>
</ul>
<p>This makes I² a helpful indicator when assessing consistency in
meta-analysis results.</p>
<p><strong>Formula</strong></p>
<p>The I² statistic is calculated as:</p>
<p> I² = (Q - (K - 1)) / Q</p>
<p>Where:</p>
<ul>
<li>Q is Cochran’s Q value.</li>
<li>K is the number of studies.</li>
<li>K − 1 is the degrees of freedom.</li>
</ul>
<p>If the result is negative (i.e., when Q &lt; K − 1), I² is set to 0,
since negative variance proportions are not meaningful.</p>
<p>This value ranges from 0% to 100%:</p>
<ul>
<li><strong>0–25%</strong>: low heterogeneity</li>
<li><strong>25–50%</strong>: moderate heterogeneity</li>
<li><strong>50–75%</strong>: substantial heterogeneity</li>
<li><strong>75–100%</strong>: considerable heterogeneity</li>
<li>Negative values of I² are set to 0 by convention. If the
<strong>lower bound of the confidence interval for I² is greater than
zero</strong>, heterogeneity is considered statistically
significant.</li>
</ul>
<p>Alternatively, I² can be computed from H:</p>
<p><strong>I² = (H² − 1) / H² × 100%</strong></p>
<p>This formulation is used to derive confidence intervals for I² from
those for H.</p>
<p><strong>Strengths of the I² Statistic</strong></p>
<ul>
<li>It offers a standardized, easy-to-understand measure of
heterogeneity.</li>
<li>It is commonly included by default in meta-analysis software
outputs.</li>
<li>It allows comparison across meta-analyses with different sample
sizes or outcomes.</li>
</ul>
<p><strong>Limitations</strong></p>
<ul>
<li>Since I² depends on Cochran’s Q, it inherits Q’s sensitivity to the
number of studies and study precision.</li>
<li>Low I² values might appear even when real heterogeneity exists if
studies are small or imprecise.</li>
<li>It does not express the <strong>magnitude</strong> of heterogeneity
in absolute terms, unlike τ².</li>
</ul>
</div>
<div id="assessing-heterogeneity-in-r" class="section level2"
number="2.5">
<h2><span class="header-section-number">2.5</span> Assessing
Heterogeneity in R</h2>
<div id="basic-outlier-removal-in-r" class="section level3"
number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Basic Outlier
Removal in R</h3>
<p>In meta-analysis, some studies may report unusually extreme results
that differ substantially from the overall effect size. These studies
are called outliers. Removing such outliers can help improve the
accuracy of the pooled effect estimate and the measures of
heterogeneity. One simple method to detect outliers is based on
comparing confidence intervals.</p>
<p>A study is considered an outlier if its 95% confidence interval does
not overlap with the confidence interval of the pooled effect estimate.
Specifically, this means:</p>
<ul>
<li><p>The upper bound of the study’s confidence interval is lower than
the lower bound of the pooled effect’s confidence interval, suggesting
the study shows an extremely small effect.</p></li>
<li><p>The lower bound of the study’s confidence interval is higher than
the upper bound of the pooled effect’s confidence interval, suggesting
the study shows an extremely large effect.</p></li>
</ul>
<p>This method is straightforward and practical. Studies with large
standard errors often have wide confidence intervals that are more
likely to overlap with the pooled effect. Therefore, they are less
likely to be flagged as outliers. However, if a study has a narrow
confidence interval and still shows a strong deviation from the pooled
effect, it is more likely to be classified as an outlier.</p>
<p><strong>Using the find.outliers Function from the dmetar
Package</strong></p>
<p>The <code>find.outliers</code> function, available in the R package
<code>dmetar</code>, automates the process of detecting outliers using
the method described above. This function requires a meta-analysis
object created using the <code>meta</code> package (such as from the
<code>metagen</code> function).</p>
<pre class="r"><code>m.gen &lt;- metagen(TE = TE,
                 seTE = seTE,
                 studlab = Author,
                 data = ThirdWave,
                 sm = &quot;SMD&quot;,
                 fixed = FALSE,
                 random = TRUE,
                 method.tau = &quot;REML&quot;,
                 method.random.ci = &quot;HK&quot;,
                 title = &quot;Third Wave Psychotherapies&quot;)
summary(m.gen)</code></pre>
<pre><code>## Review:     Third Wave Psychotherapies
## 
##                           SMD            95%-CI %W(random)
## Call et al.            0.7091 [ 0.1979; 1.2203]        5.0
## Cavanagh et al.        0.3549 [-0.0300; 0.7397]        6.3
## DanitzOrsillo          1.7912 [ 1.1139; 2.4685]        3.8
## de Vibe et al.         0.1825 [-0.0484; 0.4133]        7.9
## Frazier et al.         0.4219 [ 0.1380; 0.7057]        7.3
## Frogeli et al.         0.6300 [ 0.2458; 1.0142]        6.3
## Gallego et al.         0.7249 [ 0.2846; 1.1652]        5.7
## Hazlett-Stevens &amp; Oren 0.5287 [ 0.1162; 0.9412]        6.0
## Hintz et al.           0.2840 [-0.0453; 0.6133]        6.9
## Kang et al.            1.2751 [ 0.6142; 1.9360]        3.9
## Kuhlmann et al.        0.1036 [-0.2781; 0.4853]        6.3
## Lever Taylor et al.    0.3884 [-0.0639; 0.8407]        5.6
## Phang et al.           0.5407 [ 0.0619; 1.0196]        5.3
## Rasanen et al.         0.4262 [-0.0794; 0.9317]        5.1
## Ratanasiripong         0.5154 [-0.1731; 1.2039]        3.7
## Shapiro et al.         1.4797 [ 0.8618; 2.0977]        4.2
## Song &amp; Lindquist       0.6126 [ 0.1683; 1.0569]        5.7
## Warnecke et al.        0.6000 [ 0.1120; 1.0880]        5.2
## 
## Number of studies: k = 18
## 
##                              SMD           95%-CI    t  p-value
## Random effects model (HK) 0.5771 [0.3782; 0.7760] 6.12 &lt; 0.0001
## 
## Quantifying heterogeneity (with 95%-CIs):
##  tau^2 = 0.0820 [0.0295; 0.3533]; tau = 0.2863 [0.1717; 0.5944]
##  I^2 = 62.6% [37.9%; 77.5%]; H = 1.64 [1.27; 2.11]
## 
## Test of heterogeneity:
##      Q d.f. p-value
##  45.50   17  0.0002
## 
## Details of meta-analysis methods:
## - Inverse variance method
## - Restricted maximum-likelihood estimator for tau^2
## - Q-Profile method for confidence interval of tau^2 and tau
## - Calculation of I^2 based on Q
## - Hartung-Knapp adjustment for random effects model (df = 17)</code></pre>
<pre class="r"><code>find.outliers(m.gen)</code></pre>
<pre><code>## Identified outliers (random-effects model) 
## ------------------------------------------ 
## &quot;DanitzOrsillo&quot;, &quot;Shapiro et al.&quot; 
##  
## Results with outliers removed 
## ----------------------------- 
## Review:     Third Wave Psychotherapies
## 
## Number of studies: k = 16
## 
##                              SMD           95%-CI    t  p-value
## Random effects model (HK) 0.4528 [0.3257; 0.5800] 7.59 &lt; 0.0001
## 
## Quantifying heterogeneity (with 95%-CIs):
##  tau^2 = 0.0139 [0.0000; 0.1032]; tau = 0.1180 [0.0000; 0.3213]
##  I^2 = 24.8% [0.0%; 58.7%]; H = 1.15 [1.00; 1.56]
## 
## Test of heterogeneity:
##      Q d.f. p-value
##  19.95   15  0.1739
## 
## Details of meta-analysis methods:
## - Inverse variance method
## - Restricted maximum-likelihood estimator for tau^2
## - Q-Profile method for confidence interval of tau^2 and tau
## - Calculation of I^2 based on Q
## - Hartung-Knapp adjustment for random effects model (df = 15)</code></pre>
<p>The function outputs:</p>
<ul>
<li>A list of studies identified as outliers.</li>
<li>A new meta-analysis object excluding these outliers.</li>
<li>Updated heterogeneity statistics and pooled effect estimates.</li>
</ul>
<p><strong>Advantages of This Method</strong></p>
<ul>
<li>It is simple and intuitive.</li>
<li>It helps to quickly identify influential studies that may distort
the results.</li>
<li>It supports sensitivity analyses by allowing comparison of results
before and after outlier removal.</li>
</ul>
<p><strong>Limitations to Consider</strong></p>
<ul>
<li>The method is rule-based and does not rely on statistical testing,
which can sometimes lead to over-identification of outliers.</li>
<li>It is not suitable for automatic or blind removal of studies. Each
flagged study should be reviewed for methodological quality or reasons
for its deviation.</li>
<li>The presence of moderate or high heterogeneity can make
interpretation more complex.</li>
</ul>
</div>
<div id="influence-analysis" class="section level3" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Influence
Analysis</h3>
<p>While detecting and removing outliers is a useful step in
meta-analysis, it only partially addresses the issue of robustness. Some
studies might not have extreme effect sizes, but they can still exert a
large influence on the overall meta-analytic results. Influence analysis
helps us identify such cases.</p>
<p>For instance, if a meta-analysis finds a significant pooled effect,
but this significance disappears once a single study is removed, that
study is considered influential. Recognizing such studies is crucial,
particularly when we want to evaluate the reliability of our
findings.</p>
<p><strong>Distinction Between Outliers and Influential
Studies</strong></p>
<p>Outliers and influential studies are related but not identical:</p>
<ul>
<li>Outliers are identified by the <strong>magnitude</strong> of their
effect size—i.e., their result is extreme compared to the pooled
average.</li>
<li>Influential studies are those that significantly <strong>impact the
meta-analysis results</strong>, such as the pooled effect size or
heterogeneity, <strong>regardless</strong> of their effect size
value.</li>
</ul>
<p>A study can be an outlier without being influential (if it does not
change the overall result much), and vice versa. However, in practice,
some outliers are indeed influential.</p>
<p><strong>Methodology: Leave-One-Out Analysis</strong></p>
<p>The most common way to detect influential studies is through the
<strong>leave-one-out method</strong>. This method involves repeating
the meta-analysis multiple times, each time leaving out one study from
the dataset. If there are <code>K</code> studies, the meta-analysis is
run <code>K</code> times.</p>
<p>Each of these iterations provides an updated pooled effect estimate.
By comparing how the overall results change when each study is excluded,
we can evaluate which studies have the most influence.</p>
<p><strong>Influence Diagnostics</strong></p>
<p>The results from leave-one-out analyses can be used to compute
several diagnostic metrics. These include:</p>
<ul>
<li>The change in the pooled effect size</li>
<li>The change in heterogeneity (I²)</li>
<li>Measures like DFBETAs and Cook’s distance, similar to regression
diagnostics</li>
</ul>
<p>These metrics help us systematically assess the influence of each
individual study.</p>
<p><strong>Using the <code>InfluenceAnalysis()</code> Function from
dmetar</strong></p>
<p>The <code>InfluenceAnalysis()</code> function from the
<code>dmetar</code> R package makes it easy to perform influence
diagnostics for any meta-analysis object created using functions from
the <code>meta</code> package.</p>
<pre class="r"><code>m.gen.inf &lt;- InfluenceAnalysis(m.gen, random = TRUE)</code></pre>
<pre><code>## [===========================================================================] DONE</code></pre>
<pre class="r"><code>m.gen.inf</code></pre>
<pre><code>## Leave-One-Out Analysis (Sorted by I2) 
##  ----------------------------------- 
##                                 Effect  LLCI  ULCI    I2
## Omitting DanitzOrsillo           0.507 0.349 0.666 0.481
## Omitting Shapiro et al.          0.521 0.344 0.699 0.546
## Omitting de Vibe et al.          0.608 0.404 0.811 0.576
## Omitting Kang et al.             0.542 0.349 0.735 0.598
## Omitting Kuhlmann et al.         0.606 0.405 0.806 0.614
## Omitting Hintz et al.            0.601 0.391 0.811 0.636
## Omitting Gallego et al.          0.572 0.359 0.784 0.638
## Omitting Call et al.             0.574 0.362 0.786 0.642
## Omitting Frogeli et al.          0.579 0.364 0.793 0.644
## Omitting Cavanagh et al.         0.596 0.384 0.808 0.645
## Omitting Song &amp; Lindquist        0.580 0.366 0.793 0.646
## Omitting Frazier et al.          0.595 0.380 0.809 0.647
## Omitting Lever Taylor et al.     0.593 0.381 0.805 0.647
## Omitting Warnecke et al.         0.580 0.367 0.794 0.647
## Omitting Hazlett-Stevens &amp; Oren  0.585 0.371 0.800 0.648
## Omitting Phang et al.            0.584 0.371 0.797 0.648
## Omitting Rasanen et al.          0.590 0.378 0.802 0.648
## Omitting Ratanasiripong          0.583 0.372 0.794 0.648
## 
## 
## Influence Diagnostics 
##  ------------------- 
##                                 rstudent dffits cook.d cov.r QE.del   hat
## Omitting Call et al.               0.332  0.040  0.002 1.119 44.706 0.050
## Omitting Cavanagh et al.          -0.647 -0.209  0.047 1.145 45.066 0.063
## Omitting DanitzOrsillo             3.211  0.914  0.643 0.655 30.819 0.038
## Omitting de Vibe et al.           -1.377 -0.368  0.124 1.019 37.742 0.079
## Omitting Frazier et al.           -0.489 -0.190  0.040 1.184 45.317 0.073
## Omitting Frogeli et al.            0.136 -0.018  0.000 1.165 44.882 0.063
## Omitting Gallego et al.            0.396  0.059  0.004 1.129 44.260 0.057
## Omitting Hazlett-Stevens &amp; Oren   -0.148 -0.092  0.009 1.166 45.447 0.060
## Omitting Hintz et al.             -0.899 -0.269  0.076 1.120 44.006 0.069
## Omitting Kang et al.               1.699  0.421  0.162 0.906 39.829 0.039
## Omitting Kuhlmann et al.          -1.448 -0.338  0.107 1.007 41.500 0.063
## Omitting Lever Taylor et al.      -0.520 -0.172  0.032 1.142 45.336 0.056
## Omitting Phang et al.             -0.107 -0.076  0.006 1.149 45.439 0.053
## Omitting Rasanen et al.           -0.400 -0.139  0.021 1.137 45.456 0.051
## Omitting Ratanasiripong           -0.143 -0.065  0.004 1.103 45.493 0.037
## Omitting Shapiro et al.            2.460  0.718  0.416 0.754 35.207 0.042
## Omitting Song &amp; Lindquist          0.084 -0.029  0.001 1.152 45.146 0.057
## Omitting Warnecke et al.           0.049 -0.036  0.001 1.143 45.263 0.052
##                                 weight infl
## Omitting Call et al.             5.036     
## Omitting Cavanagh et al.         6.267     
## Omitting DanitzOrsillo           3.751    *
## Omitting de Vibe et al.          7.880     
## Omitting Frazier et al.          7.337     
## Omitting Frogeli et al.          6.274     
## Omitting Gallego et al.          5.703     
## Omitting Hazlett-Stevens &amp; Oren  5.982     
## Omitting Hintz et al.            6.854     
## Omitting Kang et al.             3.860     
## Omitting Kuhlmann et al.         6.300     
## Omitting Lever Taylor et al.     5.586     
## Omitting Phang et al.            5.332     
## Omitting Rasanen et al.          5.086     
## Omitting Ratanasiripong          3.678     
## Omitting Shapiro et al.          4.165     
## Omitting Song &amp; Lindquist        5.664     
## Omitting Warnecke et al.         5.246     
## 
## 
## Baujat Diagnostics (sorted by Heterogeneity Contribution) 
##  ------------------------------------------------------- 
##                                 HetContrib InfluenceEffectSize
## Omitting DanitzOrsillo              14.385               0.298
## Omitting Shapiro et al.             10.044               0.251
## Omitting de Vibe et al.              6.403               1.357
## Omitting Kang et al.                 5.552               0.121
## Omitting Kuhlmann et al.             3.746               0.256
## Omitting Hintz et al.                1.368               0.129
## Omitting Gallego et al.              1.183               0.060
## Omitting Call et al.                 0.768               0.028
## Omitting Frogeli et al.              0.582               0.039
## Omitting Cavanagh et al.             0.409               0.027
## Omitting Song &amp; Lindquist            0.339               0.017
## Omitting Warnecke et al.             0.230               0.009
## Omitting Frazier et al.              0.164               0.021
## Omitting Lever Taylor et al.         0.159               0.008
## Omitting Phang et al.                0.061               0.003
## Omitting Hazlett-Stevens &amp; Oren      0.052               0.003
## Omitting Rasanen et al.              0.044               0.002
## Omitting Ratanasiripong              0.010               0.000</code></pre>
<p><strong>Generated Diagnostic Plots</strong></p>
<p>The function produces four different types of plots to visualize
influence:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Baujat Plot</strong> This plot displays studies based on
their contribution to heterogeneity (x-axis) and to the pooled effect
size (y-axis). Studies in the top right corner are both highly
heterogeneous and influential.</p></li>
<li><p><strong>Influence Diagnostics (Viechtbauer and Cheung)</strong> A
panel of diagnostic statistics like DFBETAs, Cook’s distance, and
covariance ratios is calculated and visualized. These are adapted from
influence analysis in regression models.</p></li>
<li><p><strong>Leave-One-Out Results (Sorted by Effect Size)</strong>
This plot shows how the overall pooled effect estimate changes when each
study is excluded, sorted by effect size.</p></li>
<li><p><strong>Leave-One-Out Results (Sorted by I²)</strong> This shows
how heterogeneity changes when each study is excluded, helping to
identify studies responsible for most of the between-study
variability.</p></li>
</ol>
<pre class="r"><code>plot(m.gen.inf, type = &quot;baujat&quot;)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>plot(m.gen.inf, type = &quot;influence&quot;)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
<pre class="r"><code>plot(m.gen.inf, type = &quot;es.id&quot;)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-8-3.png" width="672" /></p>
<pre class="r"><code>plot(m.gen.inf, type = &quot;i2.id&quot;)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-8-4.png" width="672" /></p>
<p><strong>Interpreting Results</strong></p>
<ul>
<li>If the pooled effect size or I² changes significantly after
excluding a particular study, that study may be influential.</li>
<li>If several studies affect the results, it may indicate that your
meta-analysis is sensitive and that the pooled effect is not
robust.</li>
<li>These findings can be used in sensitivity analyses and discussed in
the results section of your report or publication.</li>
</ul>
</div>
<div id="gosh-plot-analysis" class="section level3" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> GOSH Plot
Analysis</h3>
<p>After conducting leave-one-out influence analyses, another powerful
approach to exploring heterogeneity in a meta-analysis is the Graphic
Display of Heterogeneity (GOSH) plot, introduced by Olkin, Dahabreh, and
Trikalinos (2012). A GOSH plot visualizes the effect size and
heterogeneity across all possible subsets of included studies. Unlike
leave-one-out methods, which generate K models (one per study excluded),
the GOSH approach fits 2^K − 1 meta-analysis models, corresponding to
every possible subset of studies.</p>
<p>Due to the computational intensity of fitting this many models, the R
implementation limits the number of combinations to a maximum of one
million randomly selected subsets. Once calculated, the effect sizes are
plotted on the x-axis, and the corresponding heterogeneity statistics
(typically I²) on the y-axis. The distribution of these points can
indicate clusters or patterns. For example, the presence of several
distinct clusters may suggest the existence of different subpopulations
in the dataset.</p>
<p>To use GOSH plots in R, the {metafor} package is required.
Additionally, since GOSH relies on {metafor}, the original {meta}
analysis object must be translated into a format accepted by the rma
function from {metafor}. The required arguments are the effect sizes
(TE), their standard errors (seTE), and the between-study heterogeneity
estimator method.tau. Knapp-Hartung adjustments can also be
specified.</p>
<pre class="r"><code>## save the newly generated {metafor}-based meta-analysis under the name m.rma.
m.rma &lt;- rma(yi = m.gen$TE,
             sei = m.gen$seTE,
             method = m.gen$method.tau,
             test = &quot;knha&quot;)
res.gosh &lt;- gosh(m.rma)
plot(res.gosh, alpha = 0.01)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>To investigate which studies contribute to clusters in the GOSH plot,
use the gosh.diagnostics function from the {dmetar} package. This
function applies three clustering algorithms: k-means, DBSCAN
(density-based clustering), and Gaussian Mixture Models. It highlights
studies that frequently appear in specific clusters—especially those
associated with high heterogeneity or large influence.</p>
<pre class="r"><code>res.gosh.diag &lt;- gosh.diagnostics(res.gosh, 
                                  km.params = list(centers = 2),
                                  db.params = list(eps = 0.08, 
                                                   MinPts = 50))</code></pre>
<pre><code>##   
##  Perform Clustering... 
##  |==========================================================================================| DONE</code></pre>
<pre class="r"><code>res.gosh.diag</code></pre>
<pre><code>## GOSH Diagnostics 
## ================================ 
## 
##  - Number of K-means clusters detected: 2
##  - Number of DBSCAN clusters detected: 4
##  - Number of GMM clusters detected: 7
## 
##  Identification of potential outliers 
##  --------------------------------- 
## 
##  - K-means: Study 3, Study 16
##  - DBSCAN: Study 3, Study 4, Study 16
##  - Gaussian Mixture Model: Study 3, Study 4, Study 11, Study 16</code></pre>
<p><strong>Sensitivity Analysis After GOSH Diagnostics</strong></p>
<p>To assess the impact of removing potentially influential studies
(e.g., study 3, 4, and 16), rerun the meta-analysis without them:</p>
<p>update(m.gen, exclude = c(3, 4, 16)) %&gt;% summary()</p>
<p>In the example given, the revised meta-analysis showed:</p>
<ul>
<li>A reduced I² of 4.6%, indicating much lower between-study
heterogeneity.</li>
<li>A slightly smaller pooled effect size (g = 0.48 vs. 0.58).</li>
<li>Similar statistical significance, suggesting robustness of the
result.</li>
</ul>
<p><strong>Reporting Influence Analysis Results</strong></p>
<p>When studies are identified as influential, report both the original
results and a sensitivity analysis excluding those studies. A table is
an effective format:</p>
<table>
<colgroup>
<col width="31%" />
<col width="6%" />
<col width="14%" />
<col width="10%" />
<col width="15%" />
<col width="4%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th>Analysis</th>
<th>g</th>
<th>95% CI</th>
<th>p-value</th>
<th>95% PI</th>
<th>I²</th>
<th>95% CI (I²)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Main Analysis</td>
<td>0.58</td>
<td>0.38–0.78</td>
<td>&lt;0.001</td>
<td>–0.06–1.22</td>
<td>63%</td>
<td>39–78</td>
</tr>
<tr class="even">
<td>Infl. Cases Removed¹</td>
<td>0.48</td>
<td>0.36–0.60</td>
<td>&lt;0.001</td>
<td>0.36–0.61</td>
<td>5%</td>
<td>0–56</td>
</tr>
</tbody>
</table>
<p>¹Removed as outliers: DanitzOrsillo, de Vibe, Shapiro</p>
<p>This kind of presentation ensures transparency and clearly shows the
robustness of the meta-analytic conclusions. Additional rows can be
added for other sensitivity analyses, such as risk-of-bias
restrictions.</p>
</div>
</div>
</div>
<div id="continuous-outcomes-using-fixed-effect-model"
class="section level1" number="3">
<h1><span class="header-section-number">3</span> Continuous Outcomes
using Fixed Effect Model</h1>
<div id="effect-measures" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Effect Measures</h2>
<p>Meta-analysis typically focuses on comparing two interventions, which
we refer to as experimental and control. When the response is continuous
(i.e. quantitative) typically the mean, standard deviation and sample
size are reported for each group.</p>
<p>Suppose the goal is to evaluate whether the two groups differ in
terms of their population means. Let:</p>
<ul>
<li><strong>µ₁</strong> = true mean of the Treated group</li>
<li><strong>µ₂</strong> = true mean of the Control group</li>
<li><strong>Δ = µ₁ − µ₂</strong> = difference in population means (also
called the <strong>mean difference</strong>)</li>
<li><strong>δ = (µ₁ − µ₂)/σ</strong> = standardized mean difference (or
<strong>effect size</strong>), where <strong>σ</strong> is a standard
deviation (either pooled or from the control group)</li>
</ul>
<p><strong>Summary</strong></p>
<ul>
<li>Use raw <strong>mean differences</strong> (D) when outcome scales
are consistent across studies</li>
<li>Use <strong>standardized mean differences</strong> (δ, Cohen’s d,
Hedges’ g) when outcome scales differ</li>
<li>Hedges’ g is preferred in meta-analysis due to its bias
correction</li>
<li>Statistical inference (Z-tests, confidence intervals) depends on the
variance of the effect size estimate</li>
<li>Correction factors and variance formulas vary slightly in the
literature but typically lead to very similar results for reasonably
sized samples</li>
</ul>
<div id="estimating-the-mean-difference-δ" class="section level3"
number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Estimating the Mean
Difference (Δ)</h3>
<p>When all studies in the meta-analysis report outcomes using the same
scale or unit, we can directly compute and combine the raw <strong>mean
differences</strong>.</p>
<p>For a single study:</p>
<ul>
<li>Let <strong>X̄₁</strong> and <strong>X̄₂</strong> be the sample means
of the Treated and Control groups, respectively</li>
<li>Let <strong>S₁</strong> and <strong>S₂</strong> be the corresponding
sample standard deviations</li>
<li>Let <strong>n₁</strong> and <strong>n₂</strong> be the sample sizes
of the two groups</li>
</ul>
<p>Then, the sample mean difference <strong>D</strong> is estimated
by:</p>
<p><strong>D = X̄₁ − X̄₂</strong></p>
<p>To compute confidence intervals or perform hypothesis testing, we
need the <strong>variance</strong> of D.</p>
<p>There are two cases:</p>
<p><strong>Case 1: Unequal Variances (Heteroscedasticity)</strong></p>
<p>Assume the two groups have different variances (σ₁² ≠ σ₂²). Then the
variance of D is:</p>
<p><strong>Var(D) = (S₁² / n₁) + (S₂² / n₂)</strong></p>
<p><strong>Case 2: Equal Variances (Homoscedasticity)</strong></p>
<p>If variances are assumed equal (σ₁ = σ₂ = σ), a <strong>pooled
variance</strong> is used:</p>
<p><strong>S²_pooled = [ (n₁ − 1)S₁² + (n₂ − 1)S₂² ] / (n₁ + n₂ −
2)</strong></p>
<p>Then, the variance of D is:</p>
<p><strong>Var(D) = (n₁ + n₂) / (n₁n₂) × S²_pooled</strong></p>
<p>The <strong>standard error</strong> of D (SED) is simply the square
root of its variance:</p>
<p><strong>SED = √Var(D)</strong></p>
<p>In meta-analysis, we combine the D estimates from multiple studies,
weighting them by the inverse of their variances.</p>
<pre class="r"><code># use the metacont function to calculate mean difference and confidence interval
# sm=&quot;MD&quot; (i.e. summary measure is the Mean Difference) as default setting.
data(Fleiss1993cont) 
m1_MD &lt;- metacont(n.psyc, mean.psyc, sd.psyc, n.cont, mean.cont, sd.cont,
                  data = Fleiss1993cont, 
                  studlab=rownames(Fleiss1993cont),
                  sm = &quot;MD&quot;)
summary(m1_MD)</code></pre>
<pre><code>##        MD             95%-CI %W(common) %W(random)
## 1 -1.5000 [-4.7855;  1.7855]        2.8        5.2
## 2 -1.2000 [-2.0837; -0.3163]       38.6       33.3
## 3 -2.4000 [-6.1078;  1.3078]        2.2        4.1
## 4  0.2000 [-0.7718;  1.1718]       31.9       30.6
## 5 -0.8800 [-1.9900;  0.2300]       24.5       26.8
## 
## Number of studies: k = 5
## Number of observations: o = 232 (o.e = 106, o.c = 126)
## 
##                           MD             95%-CI     z p-value
## Common effect model  -0.7094 [-1.2585; -0.1603] -2.53  0.0113
## Random effects model -0.7509 [-1.5328;  0.0311] -1.88  0.0598
## 
## Quantifying heterogeneity (with 95%-CIs):
##  tau^2 = 0.2742 [0.0000; 5.8013]; tau = 0.5236 [0.0000; 2.4086]
##  I^2 = 29.3% [0.0%; 72.6%]; H = 1.19 [1.00; 1.91]
## 
## Test of heterogeneity:
##     Q d.f. p-value
##  5.66    4  0.2260
## 
## Details of meta-analysis methods:
## - Inverse variance method
## - Restricted maximum-likelihood estimator for tau^2
## - Q-Profile method for confidence interval of tau^2 and tau
## - Calculation of I^2 based on Q</code></pre>
<pre class="r"><code>plot(m1_MD)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div id="estimating-the-standardized-mean-difference-δ"
class="section level3" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Estimating the
Standardized Mean Difference (δ)</h3>
<p>When different studies use <strong>different measurement
scales</strong>, it’s not meaningful to compare or pool raw mean
differences directly. Instead, we use the <strong>standardized mean
difference (SMD)</strong>, which removes scale effects.</p>
<p>SMD is defined as:</p>
<p><strong>δ = (µ₁ − µ₂) / σ</strong></p>
<p>where <strong>σ</strong> is a standard deviation, either from the
control group or pooled from both.</p>
<hr />
<p><strong>Two Common Estimators of δ:</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Cohen’s d</strong> (proposed by Cohen, 1988)</li>
</ol>
<p>This is calculated as:</p>
<p><strong>d = (X̄₁ − X̄₂) / S</strong></p>
<p>Where <strong>S</strong> is the pooled standard deviation:</p>
<p><strong>S² = [ (n₁ − 1)S₁² + (n₂ − 1)S₂² ] / (n₁ + n₂)</strong></p>
<p>Then, <strong>S = √S²</strong></p>
<p>Note: Cohen’s d slightly <strong>overestimates</strong> the true δ
when sample sizes are small.</p>
<hr />
<ol start="2" style="list-style-type: decimal">
<li><strong>Hedges’ g</strong> (proposed by Hedges, 1982)</li>
</ol>
<p>This is a corrected version of Cohen’s d for small sample bias:</p>
<p><strong>g = (X̄₁ − X̄₂) / S∗</strong></p>
<p>Where:</p>
<p><strong>S∗² = [ (n₁ − 1)S₁² + (n₂ − 1)S₂² ] / (n₁ + n₂ − 2)</strong>
This is the traditional <strong>pooled sample variance</strong>. Taking
the square root gives <strong>S∗</strong>.</p>
<p>However, <strong>g is biased</strong>, and this bias can be corrected
using a <strong>correction factor J</strong>:</p>
<p><strong>g∗ = J × g</strong>, where <strong>J = 1 − (3 / (4N −
9))</strong> and <strong>N = n₁ + n₂</strong></p>
<p>Then <strong>g∗</strong> is an approximately <strong>unbiased
estimate</strong> of δ.</p>
<p>An approximate formula for the variance of g∗ is:</p>
<p><strong>Var(g∗) ≈ (1 / ñ) + (g∗² / (2(N − 3.94)))</strong></p>
<p>Where:</p>
<p><strong>ñ = (n₁ × n₂) / (n₁ + n₂)</strong> is the <strong>harmonic
mean</strong> of sample sizes</p>
<p>This variance formula is compatible with the <strong>R
<code>meta</code> package</strong>, although other literature might use
alternatives such as 2N, 2(N − 2), etc. These make very little
difference unless <strong>n₁ and n₂ are very small</strong>.</p>
<p><strong>Hypothesis Testing for Effect Size δ</strong></p>
<p>To test <strong>H₀: δ = 0</strong> (no effect) versus <strong>H₁: δ ≠
0</strong>, we use a <strong>Z-statistic</strong>:</p>
<p><strong>Z = g∗ / √Var(g∗)</strong></p>
<p>We reject the null hypothesis if |Z| exceeds the critical value from
the standard normal distribution (typically zₐ/2 = 1.96 for a 95%
confidence level).</p>
<p>A <strong>confidence interval</strong> for δ can be constructed
as:</p>
<p><strong>CI = g∗ ± zₐ/2 × √Var(g∗)</strong></p>
<blockquote>
<p>There is a proportional relationship:</p>
</blockquote>
<p><strong>d = (n₁ + n₂) / (n₁ + n₂ − 2) × g = (n₁ + n₂) / (n₁ + n₂ − 2)
× g∗ / J</strong></p>
<ul>
<li>use the metacont function to calculate mean difference and
confidence interval</li>
</ul>
<pre class="r"><code># use the metacont function to calculate mean difference and confidence interval
# sm=&quot;MD&quot; (i.e. summary measure is the Mean Difference) as default setting.
data(Fleiss1993cont) 
m1_SMD &lt;- metacont(n.psyc, mean.psyc, sd.psyc, n.cont, mean.cont, sd.cont,
                   data = Fleiss1993cont, 
                   studlab=rownames(Fleiss1993cont),
                   sm = &quot;SMD&quot;)
data.frame(
  Study      = m1_SMD$studlab,
  SMD        = round(m1_SMD$TE, 3),
  SE         = round(m1_SMD$seTE, 3),
  CI_lower   = round(m1_SMD$lower, 3),
  CI_upper   = round(m1_SMD$upper, 3)
)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Study"],"name":[1],"type":["chr"],"align":["left"]},{"label":["SMD"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["SE"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["CI_lower"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["CI_upper"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"1","2":"-0.340","3":"0.396","4":"-1.115","5":"0.435"},{"1":"2","2":"-0.566","3":"0.235","4":"-1.027","5":"-0.104"},{"1":"3","2":"-0.300","3":"0.240","4":"-0.771","5":"0.171"},{"1":"4","2":"0.125","3":"0.317","4":"-0.495","5":"0.745"},{"1":"5","2":"-0.734","3":"0.522","4":"-1.757","5":"0.288"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r"><code>m1_SMD</code></pre>
<pre><code>## Number of studies: k = 5
## Number of observations: o = 232 (o.e = 106, o.c = 126)
## 
##                          SMD             95%-CI     z p-value
## Common effect model  -0.3434 [-0.6068; -0.0801] -2.56  0.0106
## Random effects model -0.3434 [-0.6068; -0.0801] -2.56  0.0106
## 
## Quantifying heterogeneity (with 95%-CIs):
##  tau^2 = 0 [0.0000; 0.7255]; tau = 0 [0.0000; 0.8518]
##  I^2 = 0.0% [0.0%; 79.2%]; H = 1.00 [1.00; 2.19]
## 
## Test of heterogeneity:
##     Q d.f. p-value
##  3.68    4  0.4514
## 
## Details of meta-analysis methods:
## - Inverse variance method
## - Restricted maximum-likelihood estimator for tau^2
## - Q-Profile method for confidence interval of tau^2 and tau
## - Calculation of I^2 based on Q
## - Hedges&#39; g (bias corrected standardised mean difference; using exact formulae)</code></pre>
<pre class="r"><code>plot(m1_SMD)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre class="r"><code># Use Cohen&#39;s d instead of Hedges&#39; g as effect measure
update(m1_SMD, method.smd = &quot;Cohen&quot;)</code></pre>
<pre><code>## Number of studies: k = 5
## Number of observations: o = 232 (o.e = 106, o.c = 126)
## 
##                          SMD             95%-CI     z p-value
## Common effect model  -0.3499 [-0.6133; -0.0866] -2.60  0.0092
## Random effects model -0.3497 [-0.6147; -0.0847] -2.59  0.0097
## 
## Quantifying heterogeneity (with 95%-CIs):
##  tau^2 = 0.0009 [0.0000; 0.7958]; tau = 0.0297 [0.0000; 0.8921]
##  I^2 = 0.0% [0.0%; 79.2%]; H = 1.00 [1.00; 2.19]
## 
## Test of heterogeneity:
##     Q d.f. p-value
##  3.87    4  0.4242
## 
## Details of meta-analysis methods:
## - Inverse variance method
## - Restricted maximum-likelihood estimator for tau^2
## - Q-Profile method for confidence interval of tau^2 and tau
## - Calculation of I^2 based on Q
## - Cohen&#39;s d (standardised mean difference; using exact formulae)</code></pre>
</div>
</div>
<div id="inverse-variance-weighted-average-method"
class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Inverse
Variance-Weighted Average Method</h2>
<p>There are two methods for the fixed effects model meta-analysis: the
IVW and weighted SZ. The fixed effects model assumes that all studies in
a meta-analysis share a single true effect size.</p>
<p>固定效应模型假设荟萃分析中成分研究 component studies
的估计效应来自单个同质群体 single homogeneous populatio。
因此，为了计算总体估算值，我们对每项研究的估算值取平均值，从而考虑到某些估算值比其他估算值更为精确的事实（来自较大的研究）</p>
<p>More formally, let <span class="math inline">\(k=1, \ldots,
K\)</span> index study, <span
class="math inline">\(\hat{\theta}_{k}\)</span> denote the intervention
effect estimate from study <span class="math inline">\(k\)</span>, and
<span class="math inline">\(\theta\)</span> denote the intervention
effect in the population, which we wish to estimate. Denote by <span
class="math inline">\(\hat{\sigma}_{k}^{2}\)</span> the sample estimate
of <span
class="math inline">\(\operatorname{Var}\left(\hat{\theta}_{k}\right)\)</span>.
The fixed effect model is <span class="math display">\[
\hat{\theta}_{k}=\theta+\sigma_{k} \epsilon_{k}, \quad
\epsilon_{k}^{\mathrm{i} . \mathrm{i} . \mathrm{d} .} N(0,1)
\]</span> We now consider the fixed effect estimate of <span
class="math inline">\(\theta\)</span>, denoted by <span
class="math inline">\(\hat{\theta}_{F}\)</span>. Given estimates <span
class="math inline">\(\left(\hat{\theta}_{k}, \hat{\sigma}_{k}\right),
k=1, \ldots, K\)</span>, the maximum-likelihood estimate under model
(2.7) is <span class="math display">\[
\hat{\theta}_{F}=\frac{\sum_{k=1}^{K} \hat{\theta}_{k} /
\hat{\sigma}_{k}^{2}}{\sum_{k=1}^{K} 1 /
\hat{\sigma}_{k}^{2}}=\frac{\sum_{k=1}^{K} w_{k}
\hat{\theta}_{k}}{\sum_{k=1}^{K} w_{k}}
\]</span> Accordingly, <span
class="math inline">\(\hat{\theta}_{F}\)</span> is a weighted average of
the individual effect estimates <span
class="math inline">\(\hat{\theta}_{k}\)</span> with weights <span
class="math inline">\(w_{k}=1 / \hat{\sigma}_{k}^{2}\)</span>.
Therefore, this method is called the <strong>inverse variance
method</strong>. The variance of <span
class="math inline">\(\hat{\theta}_{F}\)</span> is estimated by <span
class="math display">\[
\widehat{\operatorname{Var}}\left(\hat{\theta}_{F}\right)=\frac{1}{\sum_{k=1}^{K}
w_{k}}
\]</span> <span class="math inline">\((1-\alpha)\)</span> confidence
interval for <span class="math inline">\(\hat{\theta}_{F}\)</span> can
be calculated by <span class="math display">\[\hat{\theta}_{F} \pm
z_{1-\frac{\alpha}{2}} \text { SE
}\left(\hat{\theta}_{F}\right)\]</span></p>
<pre class="r"><code># The fixed effect estimate and its variance can be calculated using base R code

# 1. Calculate mean difference, variance and weights
MD &lt;- with(data1, Me - Mc)
varMD &lt;- with(data1, Se^2/Ne + Sc^2/Nc)
weight &lt;- 1/varMD

# 2. Calculate the inverse variance estimator
# the standard weighted.mean function is used to calculate theta_F.
round(weighted.mean(MD, weight), 4)</code></pre>
<pre><code>## [1] -15.514</code></pre>
<pre class="r"><code># 3. Calculate the variance
round(1/sum(weight), 4)</code></pre>
<pre><code>## [1] 1.4126</code></pre>
<pre class="r"><code># Alternative easier using the metacont function which yields identical results
mc1 &lt;- metacont(Ne, Me, Se, Nc, Mc, Sc,
                data=data1,
                studlab=paste(author, year))
round(c(mc1$TE.fixed, mc1$seTE.fixed^2), 4)</code></pre>
<pre><code>## [1] -15.5140   1.4126</code></pre>
<pre class="r"><code># Forest Plot
# pdf(file=&quot;Schwarzer-Fig2.3.pdf&quot;, width=9.6) 
# uncomment line to generate PDF file
forest(mc1, comb.random=FALSE, xlab=
         &quot;Difference in mean response (intervention - control)
units: maximum % fall in FEV1&quot;,
       xlim=c(-50,10), xlab.pos=-20, smlab.pos=-20)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre class="r"><code># invisible(dev.off()) 
# uncomment line to save PDF file</code></pre>
</div>
<div id="generic-inverse-variance-meta-analysis-metagen"
class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Generic inverse
variance meta-analysis <code>metagen</code></h2>
<p>Fixed effect and random effects meta-analysis based on estimates
(e.g. log hazard ratios) and their standard errors. The inverse variance
method is used for pooling.</p>
<ul>
<li>sm: A character string indicating underlying summary measure, e.g.,
“RD”, “RR”, “OR”, “ASD”, “HR”, “MD”, “SMD”, or “ROM”.</li>
<li>Confidence intervals for individual studies: For the mean difference
(argument sm = “MD”), the confidence interval for individual studies can
be based on the
<ul>
<li>standard normal distribution (method.ci = “z”), or</li>
<li>t-distribution (method.ci = “t”).</li>
</ul></li>
<li>Estimation of between-study variance:
<ul>
<li><code>method.tau = "DL"</code> DerSimonian-Laird estimator
(DerSimonian and Laird, 1986)</li>
<li><code>method.tau = "PM"</code> Paule-Mandel estimator (Paule and
Mandel, 1982)</li>
<li><code>method.tau = "REML"</code> Restricted maximum-likelihood
estimator (Viechtbauer, 2005)</li>
<li><code>method.tau = "ML"</code> Maximum-likelihood estimator
(Viechtbauer, 2005)</li>
<li><code>method.tau = "HS"</code> Hunter-Schmidt estimator (Hunter and
Schmidt, 2015)</li>
<li><code>method.tau = "SJ"</code> Sidik-Jonkman estimator (Sidik and
Jonkman, 2005)</li>
<li><code>method.tau = "HE"</code> Hedges estimator (Hedges and Olkin,
1985)</li>
<li><code>method.tau = "EB"</code> Empirical Bayes estimator (Morris,
1983)</li>
</ul></li>
<li>Confidence interval for the between-study variance:
<ul>
<li><code>method.tau.ci = "J"</code> Method by Jackson (2013)</li>
<li><code>method.tau.ci = "BJ"</code> Method by Biggerstaff and Jackson
(2008)</li>
<li><code>method.tau.ci = "QP"</code> Q-Profile method (Viechtbauer,
2007)</li>
<li><code>method.tau.ci = "PL"</code> Profile-Likelihood method for
three-level meta-analysis model (Van den Noortgate et al., 2013)</li>
</ul></li>
</ul>
<pre class="r"><code>mc1.gen &lt;- metagen(TE, seTE, data=mc1, sm=&quot;MD&quot;)
# Print results for fixed effect and random effects method
c(mc1$TE.fixed, mc1$TE.random)</code></pre>
<pre><code>## [1] -15.51403 -15.64702</code></pre>
<pre class="r"><code>c(mc1.gen$TE.fixed, mc1.gen$TE.random)</code></pre>
<pre><code>## [1] -15.51403 -15.64702</code></pre>
</div>
<div id="weighted-sum-of-z-scores" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Weighted Sum of
Z-Scores</h2>
<p>Another popular method for the fixed effects model meta-analysis is
calculating the weighted SZ from the follows studies. Let <span
class="math inline">\(Z_{i}\)</span> be the z-score from study <span
class="math inline">\(i\)</span>, which <span
class="math inline">\(N(0,1)\)</span> under the null hypothesis of no
effects. Then, the weighted SZ statistic is <span
class="math display">\[
Z_{S Z}=\frac{\sum w_{S Z, i} Z_{i}}{\sqrt{\sum w_{S Z, i}{ }^{2}}}
\]</span> By the characteristic of a normal distribution, <span
class="math inline">\(\mathrm{Z}_{\mathrm{SZ}}\)</span> also follows
<span class="math inline">\(N(0,1)\)</span> under the null hypothesis.
To combine z-scores from multiple studies, a per-study sample size was
suggested as weights of each study, as follows: <span
class="math display">\[
w_{S Z, i}=\sqrt{N_{i}}
\]</span></p>
</div>
</div>
<div id="continuous-outcomes-using-random-effects-model"
class="section level1" number="4">
<h1><span class="header-section-number">4</span> Continuous Outcomes
using Random Effects Model</h1>
<div id="introduction-1" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Introduction</h2>
<p>The random effects model seeks to account for the fact that the study
effect estimates <span class="math inline">\(\hat{\theta}_{k}\)</span>
are often more variable than assumed in the fixed effect model. Under
the random effects model, <span class="math display">\[
\hat{\theta}_{k}=\theta+u_{k}+\sigma_{k} \epsilon_{k}, \quad
\epsilon_{k}^{\text {i.i.d. }} N(0,1) ; u_{k}^{\text {i.i.d. }}
N\left(0, \tau^{2}\right)
\]</span> where the <span class="math inline">\(u\)</span> ’s and <span
class="math inline">\(\epsilon\)</span> ’s are independent. Define <span
class="math display">\[
Q=\sum_{k=1}^{K} w_{k}\left(\hat{\theta}_{k}-\hat{\theta}_{F}\right)^{2}
\]</span> the weighted sum of squares about the fixed effect estimate
with <span class="math inline">\(w_{k}=1 /
\hat{\sigma}_{k}^{2}\)</span>. This is usually referred to as either the
homogeneity test statistic or the heterogeneity statistic. Next define
<span class="math display">\[
S=\sum_{k=1}^{K} w_{k}-\frac{\sum_{k=1}^{K} w_{k}^{2}}{\sum_{k=1}^{K}
w_{k}}
\]</span> If <span class="math inline">\(Q&lt;(K-1)\)</span>, then <span
class="math inline">\(\hat{\tau}^{2}\)</span> is set to 0 and the random
effects estimate <span class="math inline">\(\hat{\theta}_{R}\)</span>
is set equal to the fixed effect estimate <span
class="math inline">\(\hat{\theta}_{F}\)</span>. Otherwise, the
<strong>Dersimonian-Laird estimator of the between-study
variance</strong> is defined as <span class="math display">\[
\hat{\tau}^{2}=\frac{Q-(K-1)}{S}
\]</span> and the random effects estimate and its variance are given by
<span class="math display">\[
\begin{array}{c}
\hat{\theta}_{R}=\frac{\sum_{k=1}^{K} w_{k}^{*}
\hat{\theta}_{k}}{\sum_{k=1}^{K} w_{k}^{*}} \\
\operatorname{Var}\left(\hat{\theta}_{R}\right)=\frac{1}{\sum_{k=1}^{K}
w_{k}^{*}}
\end{array}
\]</span> with weights <span class="math inline">\(w_{k}^{*}=1
/\left(\hat{\sigma}_{k}^{2}+\hat{\tau}^{2}\right)\)</span>. The random
effects estimator <span class="math inline">\(\hat{\theta}_{R}\)</span>
is a weighted average of the individual effect estimates <span
class="math inline">\(\hat{\theta}_{k}\)</span> with weights <span
class="math inline">\(1
/\left(\hat{\sigma}_{k}^{2}+\hat{\tau}^{2}\right)\)</span>. Accordingly,
this method is often called “<strong>Inverse variance method</strong>”,
too. A <span class="math inline">\((1-\alpha)\)</span> confidence
interval for <span class="math inline">\(\hat{\theta}_{R}\)</span> can
be calculated by <span class="math display">\[\hat{\theta}_{R} \pm
z_{1-\frac{\alpha}{2}} \text { S.E.
}\left(\hat{\theta}_{R}\right)\]</span></p>
</div>
<div id="implementation" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Implementation</h2>
<p><strong>Estimation of Between-Study Variance</strong></p>
<ul>
<li>DerSimonian–Laird estimator (method.tau=“DL”) (default)</li>
<li>Paule–Mandel estimator (method.tau=“PM”)</li>
<li>Restricted maximum-likelihood estimator (method.tau=“REML”)</li>
<li>Maximum-likelihood estimator (method.tau=“ML”)</li>
<li>Hunter–Schmidt estimator (method.tau=“HS”)</li>
<li>Sidik–Jonkman estimator (method.tau=“SJ”)</li>
<li>Hedges estimator (method.tau=“HE”)</li>
<li>Empirical Bayes estimator (method.tau=“EB”).</li>
</ul>
<pre class="r"><code># 1. Conduct meta-analyses
# 1a. DerSimonian-Laird estimator (default)
mg1.DL &lt;- metagen(TE, seTE, data=mc1)
# 1b. Paule-Mandel estimator
mg1.PM &lt;- metagen(TE, seTE, data=mc1, method.tau=&quot;PM&quot;)
# 1c. Restricted maximum-likelihood estimator
mg1.RM &lt;- metagen(TE, seTE, data=mc1, method.tau=&quot;REML&quot;)
# 1d. Maximum-likelihood estimator
mg1.ML &lt;- metagen(TE, seTE, data=mc1, method.tau=&quot;ML&quot;)
# 1e. Hunter-Schmidt estimator
mg1.HS &lt;- metagen(TE, seTE, data=mc1, method.tau=&quot;HS&quot;)
# 1f. Sidik-Jonkman estimator
mg1.SJ &lt;- metagen(TE, seTE, data=mc1, method.tau=&quot;SJ&quot;)
# 1g. Hedges estimator
mg1.HE &lt;- metagen(TE, seTE, data=mc1, method.tau=&quot;HE&quot;)
# 1h. Empirical Bayes estimator
mg1.EB &lt;- metagen(TE, seTE, data=mc1, method.tau=&quot;EB&quot;)
# 2. Extract between-study variance tau-squared
tau2 &lt;- data.frame(tau2=round(c(0,
                                mg1.DL$tau^2, mg1.PM$tau^2,
                                mg1.RM$tau^2, mg1.ML$tau^2,
                                mg1.HS$tau^2, mg1.SJ$tau^2,
                                mg1.HE$tau^2, mg1.EB$tau^2), 2),
                   row.names=c(&quot;FE&quot;, &quot;DL&quot;, &quot;PM&quot;, &quot;REML&quot;, &quot;ML&quot;,
                               &quot;HS&quot;, &quot;SJ&quot;, &quot;HE&quot;, &quot;EB&quot;))
# 3. Print tau-squared values
t(tau2)</code></pre>
<pre><code>##      FE   DL   PM REML   ML   HS    SJ HE   EB
## tau2  0 2.52 2.48 2.52 0.06 0.81 15.75  0 2.48</code></pre>
<pre class="r"><code># 4. Create dataset with summaries
res &lt;- data.frame(MD=c(mg1.DL$TE.fixed,
                       mg1.DL$TE.random, mg1.PM$TE.random,
                       mg1.RM$TE.random, mg1.ML$TE.random,
                       mg1.HS$TE.random, mg1.SJ$TE.random,
                       mg1.HE$TE.random, mg1.EB$TE.random),
                  seMD=c(mg1.DL$seTE.fixed,
                         mg1.DL$seTE.random, mg1.PM$seTE.random,
                         mg1.RM$seTE.random, mg1.ML$seTE.random,
                         mg1.HS$seTE.random, mg1.SJ$seTE.random,
                         mg1.HE$seTE.random, mg1.EB$seTE.random),
                  method=c(&quot;&quot;,
                           &quot;DerSimonian-Laird&quot;,
                           &quot;Paule-Mandel&quot;,
                           &quot;Restricted maximum-likelihood&quot;,
                           &quot;Maximum-likelihood&quot;,
                           &quot;Hunter-Schmidt&quot;,
                           &quot;Sidik-Jonkman&quot;,
                           &quot;Hedges&quot;,
                           &quot;Empirical Bayes&quot;),
                  tau2=tau2,
                  model=c(&quot;Fixed-effect model&quot;,
                          rep(&quot;Random-effect model&quot;, 8)))
knitr::kable(res)</code></pre>
<table>
<colgroup>
<col width="6%" />
<col width="12%" />
<col width="11%" />
<col width="37%" />
<col width="7%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">MD</th>
<th align="right">seMD</th>
<th align="left">method</th>
<th align="right">tau2</th>
<th align="left">model</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">FE</td>
<td align="right">-15.51403</td>
<td align="right">1.188538</td>
<td align="left"></td>
<td align="right">0.00</td>
<td align="left">Fixed-effect model</td>
</tr>
<tr class="even">
<td align="left">DL</td>
<td align="right">-15.64702</td>
<td align="right">1.274628</td>
<td align="left">DerSimonian-Laird</td>
<td align="right">2.52</td>
<td align="left">Random-effect model</td>
</tr>
<tr class="odd">
<td align="left">PM</td>
<td align="right">-15.64543</td>
<td align="right">1.273478</td>
<td align="left">Paule-Mandel</td>
<td align="right">2.48</td>
<td align="left">Random-effect model</td>
</tr>
<tr class="even">
<td align="left">REML</td>
<td align="right">-15.64702</td>
<td align="right">1.274628</td>
<td align="left">Restricted maximum-likelihood</td>
<td align="right">2.52</td>
<td align="left">Random-effect model</td>
</tr>
<tr class="odd">
<td align="left">ML</td>
<td align="right">-15.51773</td>
<td align="right">1.190701</td>
<td align="left">Maximum-likelihood</td>
<td align="right">0.06</td>
<td align="left">Random-effect model</td>
</tr>
<tr class="even">
<td align="left">HS</td>
<td align="right">-15.56254</td>
<td align="right">1.217847</td>
<td align="left">Hunter-Schmidt</td>
<td align="right">0.81</td>
<td align="left">Random-effect model</td>
</tr>
<tr class="odd">
<td align="left">SJ</td>
<td align="right">-15.95615</td>
<td align="right">1.603754</td>
<td align="left">Sidik-Jonkman</td>
<td align="right">15.75</td>
<td align="left">Random-effect model</td>
</tr>
<tr class="even">
<td align="left">HE</td>
<td align="right">-15.51403</td>
<td align="right">1.188538</td>
<td align="left">Hedges</td>
<td align="right">0.00</td>
<td align="left">Random-effect model</td>
</tr>
<tr class="odd">
<td align="left">EB</td>
<td align="right">-15.64543</td>
<td align="right">1.273478</td>
<td align="left">Empirical Bayes</td>
<td align="right">2.48</td>
<td align="left">Random-effect model</td>
</tr>
</tbody>
</table>
<pre class="r"><code># 5. Do meta-analysis
m &lt;- metagen(MD, seMD, data=res,
             studlab=method,
             sm=&quot;MD&quot;,
             comb.fixed=FALSE, comb.random=FALSE,
             byvar=model)
knitr::kable(m)</code></pre>
<table style="width:100%;">
<colgroup>
<col width="17%" />
<col width="5%" />
<col width="5%" />
<col width="6%" />
<col width="2%" />
<col width="1%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="6%" />
<col width="11%" />
<col width="5%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">studlab</th>
<th align="right">TE</th>
<th align="right">seTE</th>
<th align="right">statistic</th>
<th align="right">pval</th>
<th align="left">df</th>
<th align="right">lower</th>
<th align="right">upper</th>
<th align="right">w.common</th>
<th align="right">w.random</th>
<th align="right">zval</th>
<th align="left">subgroup</th>
<th align="right">w.fixed</th>
<th align="left">byvar</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="right">-15.51403</td>
<td align="right">1.188538</td>
<td align="right">-13.053036</td>
<td align="right">0</td>
<td align="left">NA</td>
<td align="right">-17.84353</td>
<td align="right">-13.18454</td>
<td align="right">0.7079028</td>
<td align="right">0.7079028</td>
<td align="right">-13.053036</td>
<td align="left">Fixed-effect model</td>
<td align="right">0.7079028</td>
<td align="left">Fixed-effect model</td>
</tr>
<tr class="even">
<td align="left">DerSimonian-Laird</td>
<td align="right">-15.64702</td>
<td align="right">1.274628</td>
<td align="right">-12.275758</td>
<td align="right">0</td>
<td align="left">NA</td>
<td align="right">-18.14525</td>
<td align="right">-13.14880</td>
<td align="right">0.6155073</td>
<td align="right">0.6155073</td>
<td align="right">-12.275758</td>
<td align="left">Random-effect model</td>
<td align="right">0.6155073</td>
<td align="left">Random-effect model</td>
</tr>
<tr class="odd">
<td align="left">Paule-Mandel</td>
<td align="right">-15.64543</td>
<td align="right">1.273478</td>
<td align="right">-12.285593</td>
<td align="right">0</td>
<td align="left">NA</td>
<td align="right">-18.14140</td>
<td align="right">-13.14946</td>
<td align="right">0.6166196</td>
<td align="right">0.6166196</td>
<td align="right">-12.285593</td>
<td align="left">Random-effect model</td>
<td align="right">0.6166196</td>
<td align="left">Random-effect model</td>
</tr>
<tr class="even">
<td align="left">Restricted maximum-likelihood</td>
<td align="right">-15.64702</td>
<td align="right">1.274628</td>
<td align="right">-12.275758</td>
<td align="right">0</td>
<td align="left">NA</td>
<td align="right">-18.14525</td>
<td align="right">-13.14880</td>
<td align="right">0.6155073</td>
<td align="right">0.6155073</td>
<td align="right">-12.275758</td>
<td align="left">Random-effect model</td>
<td align="right">0.6155073</td>
<td align="left">Random-effect model</td>
</tr>
<tr class="odd">
<td align="left">Maximum-likelihood</td>
<td align="right">-15.51773</td>
<td align="right">1.190701</td>
<td align="right">-13.032432</td>
<td align="right">0</td>
<td align="left">NA</td>
<td align="right">-17.85146</td>
<td align="right">-13.18400</td>
<td align="right">0.7053334</td>
<td align="right">0.7053334</td>
<td align="right">-13.032432</td>
<td align="left">Random-effect model</td>
<td align="right">0.7053334</td>
<td align="left">Random-effect model</td>
</tr>
<tr class="even">
<td align="left">Hunter-Schmidt</td>
<td align="right">-15.56254</td>
<td align="right">1.217847</td>
<td align="right">-12.778728</td>
<td align="right">0</td>
<td align="left">NA</td>
<td align="right">-17.94947</td>
<td align="right">-13.17560</td>
<td align="right">0.6742399</td>
<td align="right">0.6742399</td>
<td align="right">-12.778728</td>
<td align="left">Random-effect model</td>
<td align="right">0.6742399</td>
<td align="left">Random-effect model</td>
</tr>
<tr class="odd">
<td align="left">Sidik-Jonkman</td>
<td align="right">-15.95615</td>
<td align="right">1.603754</td>
<td align="right">-9.949252</td>
<td align="right">0</td>
<td align="left">NA</td>
<td align="right">-19.09945</td>
<td align="right">-12.81285</td>
<td align="right">0.3887986</td>
<td align="right">0.3887986</td>
<td align="right">-9.949252</td>
<td align="left">Random-effect model</td>
<td align="right">0.3887986</td>
<td align="left">Random-effect model</td>
</tr>
<tr class="even">
<td align="left">Hedges</td>
<td align="right">-15.51403</td>
<td align="right">1.188538</td>
<td align="right">-13.053036</td>
<td align="right">0</td>
<td align="left">NA</td>
<td align="right">-17.84353</td>
<td align="right">-13.18454</td>
<td align="right">0.7079028</td>
<td align="right">0.7079028</td>
<td align="right">-13.053036</td>
<td align="left">Random-effect model</td>
<td align="right">0.7079028</td>
<td align="left">Random-effect model</td>
</tr>
<tr class="odd">
<td align="left">Empirical Bayes</td>
<td align="right">-15.64543</td>
<td align="right">1.273478</td>
<td align="right">-12.285593</td>
<td align="right">0</td>
<td align="left">NA</td>
<td align="right">-18.14140</td>
<td align="right">-13.14946</td>
<td align="right">0.6166196</td>
<td align="right">0.6166196</td>
<td align="right">-12.285593</td>
<td align="left">Random-effect model</td>
<td align="right">0.6166196</td>
<td align="left">Random-effect model</td>
</tr>
</tbody>
</table>
<pre class="r"><code># 6. Do forest plot
# pdf(file=&quot;Schwarzer-Fig2.5.pdf&quot;, width=8.1, height=4.0) 
# uncomment line to generate PDF file
forest(m,
       xlim=c(-20, -12),
       hetstat=FALSE, smlab=&quot;&quot;,
       leftcols=c(&quot;studlab&quot;, &quot;tau2&quot;),
       leftlabs=c(&quot;Method&quot;, &quot;Between-study\nheterogeneity&quot;),
       print.byvar=FALSE)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code># invisible(dev.off()) # uncomment line to save PDF file   </code></pre>
</div>
<div id="hartung-knapp-adjustment" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Hartung-Knapp
Adjustment</h2>
<p>Hartung and Knapp (2001a,b) proposed an alternative method for random
effects meta-analysis based on a refined variance estimator for the
treatment estimate. Simulation studies (Hartung and Knapp, 2001a,b;
IntHout et al., 2014; Langan et al., 2019) show <strong>improved
coverage probabilities</strong> compared to the classic random effects
method.</p>
<p>Hartung和Knapp
在随机效应模型中引入了一种基于改进方差估计器的新的会萃分析方法。与DerSimonianLaird方法相比，首选Hartung-Knapp方法。
代替使方差估计 <span class="math display">\[
\widehat{\operatorname{Var}}\left(\hat{\theta}_{R}\right)=\frac{1}{\sum_{k=1}^{K}
w_{k}^{*}}
\]</span> Instead of using the variance estimate given in Eq. (2.14),
Hartung and Knapp propose to use the following variance estimator for
<span class="math inline">\(\hat{\theta}_{R}\)</span> : <span
class="math display">\[
\widehat{\operatorname{Var}}_{\mathrm{HK}}\left(\hat{\theta}_{R}\right)=\frac{1}{K-1}
\sum_{k=1}^{K}
\frac{w_{k}^{*}}{w^{*}}\left(\hat{\theta}_{k}-\hat{\theta}_{R}\right)^{2}
\]</span> with weights <span class="math inline">\(w_{k}^{*}\)</span> as
given in above and <span class="math inline">\(w=\sum k=1^{K} w
k\)</span>. Hartung showed that <span class="math display">\[
\frac{\hat{\theta}_{R}-\theta}{\text { S.E. }
\mathrm{HK}\left(\hat{\theta}_{R}\right)}
\]</span> with standard error s.E. <span
class="math inline">\(\mathrm{HK}\left(\hat{\theta}_{R}\right)=\sqrt{\widehat{\operatorname{Var}}_{\mathrm{HK}}\left(\hat{\theta}_{R}\right)}\)</span>
follows a <span class="math inline">\(t\)</span> -distribution with
<span class="math inline">\(K-1\)</span> degrees of freedom.
Accordingly, a <span class="math inline">\((1-\alpha)\)</span>
confidence interval for <span
class="math inline">\(\hat{\theta}_{R}\)</span> based on the
Hartung-Knapp method can be calculated by <span class="math display">\[
\hat{\theta}_{R} \pm t_{K-1 ; 1-\frac{\alpha}{2}} \text { S.E. }
\mathrm{HK}\left(\hat{\theta}_{R}\right)
\]</span></p>
<pre class="r"><code> # use the metacont function to conduct the Hartung–Knapp adjustment
mc2.hk &lt;- metacont(Ne, Me, Se, Nc, Mc, Sc, sm=&quot;SMD&quot;,
                   data=data2, comb.fixed=FALSE,
                   hakn=TRUE)</code></pre>
<p>However, in rare settings with very homogeneous treatment estimates,
the Hartung-Knapp (HK) variance estimate can be arbitrarily small
resulting in a very narrow confidence interval (Knapp and Hartung, 2003;
Wiksten et al., 2016). In such cases, an ad hoc variance correction has
been proposed by utilising the variance estimate from the classic random
effects model with the HK method (Knapp and Hartung, 2003; IQWiQ, 2020).
An alternative approach is to use the wider confidence interval of
classic fixed or random effects meta-analysis and the HK method (Wiksten
et al., 2016; Jackson et al., 2017).</p>
<ul>
<li><code>adhoc.hakn = ""</code> Ad hoc method not used</li>
<li><code>adhoc.hakn = "se"</code> use variance correction if HK
standard error is smaller than standard error from classic random
effects meta-analysis (Knapp and Hartung, 2003)</li>
<li><code>adhoc.hakn = "iqwig6"</code> use variance correction if HK
confidence interval is narrower than CI from classic random effects
model with DerSimonian-Laird estimator (IQWiG, 2020)</li>
<li><code>adhoc.hakn = "ci"</code> use wider confidence interval of
classic random effects and HK meta-analysis (Hybrid method 2 in Jackson
et al., 2017)</li>
</ul>
</div>
</div>
<div id="meta-regression" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Meta-Regression</h1>
<p><strong>Meta-regression</strong> is an extension of meta-analysis
designed to explain <strong>residual heterogeneity</strong> among study
results by incorporating <strong>study-level moderators</strong> (also
called covariates or independent variables). These moderators might
include characteristics such as study year, mean age of participants,
treatment dosage, study quality score, or geographic region.</p>
<p>In essence, meta-regression allows us to model how the <strong>effect
size</strong> changes across studies depending on these moderators. This
helps uncover patterns and improve understanding of why studies differ
in their results.</p>
<div id="basic-concept" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Basic Concept</h2>
<p>Just as a standard regression model evaluates how a dependent
variable is influenced by one or more predictors, a meta-regression
regresses the <strong>effect sizes</strong> reported in individual
studies on one or more <strong>study-level characteristics</strong>. The
key difference is that each effect size is estimated with a known
variance (from the original studies), and that this variance should be
incorporated into the estimation procedure.</p>
<p>The effect size from study <em>i</em> is denoted as
<strong>δ̂ᵢ</strong>. These observed effect sizes vary due to both:</p>
<ol style="list-style-type: decimal">
<li><strong>Within-study sampling variance</strong> (denoted σ̂²ᵢ),
and</li>
<li><strong>Between-study variance</strong> (denoted τ²), which is
estimated if a <strong>random-effects model</strong> is used.</li>
</ol>
<p>The general <strong>meta-regression model</strong> can be written
as:</p>
<p><strong>δᵢ = β₀ + β₁xᵢ₁ + β₂xᵢ₂ + … + βₚxᵢₚ + νᵢ + εᵢ</strong></p>
<ul>
<li>δᵢ: true effect size for study <em>i</em></li>
<li>xᵢ₁, xᵢ₂, …, xᵢₚ: study-level moderators</li>
<li>β₀: intercept (mean effect when all covariates are 0)</li>
<li>βⱼ: regression coefficient for moderator <em>j</em></li>
<li>νᵢ ~ N(0, τ²): between-study random effect (residual
heterogeneity)</li>
<li>εᵢ ~ N(0, σ̂²ᵢ): within-study error (sampling variance from each
study)</li>
</ul>
<hr />
<p><strong>Fixed-Effects vs Random-Effects Meta-Regression</strong></p>
<p>In <strong>fixed-effects meta-regression</strong>, it is assumed that
all residual variation in effect size across studies is due to
within-study sampling error (τ² = 0). This is appropriate only if there
is no heterogeneity beyond sampling error.</p>
<p>In contrast, <strong>random-effects meta-regression</strong> allows
for additional variability in true effect sizes not explained by
sampling error, acknowledging that effect sizes may truly differ across
studies.</p>
<ul>
<li>Use fixed-effects meta-regression when you believe studies share a
common true effect.</li>
<li>Use random-effects meta-regression when studies likely differ in
their true effects, even after accounting for moderators.</li>
</ul>
<hr />
<p><strong>Interpreting Meta-Regression</strong></p>
<p>The main goals are:</p>
<ul>
<li>To identify which moderators significantly <strong>explain</strong>
heterogeneity.</li>
<li>To understand the <strong>direction</strong> and
<strong>magnitude</strong> of association between moderators and effect
sizes.</li>
<li>To quantify how much <strong>residual heterogeneity</strong>
(unexplained variance) remains after including moderators.</li>
</ul>
<p>Statistical output includes:</p>
<ul>
<li>Estimates of <strong>β coefficients</strong> (effect of each
moderator on the outcome)</li>
<li>Their <strong>standard errors</strong>, <strong>z-values</strong>,
and <strong>p-values</strong></li>
<li>The estimate of <strong>τ²</strong>, which is the residual
heterogeneity <strong>after</strong> accounting for moderators</li>
<li>Measures like <strong>QM</strong> (analogous to the regression
F-test), testing whether all β’s are jointly zero</li>
</ul>
</div>
<div id="implementing-meta-regression-in-r-with-metafor"
class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Implementing
Meta-Regression in R with metafor</h2>
<p>The <code>meta</code> and <code>rmeta</code> packages do not support
meta-regression. Instead, the <strong><code>metafor</code></strong>
package is used, which is designed for flexible meta-analysis and
includes advanced features like meta-regression.</p>
<p>To perform <strong>meta-analysis</strong> using the
<code>rma()</code> function in the <strong><code>metafor</code></strong>
R package, you must specify the <strong>method</strong> used to estimate
the <strong>between-study variance τ²</strong>, which is critical for
random-effects models. The choice of method can impact the estimated τ²
and, consequently, the weighting of studies and the overall
meta-analytic result.</p>
<p><strong>When to Choose Which Method?</strong></p>
<ul>
<li>Use <code>"REML"</code> when you want <strong>accurate τ²
estimation</strong> and plan to use meta-regression.</li>
<li>Use <code>"DL"</code> when replicating older studies or when
simplicity and speed are needed.</li>
<li>Use <code>"SJ"</code> or <code>"EB"</code> if you’re concerned about
<strong>underestimating</strong> between-study heterogeneity.</li>
<li>Use <code>"ML"</code> if you want to compare models using likelihood
criteria (e.g., AIC).</li>
</ul>
<p><strong>Fixed-Effects Model</strong></p>
<p>If you assume <strong>no between-study heterogeneity</strong> (i.e.,
τ² = 0), you are using a <strong>fixed-effects model</strong>. You can
specify it explicitly with:</p>
<pre class="r"><code>method = &quot;FE&quot;</code></pre>
<p>This method assumes that all studies estimate the same true effect
size, and any observed differences are due to sampling error alone.</p>
<hr />
<p><strong>Random-Effects Models and τ² Estimation Methods</strong></p>
<p>For <strong>random-effects meta-analysis</strong>, τ² must be
estimated. The <code>rma()</code> function provides multiple estimators
for τ², each with different theoretical backgrounds and properties.</p>
<p>Here are the main methods you can choose from:</p>
<ol style="list-style-type: decimal">
<li><p><strong>DerSimonian-Laird (“DL”)</strong></p>
<ul>
<li><p>Classic method used in most meta-analyses historically.</p></li>
<li><p>Based on method of moments.</p></li>
<li><p><strong>Quick and easy</strong>, but known to
<strong>underestimate τ²</strong>, especially with few studies or when
heterogeneity is large.</p></li>
<li><p>Use:</p>
<pre class="r"><code>rma(yi, vi, data = dat, method = &quot;DL&quot;)</code></pre></li>
</ul></li>
<li><p><strong>Hunter-Schmidt (“HS”)</strong></p>
<ul>
<li><p>Used mostly in psychological and behavioral science
meta-analyses.</p></li>
<li><p>Assumes <strong>true score variance model</strong>, often used
with correlation coefficients.</p></li>
<li><p>Less common in medical or epidemiological contexts.</p></li>
<li><p>Use:</p>
<pre class="r"><code>method = &quot;HS&quot;</code></pre></li>
</ul></li>
<li><p><strong>Hedges (“HE”)</strong></p>
<ul>
<li><p>Based on Hedges and Olkin’s method.</p></li>
<li><p>Also a <strong>method-of-moments</strong> estimator, more robust
than DL under certain conditions.</p></li>
<li><p>Known to produce <strong>less biased τ² estimates</strong> when
variances are unequal.</p></li>
<li><p>Use:</p>
<pre class="r"><code>method = &quot;HE&quot;</code></pre></li>
</ul></li>
<li><p><strong>Sidik-Jonkman (“SJ”)</strong></p>
<ul>
<li><p>A newer estimator.</p></li>
<li><p>Tends to <strong>overestimate τ²</strong> in small samples but is
<strong>very conservative</strong>.</p></li>
<li><p>Good when aiming for <strong>wider confidence intervals</strong>
to avoid overconfidence.</p></li>
<li><p>Use:</p>
<pre class="r"><code>method = &quot;SJ&quot;</code></pre></li>
</ul></li>
<li><p><strong>Empirical Bayes (“EB”)</strong></p>
<ul>
<li><p>Uses Bayesian principles to shrink individual study estimates
toward the overall mean.</p></li>
<li><p>Especially useful when the number of studies is <strong>very
small</strong>.</p></li>
<li><p>Use:</p>
<pre class="r"><code>method = &quot;EB&quot;</code></pre></li>
</ul></li>
<li><p><strong>Maximum Likelihood (“ML”)</strong></p>
<ul>
<li><p>Optimizes the likelihood of observing the data given a
model.</p></li>
<li><p>Biased in small samples (underestimates τ²), but still widely
used.</p></li>
<li><p>Allows model comparisons (e.g., AIC).</p></li>
<li><p>Use:</p>
<pre class="r"><code>method = &quot;ML&quot;</code></pre></li>
</ul></li>
<li><p><strong>Restricted Maximum Likelihood (“REML”)</strong></p>
<ul>
<li><p>Default method in <code>rma()</code>.</p></li>
<li><p>Adjusts for degrees of freedom used in estimating fixed
effects.</p></li>
<li><p><strong>Asymptotically unbiased and efficient</strong>.</p></li>
<li><p>Recommended for most applications.</p></li>
<li><p>Use:</p>
<pre class="r"><code>method = &quot;REML&quot;</code></pre></li>
</ul></li>
</ol>
<p><strong>To Do with Doing Meta-Analysis in R: A Hands-on
Guide</strong></p>
<p><strong>Practical Notes</strong></p>
<ul>
<li>Use only study-level covariates (not individual participant-level
variables).</li>
<li>Be cautious when including many moderators in a model with a small
number of studies.</li>
<li>Avoid overfitting; meta-regression should be guided by theory or
prior knowledge.</li>
<li>Meta-regression is observational—<strong>associations do not imply
causation</strong>.</li>
</ul>
</div>
<div id="meta-regression-vs.-weighted-regression" class="section level2"
number="5.3">
<h2><span class="header-section-number">5.3</span> Meta-regression
vs. Weighted Regression</h2>
<p><strong>Meta-regression vs. Weighted Regression</strong> is a subtle
but important distinction in statistical modeling, especially in the
context of meta-analysis.</p>
<p>At first glance, meta-regression appears very similar to ordinary
<strong>weighted linear regression</strong>, where the <strong>effect
size estimates</strong> (e.g., log odds ratios, SMDs, or risk
differences from individual studies) are regressed on
<strong>study-level covariates</strong> using a set of
<strong>weights</strong>. However, while the <strong>parameter
estimates</strong> (the regression coefficients) from both approaches
will typically match, the <strong>standard errors</strong>,
<strong>confidence intervals</strong>, and <strong>p-values</strong> can
differ significantly. The key lies in the <strong>assumed error
structure</strong> of the models.</p>
<p>In <strong>meta-regression</strong>, the goal is to estimate how
<strong>study-level moderators</strong> (like geographic region, year,
or sample size) explain variations in the effect sizes across studies.
The correct meta-regression model assumes:</p>
<ul>
<li><p>Known and fixed variances for each study’s effect size (provided
as input)</p></li>
<li><p>Residuals that have a <strong>known variance structure</strong>,
namely:</p>
<p><strong>Var(εᵢ) = σ̂ᵢ² + τ²</strong></p></li>
</ul>
<p>This combined variance includes:</p>
<ul>
<li><strong>σ̂ᵢ²</strong>, the within-study variance (from each
study)</li>
<li><strong>τ²</strong>, the between-study variance (heterogeneity),
estimated by <code>rma()</code> from the data</li>
</ul>
<p>These two components make up the weights:</p>
<p><strong>wᵢ = 1 / (σ̂ᵢ² + τ²)</strong></p>
<hr />
<p><strong>The Mistake with <code>lm()</code> and Weighted
Regression</strong></p>
<p>The R function <code>lm()</code> can perform weighted linear
regression via the <code>weights</code> argument, and it will indeed
<strong>give the correct point estimates</strong> for the regression
coefficients if you use <strong>wᵢ</strong> as the weights. However, it
does <strong>not make the correct distributional assumption</strong> for
the residuals.</p>
<p>Specifically:</p>
<ul>
<li><code>lm()</code> assumes residuals εᵢ are from <strong>N(0, σ² ×
wᵢ)</strong>, where σ² is an <strong>unknown common error
variance</strong> that is <strong>estimated from the data</strong></li>
<li>Meta-regression assumes εᵢ ∼ <strong>N(0, 1 / wᵢ)</strong> — i.e.,
the weights are already the known inverse variances, and no additional
σ² scaling is needed</li>
</ul>
<p>Because of this mismatch, <code>lm()</code> will inflate the standard
errors by a factor of √(σ̂²), which results in <strong>incorrect
inference</strong> (i.e., p-values and confidence intervals will be
wrong).</p>
<hr />
<p><strong>Conclusion</strong></p>
<ul>
<li><strong>Meta-regression</strong> and <strong>weighted
regression</strong> use the same formula for calculating coefficient
estimates, but they differ in the assumed variance model of the
residuals.</li>
<li><strong>lm() incorrectly estimates an extra residual variance term
(σ²)</strong>, leading to inflated standard errors and incorrect
inference.</li>
<li>Always use <code>rma()</code> or manual calculations with fixed
weights and no additional residual variance term when performing
meta-regression.</li>
</ul>
</div>
</div>
<div id="binary-outcomes" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Binary Outcomes</h1>
<div id="effect-measures-1" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Effect Measures</h2>
<p>In meta-analysis, especially when dealing with binary outcomes (such
as “event occurred” vs. “event did not occur”), choosing an appropriate
<strong>effect measure</strong> is crucial for meaningful interpretation
and synthesis across studies. The most commonly used effect measures
include:</p>
<p><strong>Study Summary Data (2×2 Table)</strong></p>
<p>In each study <span class="math inline">\(k\)</span> included in the
meta-analysis (where <span class="math inline">\(k = 1, ...,
K\)</span>), the outcome data can be summarized in a 2×2 contingency
table:</p>
<p><span class="math display">\[
\begin{array}{l|c|c|c}
\hline
&amp; \text{Event} &amp; \text{No Event} &amp; \text{Group Size} \\
\hline
\text{Experimental Group} &amp; a_k &amp; b_k &amp; n_{ek} = a_k + b_k
\\
\text{Control Group} &amp; c_k &amp; d_k &amp; n_{ck} = c_k + d_k \\
\hline
\text{Totals} &amp; a_k + c_k &amp; b_k + d_k &amp; n_k = n_{ek} +
n_{ck} \\
\hline
\end{array}
\]</span></p>
<p>From this, the <strong>estimated probabilities</strong> of event
occurrence in each group are calculated as:</p>
<p><span class="math display">\[
\hat{p}_{e k} = \frac{a_k}{n_{e k}} \quad \text{and} \quad \hat{p}_{c k}
= \frac{c_k}{n_{c k}}
\]</span></p>
<p>These probabilities are then used to compute the chosen effect
measure (OR, RR, RD, ASD) for each study <span
class="math inline">\(k\)</span>, which are subsequently pooled using
either a <strong>fixed-effects</strong> or
<strong>random-effects</strong> model, depending on heterogeneity and
modeling assumptions.</p>
<hr />
<p><strong>1. Odds Ratio (OR)</strong></p>
<p>The <strong>odds ratio</strong> compares the odds of the event
occurring in the treatment group to the odds in the control group.</p>
<p><span class="math display">\[
\text{OR}_k = \frac{a_k / b_k}{c_k / d_k} = \frac{a_k d_k}{b_k c_k}
\]</span></p>
<ul>
<li><strong>Odds</strong>: the ratio of the probability that the event
occurs to the probability that it does not occur.</li>
<li>Often used in <strong>case-control studies</strong> or where
outcomes are <strong>rare</strong>.</li>
<li><strong>Log(OR)</strong> is approximately normally distributed,
making it suitable for statistical modeling and meta-analysis.</li>
</ul>
<hr />
<p><strong>2. Risk Ratio (RR)</strong></p>
<p>Also called the <strong>relative risk</strong>, this measure compares
the <strong>probability</strong> of the event between the two
groups.</p>
<p><span class="math display">\[
\text{RR}_k = \frac{\hat{p}_{e k}}{\hat{p}_{c k}} = \frac{a_k / (a_k +
b_k)}{c_k / (c_k + d_k)} = \frac{a_k / n_{e k}}{c_k / n_{c k}}
\]</span></p>
<ul>
<li>Indicates how many times more (or less) likely an event is in the
treatment group compared to the control.</li>
<li>Intuitive and widely used in <strong>randomized controlled
trials</strong>.</li>
<li>Less statistically stable than OR when event probabilities approach
0 or 1.</li>
</ul>
<hr />
<p><strong>3. Risk Difference (RD)</strong></p>
<p>The <strong>risk difference</strong> measures the absolute difference
in event probabilities between groups.</p>
<p><span class="math display">\[
\text{RD}_k = \hat{p}_{e k} - \hat{p}_{c k}
\]</span></p>
<ul>
<li>Directly interpretable as the percentage point difference in
risk.</li>
<li>More sensitive to <strong>baseline risk</strong>; can vary
substantially across populations.</li>
<li><strong>Less stable</strong> across studies than OR or RR, which are
<strong>relative</strong> measures.</li>
</ul>
<hr />
<p><strong>4. Arcsine Difference (ASD)</strong></p>
<p>The <strong>arcsine difference</strong> is used less frequently but
is especially useful for detecting <strong>small-study effects</strong>
(e.g., publication bias) in meta-analysis.</p>
<p><span class="math display">\[
\text{ASD}_k = \arcsin\left(\sqrt{\hat{p}_{e k}}\right) -
\arcsin\left(\sqrt{\hat{p}_{c k}}\right)
\]</span></p>
<ul>
<li>Stabilizes variance, especially in studies with small sample sizes
or rare events.</li>
<li>Used in certain <strong>regression-based tests</strong> for funnel
plot asymmetry or bias detection.</li>
</ul>
<hr />
<p><strong>Why Are OR and RR Usually Preferred?</strong></p>
<p>The <strong>odds ratio</strong> and <strong>risk ratio</strong> are
preferred in most meta-analyses for several reasons:</p>
<ul>
<li>They are <strong>relative measures</strong>, which tend to remain
more <strong>consistent</strong> across studies with different baseline
risks (a concept known as <strong>effect measure
homogeneity</strong>).</li>
<li><strong>Log(OR)</strong> and <strong>log(RR)</strong> are
approximately normally distributed, enabling the use of standard
meta-analytic methods (e.g., inverse-variance weighting).</li>
<li>They are less affected by the absolute event rate compared to risk
difference, making them more <strong>robust</strong> in diverse clinical
settings.</li>
</ul>
</div>
<div
id="analysis-with-risk-ratio-fixed-effect---inverse-variance-method"
class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Analysis with
Risk-Ratio <a href="#fixed-effect---inverse-variance-method">Fixed
Effect - Inverse Variance Method</a></h2>
<p>The risk ratio (RR), also known as relative risk, is a widely used
measure of treatment effect in clinical trials with binary outcomes
(e.g., event/no event). In a meta-analysis, RR quantifies how the
probability of an event (e.g., death, MI) differs between a treatment
group and a control group across multiple studies.</p>
<p>The risk ratio is defined as the ratio of the event probability
(risk) in the experimental group to the event probability in the control
group:</p>
<p><span class="math display">\[
\text{RR} = \frac{p_E}{p_C} = \frac{x_E / n_E}{x_C / n_C}
\]</span></p>
<ul>
<li><span class="math inline">\(p_E\)</span>: Risk (probability of
event) in the <strong>experimental</strong> group</li>
<li><span class="math inline">\(p_C\)</span>: Risk (probability of
event) in the <strong>control</strong> group</li>
<li><span class="math inline">\(x_E\)</span>: Number of events in the
experimental group</li>
<li><span class="math inline">\(n_E\)</span>: Total number of patients
in the experimental group</li>
<li><span class="math inline">\(x_C\)</span>: Number of events in the
control group</li>
<li><span class="math inline">\(n_C\)</span>: Total number of patients
in the control group</li>
</ul>
<p>Interpretation:</p>
<ul>
<li>RR = 1: No difference between the groups</li>
<li>RR &lt; 1: Lower risk in the experimental group (treatment is
beneficial)</li>
<li>RR &gt; 1: Higher risk in the experimental group</li>
</ul>
<p>For example, in a statin study, RR &lt; 1 would indicate that
high-dose statins reduce risk of death or MI compared to standard-dose
statins.</p>
<hr />
<p><strong>Confidence Interval for RR using Log
Transformation</strong></p>
<p>Since the distribution of the RR is skewed, it’s common to perform
inference on its <strong>logarithmic scale</strong> using:</p>
<p><span class="math display">\[
\ln(\text{RR}) = \ln(p_E) - \ln(p_C)
\]</span></p>
<p>The variance of <span class="math inline">\(\ln(\text{RR})\)</span>
is approximated by the delta method as:</p>
<p><span class="math display">\[
\text{Var}(\ln(\text{RR})) = \frac{1}{x_E} - \frac{1}{n_E} +
\frac{1}{x_C} - \frac{1}{n_C}
\]</span></p>
<p>The <strong>standard error</strong> (SE) is the square root of the
variance. A <strong>95% confidence interval</strong> on the log scale
is:</p>
<p><span class="math display">\[
\ln(\text{RR}) \pm 1.96 \times SE
\]</span></p>
<p>Exponentiating the lower and upper limits gives the 95% CI for
RR:</p>
<p><span class="math display">\[
RR = \exp(\ln(\text{RR})) \\
LRR = \exp(\ln(\text{RR}) - 1.96 \times SE) \\
URR = \exp(\ln(\text{RR}) + 1.96 \times SE)
\]</span></p>
<p>This CI helps determine statistical significance: if the interval
<strong>does not include 1</strong>, the RR is statistically
significant.</p>
<p><strong>Assessing Statistical Significance</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Confidence Interval Method</strong> Calculate the 95% CI
using the log RR and its variance. Example results:</li>
</ol>
<ul>
<li>Study 3 (TNT) 95% CI: [0.697, 0.922] → significant</li>
<li>Other studies’ CIs include 1 → not significant</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><strong>P-Value Method</strong> Compute the Z-statistic:</li>
</ol>
<p><span class="math display">\[
z = \frac{\ln(\text{RR})}{SE}
\]</span></p>
<p>Then compute p-value:</p>
<p><span class="math display">\[
p = 2(1 - \Phi(|z|))
\]</span></p>
<p>Example:</p>
<ul>
<li>TNT study p-value = 0.00166 → significant</li>
<li>Others &gt; 0.05 → not significant</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><strong>Post-Hoc Power Analysis</strong> Using the <code>pwr</code>
package in R, power calculations showed:</li>
</ol>
<ul>
<li>TNT study: power ≈ 88.5% (high)</li>
<li>Others: power ≈ 36–44% (low)</li>
</ul>
<p>Low power explains why some studies didn’t show significance despite
lower RR values.</p>
<hr />
<p><strong>Risk Ratio Meta-Analysis Step-by-Step (Fixed-Effects
Model)</strong> Before considering a random-effects model, check for
<strong>between-study heterogeneity</strong>.</p>
<ol style="list-style-type: decimal">
<li>Compute the Q statistic:</li>
</ol>
<p><span class="math display">\[
Q = \sum w_i (\ln(\text{RR}_i))^2 - \frac{(\sum w_i
\ln(\text{RR}_i))^2}{\sum w_i}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Compare Q to a chi-square distribution with <span
class="math inline">\(K - 1\)</span> degrees of freedom.</li>
</ol>
<p><strong>Automated Meta-Analysis in R using <code>meta</code>
package</strong></p>
<pre><code>metabin(Ee, Ne, Ec, Nc, sm=&quot;RR&quot;, method=&quot;I&quot;,
        data=data7, subset=study==&quot;Milpied&quot;)</code></pre>
</div>
<div
id="analysis-with-risk-difference-fixed-effect---inverse-variance-method"
class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> Analysis with Risk
Difference <a href="#fixed-effect---inverse-variance-method">Fixed
Effect - Inverse Variance Method</a></h2>
<p>While the <strong>risk ratio (RR)</strong> is widely used for
summarizing binary outcome data in meta-analysis, the <strong>risk
difference (RD)</strong> is also an important effect measure due to its
<strong>simplicity and intuitive interpretation</strong>. It tells us
the <strong>absolute difference in event rates</strong> between the
treatment and control groups.</p>
<p><strong>Definition of Risk Difference (RD)</strong></p>
<p>The <strong>risk difference</strong> is calculated as the difference
between the observed risks in the experimental and control groups:</p>
<p><span class="math display">\[
\text{RD} = \hat{p}_E - \hat{p}_C = \frac{x_E}{n_E} - \frac{x_C}{n_C}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\hat{p}_E\)</span>: estimated risk
(probability of event) in the <strong>experimental</strong> group</li>
<li><span class="math inline">\(\hat{p}_C\)</span>: estimated risk in
the <strong>control</strong> group</li>
<li><span class="math inline">\(x_E\)</span>, <span
class="math inline">\(x_C\)</span>: number of events in the experimental
and control groups</li>
<li><span class="math inline">\(n_E\)</span>, <span
class="math inline">\(n_C\)</span>: total number of patients in each
group</li>
</ul>
<p><strong>Interpretation</strong>:</p>
<ul>
<li>RD = 0: No difference between groups</li>
<li>RD &lt; 0: Lower risk in the experimental group → treatment
<strong>beneficial</strong></li>
<li>RD &gt; 0: Lower risk in the control group → treatment
<strong>harmful</strong></li>
</ul>
<p>This measure is especially appealing to clinicians and policy-makers
because it reflects the <strong>actual difference in event
rates</strong>, which can be used to estimate metrics such as
<strong>number needed to treat (NNT = 1 / |RD|)</strong>.</p>
<hr />
<p><strong>Variance and Standard Error of RD</strong></p>
<p>To construct confidence intervals and conduct hypothesis tests, we
need to estimate the <strong>variance</strong> of the risk difference.
This is derived from the binomial distribution under the assumption that
each group’s outcomes are independent:</p>
<p><span class="math display">\[
\text{Var(RD)} = \frac{\hat{p}_E(1 - \hat{p}_E)}{n_E} +
\frac{\hat{p}_C(1 - \hat{p}_C)}{n_C}
\]</span></p>
<p>Then the <strong>standard error</strong> is:</p>
<p><span class="math display">\[
\text{SE(RD)} = \sqrt{\text{Var(RD)}}
\]</span></p>
<hr />
<p><strong>Example: RD Calculation in Statin Trials</strong></p>
<p>Using the statin trial data, suppose we have:</p>
<pre class="r"><code># Estimated risks
pE = c(0.0700, 0.0905, 0.0669, 0.0926)
pC = c(0.0834, 0.1053, 0.0835, 0.1041)

# Risk differences
RD = pE - pC
# Result:
# [1] -0.0133 -0.0148 -0.0166 -0.0115</code></pre>
<p>Now calculate the <strong>variance and standard error</strong> for
each RD:</p>
<pre class="r"><code># Sample sizes (example values)
nE = c(2099, 2265, 4995, 4439)
nC = c(2063, 2232, 5006, 4449)

# Variance of RD
VarRD = pE*(1 - pE)/nE + pC*(1 - pC)/nC
# Standard Error
SERD = sqrt(VarRD)
SERD</code></pre>
<pre><code>## [1] 0.008250440 0.008862815 0.005271108 0.006315980</code></pre>
<p>This allows us to construct <strong>95% confidence intervals</strong>
for each RD:</p>
<p><span class="math display">\[
\text{RD} \pm 1.96 \times \text{SE}
\]</span></p>
<hr />
<p><strong>Meta-Analysis of Risk Differences (Fixed-Effects
Model)</strong></p>
<p>To combine the RD values from all studies and improve precision, a
meta-analysis is performed using a <strong>fixed-effects model</strong>.
In this model, each study is <strong>weighted by the inverse of the
variance</strong> of its RD:</p>
<p><span class="math display">\[
w_i = \frac{1}{\text{Var(RD)}_i}
\]</span></p>
<p>The combined (pooled) RD is then calculated as a <strong>weighted
average</strong>:</p>
<p><span class="math display">\[
\text{RD}_{\text{pooled}} = \frac{\sum w_i \cdot \text{RD}_i}{\sum w_i}
\]</span></p>
<p>And the variance of the pooled RD:</p>
<p><span class="math display">\[
\text{Var}(\text{RD}_{\text{pooled}}) = \frac{1}{\sum w_i}
\]</span></p>
<p>The standard error and 95% confidence interval follow directly from
this.</p>
<hr />
<p><strong>Performing Meta-Analysis Using the R Package
<code>meta</code></strong></p>
<p>The <code>meta</code> package allows for straightforward
implementation of RD meta-analysis with the function
<code>metabin()</code>:</p>
<pre class="r"><code># Create the statin study data
dat &lt;- data.frame(
  Study = c(&quot;Prove-It&quot;, &quot;A-to-Z&quot;, &quot;TNT&quot;, &quot;IDEAL&quot;),
  evhigh = c(147, 205, 334, 411),   # number of events in high-dose group
  nhigh  = c(2099, 2265, 4995, 4439),  # total in high-dose group
  evstd  = c(172, 235, 418, 463),   # number of events in standard-dose group
  nstd   = c(2063, 2232, 5006, 4449)   # total in standard-dose group
)
RD.Statin = metabin(evhigh, nhigh, evstd, nstd,
                    studlab = Study,
                    data = dat,
                    method = &quot;Inverse&quot;,
                    sm = &quot;RD&quot;)
summary(RD.Statin)</code></pre>
<pre><code>##               RD             95%-CI %W(common) %W(random)
## Prove-It -0.0133 [-0.0295;  0.0028]       16.6       16.6
## A-to-Z   -0.0148 [-0.0321;  0.0026]       14.4       14.4
## TNT      -0.0166 [-0.0270; -0.0063]       40.7       40.7
## IDEAL    -0.0115 [-0.0239;  0.0009]       28.3       28.3
## 
## Number of studies: k = 4
## Number of observations: o = 27548 (o.e = 13798, o.c = 13750)
## Number of events: e = 2385
## 
##                           RD             95%-CI     z  p-value
## Common effect model  -0.0144 [-0.0209; -0.0078] -4.27 &lt; 0.0001
## Random effects model -0.0144 [-0.0209; -0.0078] -4.27 &lt; 0.0001
## 
## Quantifying heterogeneity (with 95%-CIs):
##  tau^2 = 0 [0.0000; 0.0001]; tau = 0 [0.0000; 0.0096]
##  I^2 = 0.0% [0.0%; 84.7%]; H = 1.00 [1.00; 2.56]
## 
## Test of heterogeneity:
##     Q d.f. p-value
##  0.41    3  0.9379
## 
## Details of meta-analysis methods:
## - Inverse variance method
## - Restricted maximum-likelihood estimator for tau^2
## - Q-Profile method for confidence interval of tau^2 and tau
## - Calculation of I^2 based on Q</code></pre>
<pre class="r"><code>plot(RD.Statin)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre><code>metabin(Ee, Ne, Ec, Nc, sm=&quot;RD&quot;, method=&quot;I&quot;,
        data=data7, subset=study==&quot;Milpied&quot;)</code></pre>
</div>
<div id="meta-analysis-with-odds-ratio" class="section level2"
number="6.4">
<h2><span class="header-section-number">6.4</span> Meta-Analysis with
Odds Ratio</h2>
<div id="fixed-effect---inverse-variance-method" class="section level3"
number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> Fixed Effect -
Inverse Variance Method</h3>
<p>The odds ratio for study <span class="math inline">\(k,
\psi_{k}\)</span>, is defined as the ratio of the odds of an event in
the experimental arm to that in the control arm. That is <span
class="math display">\[
\psi_{k}=\frac{\left(\frac{p_{e k}}{1-p_{e k}}\right)}{\left(\frac{p_{c
k}}{1-p_{c k}}\right)}=\frac{p_{e k}\left(1-p_{c k}\right)}{p_{c
k}\left(1-p_{e k}\right)}
\]</span> - If either of the two estimated event probabilities is zero
the log odds ratio, <span class="math inline">\(\log \psi_{k}\)</span>,
is either <span class="math inline">\(-\infty\)</span> or <span
class="math inline">\(+\infty\)</span> - If both are zero, the log odds
ratio is undefined. The odds ratio from study <span
class="math inline">\(k\)</span> is estimated by <span
class="math display">\[
\hat{\psi}_{k}=\frac{a_{k} d_{k}}{b_{k} c_{k}}
\]</span> variance of the natural logarithm of the odds ratio is well
approximated by <span class="math display">\[
\widehat{\operatorname{Var}}\left(\log
\hat{\psi}_{k}\right)=\frac{1}{a_{k}}+\frac{1}{b_{k}}+\frac{1}{c_{k}}+\frac{1}{d_{k}}
\]</span> where the approximation improves as nek and <span
class="math inline">\(n_{c k}\)</span> increase. Using the estimate of
the log odds ratio, and its estimated variance, an approximate two-sided
<span class="math inline">\((1-\alpha)\)</span> confidence interval for
the odds ratio is given by</p>
<p><span class="math display">\[\exp \left(\log \hat{\psi}_{k} \pm
z_{1-\frac{\alpha}{2}} \text { S.E. }\left(\log
\hat{\psi}_{k}\right)\right)\]</span></p>
<ul>
<li>The <code>metabin</code> function can smoothly realize the
combination of effect sizes</li>
</ul>
<pre class="r"><code>data(Fleiss93)
# Can taking aspirin after a heart attack reduce mortality?
# 7 studies information
# event.e indicates the number of deaths in the treatment group
# n.e indicates the total number of people in the treatment group
# event.c indicates the total number of people in the control group
# n.c indicates the total number of people in the control group

metabin(event.e, n.e, event.c, n.c, data=Fleiss93, sm=&quot;OR&quot;)</code></pre>
<pre><code>## Number of studies: k = 7
## Number of observations: o = 28003 (o.e = 14186, o.c = 13817)
## Number of events: e = 4414
## 
##                          OR           95%-CI     z p-value
## Common effect model  0.8969 [0.8405; 0.9570] -3.29  0.0010
## Random effects model 0.8683 [0.7559; 0.9973] -2.00  0.0457
## 
## Quantifying heterogeneity (with 95%-CIs):
##  tau^2 = 0.0147 [0.0000; 0.1145]; tau = 0.1214 [0.0000; 0.3384]
##  I^2 = 39.7% [0.0%; 74.6%]; H = 1.29 [1.00; 1.99]
## 
## Test of heterogeneity:
##     Q d.f. p-value
##  9.95    6  0.1269
## 
## Details of meta-analysis methods:
## - Mantel-Haenszel method (common effect model)
## - Inverse variance method (random effects model)
## - Restricted maximum-likelihood estimator for tau^2
## - Q-Profile method for confidence interval of tau^2 and tau
## - Calculation of I^2 based on Q</code></pre>
<pre class="r"><code># Meta-analysis forest plot is generally indispensable
# The OR value and 95% confidence interval, weight, combined effect size value and heterogeneity test results of each study are all in this figure
# comb.random=FALSE, the random effect model is not used for effect size combination, but the fixed effect model is used
metaresult&lt;-metabin(event.e, n.e,event.c,n.c,data=Fleiss93,sm=&quot;OR&quot;,
                    studlab=paste(study, year),random=FALSE)
plot(metaresult)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p><strong>Unfortunately, for sparse data, the pooled estimates based on
the inverse variance method are biased</strong></p>
</div>
<div id="meta-analysis-using-mantel-haenszel-method-fixed-effect"
class="section level3" number="6.4.2">
<h3><span class="header-section-number">6.4.2</span> Meta-Analysis using
Mantel-Haenszel Method (Fixed Effect)</h3>
<p><strong>Detailed Explanation of the Mantel-Haenszel (MH) Method in
Meta-Analysis</strong></p>
<p>The <strong>Mantel-Haenszel method</strong> is a classical and widely
used approach in meta-analysis for combining binary outcome data (e.g.,
event vs. non-event) across multiple studies. It estimates a
<strong>pooled odds ratio</strong> under the assumption of a
<strong>fixed effect</strong>, meaning that the true treatment effect
(odds ratio) is assumed to be the same in all studies, and differences
arise purely from sampling variability.</p>
<hr />
<p><strong>Data Structure and Odds Ratio Definition</strong></p>
<p>For each study, data are typically organized in a 2-by-2 contingency
table:</p>
<table>
<colgroup>
<col width="18%" />
<col width="23%" />
<col width="29%" />
<col width="28%" />
</colgroup>
<thead>
<tr class="header">
<th>Group</th>
<th>Events <span class="math inline">\((A, C)\)</span></th>
<th>Non-Events <span class="math inline">\((B, D)\)</span></th>
<th>Total <span class="math inline">\((n_1, n_2)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Experimental</td>
<td><span class="math inline">\(A\)</span></td>
<td><span class="math inline">\(B\)</span></td>
<td><span class="math inline">\(A + B\)</span></td>
</tr>
<tr class="even">
<td>Control</td>
<td><span class="math inline">\(C\)</span></td>
<td><span class="math inline">\(D\)</span></td>
<td><span class="math inline">\(C + D\)</span></td>
</tr>
</tbody>
</table>
<p>The <strong>odds</strong> of the event in each group is calculated
as:</p>
<ul>
<li>Experimental: <span class="math inline">\(\text{Odds}_E = A /
B\)</span></li>
<li>Control: <span class="math inline">\(\text{Odds}_C = C /
D\)</span></li>
</ul>
<p>The <strong>odds ratio (OR)</strong> for each study is:</p>
<p><span class="math display">\[
\text{OR}_i = \frac{A_i \cdot D_i}{B_i \cdot C_i}
\]</span></p>
<hr />
<p><strong>Mantel-Haenszel Pooled Odds Ratio</strong></p>
<p>The <strong>Mantel-Haenszel pooled OR</strong> is calculated as:</p>
<p><span class="math display">\[
\text{OR}_{MH} = \frac{\sum_{i=1}^K \frac{A_i D_i}{N_i}}{\sum_{i=1}^K
\frac{B_i C_i}{N_i}}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(N_i = A_i + B_i + C_i + D_i\)</span>,
the total sample size in the ith study.</li>
<li>This estimator is <strong>more robust</strong> than inverse variance
methods when individual studies have <strong>small cell
counts</strong>.</li>
</ul>
<p>It can be viewed as a <strong>weighted average</strong> of individual
study odds ratios, where each weight is:</p>
<p><span class="math display">\[
w_i = \frac{B_i C_i}{N_i}
\]</span></p>
<p>This leads to a relative weight for each study:</p>
<p><span class="math display">\[
\text{relWeight}_i = \frac{w_i}{\sum w_i}
\]</span></p>
<p>and the pooled estimate becomes:</p>
<p><span class="math display">\[
\text{OR}_{MH} = \sum \text{relWeight}_i \cdot \text{OR}_i
\]</span></p>
<hr />
<p><strong>Naïve Variance and Confidence Interval (Not
Recommended)</strong></p>
<p>A simple variance estimate for <span
class="math inline">\(\text{OR}_{MH}\)</span> is the inverse of the
total weight:</p>
<p><span class="math display">\[
\text{Var}(\text{OR}_{MH}) = \frac{1}{\sum w_i}
\]</span></p>
<p>This is used to construct an approximate 95% confidence interval:</p>
<p><span class="math display">\[
\text{CI} = \text{OR}_{MH} \pm 1.96 \times \sqrt{\text{Var}}
\]</span></p>
<p>However, this method is not ideal because the <strong>distribution of
the odds ratio is skewed</strong>, especially for small probabilities,
and the variance of the OR itself does not follow a normal
distribution.</p>
<hr />
<p><strong>Improved Confidence Interval Using Emerson’s
Approximation</strong></p>
<p>To improve accuracy, it is common to perform inference on the
<strong>log scale</strong> and construct a confidence interval for the
<strong>log odds ratio</strong>, then exponentiate the limits. Emerson’s
method uses an advanced formula proposed by Robins et al. (1986) to
estimate the <strong>variance of the log odds ratio</strong> more
precisely.</p>
<p>The steps are:</p>
<ol style="list-style-type: decimal">
<li><p>For each study, define:</p>
<ul>
<li><span class="math inline">\(T1_i = \frac{A_i +
D_i}{N_i}\)</span></li>
<li><span class="math inline">\(T2_i = \frac{B_i +
C_i}{N_i}\)</span></li>
<li><span class="math inline">\(T3_i = \frac{A_i D_i}{N_i}\)</span></li>
<li><span class="math inline">\(T4_i = \frac{B_i C_i}{N_i}\)</span></li>
</ul></li>
<li><p>Sum across all studies:</p>
<ul>
<li><span class="math inline">\(ST3 = \sum T3_i\)</span></li>
<li><span class="math inline">\(ST4 = \sum T4_i\)</span></li>
</ul></li>
<li><p>Compute the <strong>variance of the log odds
ratio</strong>:</p></li>
</ol>
<p><span class="math display">\[
\text{Var}(\log(\text{OR}_{MH})) = \frac{1}{2} \sum \left[
\frac{T1_i T3_i}{ST3^2} +
\frac{T1_i T4_i + T2_i T3_i}{ST3 \cdot ST4} +
\frac{T2_i T4_i}{ST4^2}
\right]
\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>The confidence interval is then:</li>
</ol>
<ul>
<li><p>Lower bound on log scale:</p>
<p><span class="math display">\[
\log(\text{OR}_{MH}) - 1.96 \cdot \sqrt{\text{Var}}
\]</span></p></li>
<li><p>Upper bound on log scale:</p>
<p><span class="math display">\[
\log(\text{OR}_{MH}) + 1.96 \cdot \sqrt{\text{Var}}
\]</span></p></li>
<li><p>Exponentiate both bounds to return to the original
scale.</p></li>
</ul>
<hr />
<pre class="r"><code># Create the statin study dataset
dat &lt;- data.frame(
  Study  = c(&quot;Prove-It&quot;, &quot;A-to-Z&quot;, &quot;TNT&quot;, &quot;IDEAL&quot;),
  evhigh = c(147, 205, 334, 411),    # events in high-dose group
  nhigh  = c(2099, 2265, 4995, 4439),# total in high-dose group
  evstd  = c(172, 235, 418, 463),    # events in standard-dose group
  nstd   = c(2063, 2232, 5006, 4449) # total in standard-dose group
)

# Odds ratios from individual studies
OR &lt;- c(0.828, 0.846, 0.787, 0.878)

# Step 1: compute raw Mantel-Haenszel weights
w0 &lt;- (dat$nhigh - dat$evhigh) * dat$evstd / (dat$nhigh + dat$nstd)

# Step 2: total weight and relative weights
TotWeight &lt;- sum(w0)
relWt &lt;- w0 / TotWeight

# Step 3: compute pooled MH OR
OR.MH &lt;- sum(relWt * OR)

# Step 4: naive variance
Var.ORMH1 &lt;- 1 / TotWeight
lowCI_naive &lt;- OR.MH - 1.96 * sqrt(Var.ORMH1)
upCI_naive &lt;- OR.MH + 1.96 * sqrt(Var.ORMH1)

print(paste0(&quot;MH estimate = &quot;,round(OR.MH,3), 
             &quot;, 95% CI (naive) = [&quot;,
             round(lowCI_naive,3),&quot;,&quot;,
             round(upCI_naive,3),&quot;]&quot;))</code></pre>
<pre><code>## [1] &quot;MH estimate = 0.835, 95% CI (naive) = [0.755,0.916]&quot;</code></pre>
<p><strong>R Implementation: Emerson’s Improved Method</strong></p>
<pre class="r"><code>A &lt;- dat$evhigh
B &lt;- dat$nhigh - dat$evhigh
C &lt;- dat$evstd
D &lt;- dat$nstd - dat$evstd
N &lt;- A + B + C + D

T1 &lt;- (A + D) / N
T2 &lt;- (B + C) / N
T3 &lt;- A * D / N
T4 &lt;- B * C / N

ST3 &lt;- sum(T3)
ST4 &lt;- sum(T4)

Var.lnOR &lt;- 0.5 * (
  (T1 * T3) / ST3^2 +
  (T1 * T4 + T2 * T3) / (ST3 * ST4) +
  (T2 * T4) / ST4^2
)

Var.lnMH &lt;- sum(Var.lnOR)

# 95% CI on log scale
lowCI_lnMH &lt;- log(OR.MH) - 1.96 * sqrt(Var.lnMH)
upCI_lnMH &lt;- log(OR.MH) + 1.96 * sqrt(Var.lnMH)

# Convert to OR scale
lowCI_MH &lt;- exp(lowCI_lnMH)
upCI_MH &lt;- exp(upCI_lnMH)

print(paste0(&quot;MH estimate = &quot;,round(OR.MH,3), 
             &quot;, 95% CI (Emerson) = [&quot;,
             round(lowCI_MH,3),&quot;,&quot;,
             round(upCI_MH,3),&quot;]&quot;))</code></pre>
<pre><code>## [1] &quot;MH estimate = 0.835, 95% CI (Emerson) = [0.768,0.909]&quot;</code></pre>
<p>This confidence interval matches the output from the
<code>meta</code> package.</p>
<pre class="r"><code>ORMH.Statin &lt;- metabin(
  event.e = evhigh, n.e = nhigh,
  event.c = evstd,  n.c = nstd,
  studlab = Study,
  data = dat,
  method = &quot;MH&quot;,   # Mantel-Haenszel
  sm = &quot;OR&quot;
)

summary(ORMH.Statin)</code></pre>
<pre><code>##              OR           95%-CI %W(common) %W(random)
## Prove-It 0.8279 [0.6585; 1.0411]       13.6       13.6
## A-to-Z   0.8457 [0.6943; 1.0300]       18.2       18.3
## TNT      0.7865 [0.6773; 0.9134]       32.9       31.8
## IDEAL    0.8784 [0.7638; 1.0103]       35.4       36.3
## 
## Number of studies: k = 4
## Number of observations: o = 27548 (o.e = 13798, o.c = 13750)
## Number of events: e = 2385
## 
##                          OR           95%-CI     z  p-value
## Common effect model  0.8354 [0.7679; 0.9089] -4.18 &lt; 0.0001
## Random effects model 0.8355 [0.7679; 0.9090] -4.18 &lt; 0.0001
## 
## Quantifying heterogeneity (with 95%-CIs):
##  tau^2 = 0 [0.0000; 0.0239]; tau = 0 [0.0000; 0.1545]
##  I^2 = 0.0% [0.0%; 84.7%]; H = 1.00 [1.00; 2.56]
## 
## Test of heterogeneity:
##     Q d.f. p-value
##  1.14    3  0.7673
## 
## Details of meta-analysis methods:
## - Mantel-Haenszel method (common effect model)
## - Inverse variance method (random effects model)
## - Restricted maximum-likelihood estimator for tau^2
## - Q-Profile method for confidence interval of tau^2 and tau
## - Calculation of I^2 based on Q</code></pre>
<pre class="r"><code>plot(ORMH.Statin)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>The <strong>Mantel-Haenszel method</strong> is a reliable and
efficient approach to pooling odds ratios across studies, especially
when:</p>
<ul>
<li>The event is rare</li>
<li>Some cell counts are small</li>
<li>The number of studies is moderate to large</li>
</ul>
</div>
<div id="estimation-in-sparse-data---continuity-correction"
class="section level3" number="6.4.3">
<h3><span class="header-section-number">6.4.3</span> Estimation in
Sparse Data - Continuity correction</h3>
<p><strong>Sparse data</strong> refers to cases where the number of
events is very low—sometimes <strong>zero</strong>—in one or more groups
of a 2×2 contingency table:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Event (Yes)</th>
<th>Event (No)</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Experimental</td>
<td><span class="math inline">\(a_k\)</span></td>
<td><span class="math inline">\(b_k\)</span></td>
<td><span class="math inline">\(n_{ek} = a_k + b_k\)</span></td>
</tr>
<tr class="even">
<td>Control</td>
<td><span class="math inline">\(c_k\)</span></td>
<td><span class="math inline">\(d_k\)</span></td>
<td><span class="math inline">\(n_{ck} = c_k + d_k\)</span></td>
</tr>
</tbody>
</table>
<p>If <strong>either <span class="math inline">\(a_k = 0\)</span> or
<span class="math inline">\(c_k = 0\)</span></strong> (i.e. no events in
the treatment or control group), then:</p>
<ul>
<li>The <strong>odds ratio (OR)</strong> becomes either 0 or ∞
(undefined on the log scale).</li>
<li>The <strong>risk ratio (RR)</strong> becomes either 0 or 1, but the
<strong>variance estimate becomes unstable or unreliable</strong>.</li>
</ul>
<p>This results in high <strong>statistical uncertainty</strong> and
<strong>biased estimates</strong>, especially when you try to combine
such studies in a meta-analysis.</p>
<hr />
<p>To address this problem, researchers typically consider two
approaches:</p>
<p><strong>1. Exclude the study</strong>: Omitting studies with zero
events may avoid the distortion of summary statistics, but it
<strong>discards potentially valuable data</strong> and can introduce
<strong>selection bias</strong>.</p>
<p><strong>2. Apply continuity correction</strong>: This involves
<strong>adding a small constant</strong> to each cell of the 2×2 table
to avoid zeros and make OR and RR computable. This is the more commonly
used method.</p>
<p>The <strong>standard correction</strong> is to add
<strong>0.5</strong> to every cell of the table:</p>
<p><span class="math display">\[
\hat{\psi}_{k}^{\text{mod}} = \frac{(a_k + 0.5)(d_k + 0.5)}{(b_k +
0.5)(c_k + 0.5)}
\]</span></p>
<p>This adjusted odds ratio becomes computable even if some original
counts were zero.</p>
<p>The variance of the <strong>log-transformed adjusted odds
ratio</strong> is:</p>
<p><span class="math display">\[
\widehat{\operatorname{Var}}(\log \hat{\psi}_{k}^{\text{mod}}) =
\frac{1}{a_k + 0.5} + \frac{1}{b_k + 0.5} + \frac{1}{c_k + 0.5} +
\frac{1}{d_k + 0.5}
\]</span></p>
<p>This method is easy to implement and prevents division by zero.
However, it is <strong>sensitive to the choice of increment</strong>,
especially in small studies.</p>
<ul>
<li>Using 0.5 may <strong>artificially shrink</strong> or
<strong>inflate</strong> estimates.</li>
<li>To assess this, one can try other small constants like 0.01 or 0.1
to check <strong>sensitivity</strong>.</li>
</ul>
<hr />
<p>Sweeting et al. (2004) proposed a <strong>more refined continuity
correction</strong> that adjusts for <strong>imbalance in sample
sizes</strong> between treatment groups.</p>
<p>Define:</p>
<ul>
<li><span class="math inline">\(n_e\)</span>: sample size in the
<strong>experimental</strong> group</li>
<li><span class="math inline">\(n_c\)</span>: sample size in the
<strong>control</strong> group</li>
</ul>
<p>Then the continuity corrections for each arm are:</p>
<p><span class="math display">\[
\text{incr}_e = \frac{n_e}{n_e + n_c}, \quad \text{incr}_c =
\frac{n_c}{n_e + n_c}
\]</span></p>
<p><strong>Key points:</strong></p>
<ul>
<li>The sum of <span class="math inline">\(\text{incr}_e + \text{incr}_c
= 1\)</span></li>
<li>If sample sizes are equal, both are 0.5 (same as the standard
method)</li>
<li>If <strong>sample sizes are unbalanced</strong>, the group with the
smaller size gets a <strong>smaller correction</strong>, leading to
<strong>less bias</strong></li>
</ul>
<p>This approach helps produce more accurate estimates of ORs in
<strong>asymmetric studies</strong> where one group is much larger than
the other.</p>
<hr />
<p><strong>Summary of Continuity Correction Techniques</strong></p>
<table>
<colgroup>
<col width="24%" />
<col width="17%" />
<col width="27%" />
<col width="31%" />
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Increment Type</th>
<th>When Used</th>
<th>Advantage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Standard (0.5 rule)</strong></td>
<td>0.5 to each cell</td>
<td>Common, default in many packages</td>
<td>Simple, avoids zero counts</td>
</tr>
<tr class="even">
<td><strong>Smaller fixed increments</strong></td>
<td>e.g. 0.01, 0.1</td>
<td>Sensitivity analysis</td>
<td>Tests robustness of results</td>
</tr>
<tr class="odd">
<td><strong>Sweeting’s method</strong></td>
<td>Based on group sizes</td>
<td>Unbalanced study arms</td>
<td>Reduces bias in odds ratio estimates</td>
</tr>
</tbody>
</table>
<hr />
<p><strong>Practical Recommendation</strong></p>
<p>When working with sparse data in meta-analysis:</p>
<ol style="list-style-type: decimal">
<li><strong>Do not blindly discard studies</strong> with zero
events.</li>
<li>Use a <strong>continuity correction</strong>, especially if using OR
or RR.</li>
<li>Prefer <strong>Sweeting’s method</strong> when arms are highly
unbalanced.</li>
<li>Always perform a <strong>sensitivity analysis</strong> using
different corrections to ensure robustness of conclusions.</li>
</ol>
<p>These approaches are implemented in most statistical software
packages (e.g., R <code>meta</code>, <code>metafor</code>) via options
for continuity correction (e.g., <code>add=0.5</code> or
<code>add="tacc"</code> for treatment-arm continuity correction).</p>
<pre class="r"><code># With 0.5 continuity correction for sparse data
metabin(Ee, Ne, Ec, Nc, sm=&quot;OR&quot;, method=&quot;I&quot;,
        data=data8, subset=study==&quot;Australian&quot;)</code></pre>
<pre><code>## Number of observations: o = 595 (o.e = 300, o.c = 295)
## Number of events: e = 1
## 
##       OR           95%-CI     z p-value
## 1 0.3267 [0.0133; 8.0515] -0.68  0.4938
## 
## Details:
## - Continuity correction of 0.5</code></pre>
<pre class="r"><code># With 0.1 continuity correction for sparse data
metabin(Ee, Ne, Ec, Nc, sm=&quot;OR&quot;, method=&quot;I&quot;,
        data=data8, subset=study==&quot;Australian&quot;,
        incr=0.1)</code></pre>
<pre><code>## Number of observations: o = 595 (o.e = 300, o.c = 295)
## Number of events: e = 1
## 
##       OR            95%-CI     z p-value
## 1 0.0891 [0.0001; 57.8269] -0.73  0.4642
## 
## Details:
## - Continuity correction of 0.1</code></pre>
<pre class="r"><code># conduct an analysis based on the treatment arm continuity correction
# using argument incr=&quot;TACC&quot;
metabin(Ee, Ne, Ec, Nc, sm=&quot;OR&quot;, method=&quot;I&quot;,
        data=data8, subset=study==&quot;Australian&quot;,
        incr=&quot;TACC&quot;)        </code></pre>
<pre><code>## Number of observations: o = 595 (o.e = 300, o.c = 295)
## Number of events: e = 1
## 
##       OR           95%-CI     z p-value
## 1 0.3303 [0.0135; 8.0698] -0.68  0.4969
## 
## Details:
## - Treatment arm continuity correction in study with zero cell frequencies</code></pre>
<p><strong>Calculating the risk ratio with sparse data, (Pettigrew et
al)</strong></p>
<p><span
class="math display">\[\hat{\phi}_{k}^{\mathrm{mod}}=\frac{a_{k}+0.5}{a_{k}+b_{k}+0.5}
/ \frac{c_{k}+0.5}{c_{k}+d_{k}+0.5}\]</span> <span
class="math display">\[\widehat{\operatorname{Var}}\left(\log
\hat{\phi}_{k}^{\bmod
}\right)=\frac{1}{a_{k}+0.5}+\frac{1}{c_{k}+0.5}-\frac{1}{a_{k}+b_{k}+0.5}-\frac{1}{c_{k}+d_{k}+0.5}
\cdot\]</span></p>
</div>
<div id="peto-odds-ratio-fixed-effect" class="section level3"
number="6.4.4">
<h3><span class="header-section-number">6.4.4</span> Peto Odds Ratio
(Fixed Effect)</h3>
<p>The <strong>Peto method</strong> is another fixed-effect approach to
meta-analyzing binary outcomes, particularly when effect sizes are small
and events are relatively rare. This method provides an approximation of
the <strong>pooled odds ratio (OR)</strong> without requiring the
individual odds ratios from each study to be calculated directly. It’s
especially suited for <strong>unbalanced designs</strong> (i.e.,
differing group sizes) or <strong>sparse data</strong>, although it can
become biased if the treatment and control group sizes are very unequal
or if the treatment effect is large.</p>
<hr />
<p><strong>1. Peto’s Odds Ratio: Concept</strong></p>
<p>The <strong>Peto odds ratio</strong> for each study is based on the
<strong>difference between observed and expected event counts</strong>
under the null hypothesis (no effect), and uses this difference to
construct the log odds ratio.</p>
<p>For study <span class="math inline">\(i\)</span>, define:</p>
<ul>
<li><p><span class="math inline">\(O_i\)</span>: Observed number of
events in the <strong>experimental</strong> group (i.e., <span
class="math inline">\(A_i\)</span>)</p></li>
<li><p><span class="math inline">\(E_i\)</span>: Expected number of
events in the experimental group under the null:</p>
<p><span class="math display">\[
E_i = \frac{(A_i + B_i)(A_i + C_i)}{N_i}
\]</span></p></li>
<li><p><span class="math inline">\(V_i\)</span>: Variance of <span
class="math inline">\(O_i - E_i\)</span>:</p>
<p><span class="math display">\[
V_i = \frac{(A_i + B_i)(C_i + D_i)(A_i + C_i)(B_i + D_i)}{N_i^2(N_i -
1)}
\]</span></p></li>
</ul>
<p>Using these, the <strong>Peto odds ratio</strong> for each study
is:</p>
<p><span class="math display">\[
\hat{\Psi}_i = \exp\left(\frac{O_i - E_i}{V_i}\right)
\]</span></p>
<p>And the <strong>95% confidence interval</strong> for this estimate
is:</p>
<p><span class="math display">\[
\text{CI}_i = \exp\left(\frac{O_i - E_i \pm 1.96 \cdot
\sqrt{V_i}}{V_i}\right)
\]</span></p>
<hr />
<p><strong>2. Pooled Peto’s Odds Ratio Across Studies</strong></p>
<p>The <strong>pooled odds ratio</strong> is calculated as:</p>
<p><span class="math display">\[
\hat{\Psi}_{\text{pooled}} = \exp\left(\frac{\sum_{i=1}^K (O_i -
E_i)}{\sum_{i=1}^K V_i}\right)
\]</span></p>
<p>And its <strong>95% confidence interval</strong> is:</p>
<p><span class="math display">\[
\exp\left( \frac{\sum (O_i - E_i) \pm 1.96 \cdot \sqrt{\sum V_i}}{\sum
V_i} \right)
\]</span></p>
<p>This method avoids direct computation of individual study ORs and
variances, focusing instead on the <strong>expected
vs. observed</strong> framework.</p>
<hr />
<p><strong>3. Manual Calculation in R (Step-by-Step)</strong></p>
<pre class="r"><code>dat &lt;- data.frame(
  Study  = c(&quot;Prove-It&quot;, &quot;A-to-Z&quot;, &quot;TNT&quot;, &quot;IDEAL&quot;),
  evhigh = c(147, 205, 334, 411),
  nhigh  = c(2099, 2265, 4995, 4439),
  evstd  = c(172, 235, 418, 463),
  nstd   = c(2063, 2232, 5006, 4449)
)

## First, define the 2×2 table components:

A &lt;- dat$evhigh
B &lt;- dat$nhigh - A
C &lt;- dat$evstd
D &lt;- dat$nstd - C
N &lt;- A + B + C + D
 
# Now compute the Peto components for each study:
# Observed
Oi &lt;- A

# Expected under null
Ei &lt;- (A + B) * (A + C) / N

# Variance
Vi &lt;- (A + B) * (C + D) * (A + C) * (B + D) / (N^2 * (N - 1))

# Peto’s OR
psii &lt;- exp((Oi - Ei) / Vi)
print(psii)</code></pre>
<pre><code>## [1] 0.8282381 0.8458672 0.7872841 0.8785694</code></pre>
<pre class="r"><code>## Compute confidence intervals for each study:
lowCIi &lt;- exp((Oi - Ei - 1.96 * sqrt(Vi)) / Vi)
upCIi  &lt;- exp((Oi - Ei + 1.96 * sqrt(Vi)) / Vi)

print(lowCIi)</code></pre>
<pre><code>## [1] 0.6591255 0.6948022 0.6785452 0.7640730</code></pre>
<pre class="r"><code>print(upCIi)</code></pre>
<pre><code>## [1] 1.0407400 1.0297771 0.9134487 1.0102231</code></pre>
<pre class="r"><code>print(&quot;Interpretation: Only the third study may have a CI that excludes 1 → statistically significant&quot;)</code></pre>
<pre><code>## [1] &quot;Interpretation: Only the third study may have a CI that excludes 1 → statistically significant&quot;</code></pre>
<pre class="r"><code>## Now compute the **pooled Peto’s OR**:
psi &lt;- exp(sum(Oi - Ei) / sum(Vi))
lowCI &lt;- exp((sum(Oi - Ei) - 1.96 * sqrt(sum(Vi))) / sum(Vi))
upCI  &lt;- exp((sum(Oi - Ei) + 1.96 * sqrt(sum(Vi))) / sum(Vi))

cat(&quot;Pooled Peto OR =&quot;, round(psi, 3), &quot;\n&quot;)</code></pre>
<pre><code>## Pooled Peto OR = 0.836</code></pre>
<pre class="r"><code>cat(&quot;95% CI = [&quot;, round(lowCI, 3), &quot;,&quot;, round(upCI, 3), &quot;]\n&quot;)</code></pre>
<pre><code>## 95% CI = [ 0.768 , 0.909 ]</code></pre>
<hr />
<p><strong>4. Using the <code>meta</code> R Package for Peto’s
Method</strong></p>
<p>The <code>meta</code> package automates this calculation. Here’s how
to perform Peto’s OR meta-analysis using <code>metabin()</code>:</p>
<pre class="r"><code>ORPeto.Statin &lt;- metabin(
  event.e = evhigh, n.e = nhigh,
  event.c = evstd,  n.c = nstd,
  studlab = Study,
  data = dat,
  method = &quot;Peto&quot;,  # use Peto&#39;s method
  sm = &quot;OR&quot;
)

summary(ORPeto.Statin)</code></pre>
<pre><code>##              OR           95%-CI %W(common) %W(random)
## Prove-It 0.8282 [0.6591; 1.0407]       13.5       13.5
## A-to-Z   0.8459 [0.6948; 1.0298]       18.3       18.3
## TNT      0.7873 [0.6785; 0.9134]       32.0       32.0
## IDEAL    0.8786 [0.7641; 1.0102]       36.2       36.2
## 
## Number of studies: k = 4
## Number of observations: o = 27548 (o.e = 13798, o.c = 13750)
## Number of events: e = 2385
## 
##                          OR           95%-CI     z  p-value
## Common effect model  0.8357 [0.7684; 0.9090] -4.18 &lt; 0.0001
## Random effects model 0.8357 [0.7684; 0.9090] -4.18 &lt; 0.0001
## 
## Quantifying heterogeneity (with 95%-CIs):
##  tau^2 = 0 [0.0000; 0.0235]; tau = 0 [0.0000; 0.1532]
##  I^2 = 0.0% [0.0%; 84.7%]; H = 1.00 [1.00; 2.56]
## 
## Test of heterogeneity:
##     Q d.f. p-value
##  1.13    3  0.7692
## 
## Details of meta-analysis methods:
## - Peto method
## - Restricted maximum-likelihood estimator for tau^2
## - Q-Profile method for confidence interval of tau^2 and tau
## - Calculation of I^2 based on Q</code></pre>
<pre class="r"><code>plot(ORPeto.Statin)</code></pre>
<p><img src="06-Analysis-Meta-Analysis_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p><strong>When to Use Peto’s Method</strong></p>
<p>Peto’s method is most appropriate when:</p>
<ul>
<li>Treatment effects are small (i.e., odds ratios close to 1)</li>
<li>Event rates are low (rare outcomes)</li>
<li>Treatment and control group sizes are approximately balanced</li>
</ul>
<p>However, <strong>Peto’s method can be biased</strong> if:</p>
<ul>
<li>There are large treatment effects</li>
<li>Treatment and control groups are imbalanced</li>
<li>There are studies with zero events in one arm</li>
</ul>
<p>For general applications with more robust statistical properties, the
<strong>Mantel-Haenszel method</strong> or <strong>inverse-variance
method</strong> is typically preferred.</p>
</div>
</div>
<div id="random-effect-model" class="section level2" number="6.5">
<h2><span class="header-section-number">6.5</span> Random Effect
Model</h2>
<p>In real-world meta-analysis of binary outcomes (e.g., treatment
success/failure, survival/death), the differences between study results
often cannot be explained by random sampling error alone. These
differences may come from:</p>
<ul>
<li>Varying populations across studies</li>
<li>Differences in study design or quality</li>
<li>Variations in intervention delivery or control conditions</li>
</ul>
<p>The <strong>random effects model</strong> explicitly models this
extra variability between studies.</p>
<hr />
<p><strong>Model Specification</strong></p>
<p>Suppose each study <span class="math inline">\(k\)</span> provides an
estimate <span class="math inline">\(\hat{\theta}_k\)</span>, such as
the <strong>log odds ratio</strong> or <strong>log risk ratio</strong>.
The random effects model assumes:</p>
<p><span class="math display">\[
\hat{\theta}_{k} = \theta + u_k + \sigma_k \epsilon_k
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\hat{\theta}_k\)</span>: Observed
treatment effect (e.g., log OR) in study <span
class="math inline">\(k\)</span></li>
<li><span class="math inline">\(\theta\)</span>: The <strong>overall
average effect size</strong> across all studies (the pooled
estimate)</li>
<li><span class="math inline">\(u_k \sim N(0, \tau^2)\)</span>: Random
<strong>between-study deviation</strong>, capturing heterogeneity</li>
<li><span class="math inline">\(\sigma_k \epsilon_k \sim N(0,
\sigma_k^2)\)</span>: Within-study error due to sampling
variability</li>
<li><span class="math inline">\(\tau^2\)</span>: The
<strong>between-study variance</strong>, a key quantity that reflects
the extent of heterogeneity</li>
<li><span class="math inline">\(\epsilon_k\)</span>: Standard normal
error, i.i.d. <span class="math inline">\(N(0,1)\)</span></li>
</ul>
<p>In words, this model says that each study’s effect estimate deviates
from the overall effect due to:</p>
<ul>
<li><strong>Between-study variability</strong> (<span
class="math inline">\(u_k\)</span>) → heterogeneity</li>
<li><strong>Within-study sampling error</strong> (<span
class="math inline">\(\sigma_k \epsilon_k\)</span>)</li>
</ul>
<p>Both sources of error are assumed to be independent.</p>
<hr />
<p><strong>Key Assumption: Distribution of Effects</strong></p>
<p>Instead of assuming all studies estimate the <strong>same true
effect</strong> (<span class="math inline">\(\theta\)</span>), we assume
that the <strong>true effect for each study</strong> is <strong>drawn
from a distribution</strong> centered at <span
class="math inline">\(\theta\)</span>, with variability <span
class="math inline">\(\tau^2\)</span>:</p>
<p><span class="math display">\[
\theta_k^{\text{true}} = \theta + u_k, \quad \text{with } u_k \sim N(0,
\tau^2)
\]</span></p>
<p>So, the treatment effect itself is <strong>random across
studies</strong>, and the observed effect <span
class="math inline">\(\hat{\theta}_k\)</span> is a noisy version of that
random effect.</p>
<p>Given the above, the <strong>total variance</strong> of the observed
effect in each study is:</p>
<p><span class="math display">\[
\operatorname{Var}(\hat{\theta}_k) = \sigma_k^2 + \tau^2
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\sigma_k^2\)</span>:
<strong>Within-study variance</strong>, often derived from 2×2 tables
(as in log OR variance)</li>
<li><span class="math inline">\(\tau^2\)</span>: <strong>Between-study
variance</strong>, to be estimated</li>
</ul>
<hr />
<p><strong>Estimating Between-Study Variance: DerSimonian–Laird
Method</strong></p>
<p>The <strong>DerSimonian–Laird (DL) method</strong> is a widely used
approach for estimating <span class="math inline">\(\tau^2\)</span>.</p>
<p><strong>Step 1: Compute Q Statistic (Cochran’s Q)</strong></p>
<p><span class="math display">\[
Q = \sum_{k=1}^{K} w_k (\hat{\theta}_k - \hat{\theta}_F)^2
\]</span></p>
<ul>
<li><span class="math inline">\(\hat{\theta}_F\)</span>: Fixed-effect
estimate (weighted average of study estimates)</li>
<li><span class="math inline">\(w_k =
\frac{1}{\widehat{\operatorname{Var}}(\hat{\theta}_k)}\)</span>: Inverse
variance weights</li>
</ul>
<p><strong>Step 2: Compute the DL estimator of <span
class="math inline">\(\tau^2\)</span>:</strong></p>
<p><span class="math display">\[
\hat{\tau}^2 = \frac{Q - (K - 1)}{\sum_{k=1}^{K} w_k -
\frac{\sum_{k=1}^{K} w_k^2}{\sum_{k=1}^{K} w_k}}
\]</span></p>
<ul>
<li>If <span class="math inline">\(Q &lt; K - 1\)</span>, then <span
class="math inline">\(\hat{\tau}^2\)</span> is <strong>set to 0</strong>
(i.e., no detected heterogeneity)</li>
</ul>
<p><strong>Interpretation:</strong></p>
<ul>
<li><span class="math inline">\(Q &gt; K - 1\)</span> → significant
heterogeneity → <span class="math inline">\(\tau^2 &gt; 0\)</span></li>
<li><span class="math inline">\(Q \approx K - 1\)</span> → little/no
heterogeneity → <span class="math inline">\(\tau^2 \approx
0\)</span></li>
</ul>
<hr />
<p><strong>Pooled Effect and Weighting</strong></p>
<p>Once <span class="math inline">\(\tau^2\)</span> is estimated, the
weights used for the random effects pooled estimate become:</p>
<p><span class="math display">\[
w_k^{\text{RE}} = \frac{1}{\sigma_k^2 + \hat{\tau}^2}
\]</span></p>
<p>Then the random effects pooled estimate is:</p>
<p><span class="math display">\[
\hat{\theta}_{\text{RE}} = \frac{\sum w_k^{\text{RE}}
\hat{\theta}_k}{\sum w_k^{\text{RE}}}
\]</span></p>
<p>With variance:</p>
<p><span class="math display">\[
\widehat{\operatorname{Var}}(\hat{\theta}_{\text{RE}}) = \frac{1}{\sum
w_k^{\text{RE}}}
\]</span></p>
<hr />
<table>
<colgroup>
<col width="27%" />
<col width="72%" />
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Random Effects Model</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Treatment Effect</td>
<td>Varies across studies: <span class="math inline">\(\theta_k = \theta
+ u_k\)</span></td>
</tr>
<tr class="even">
<td>Between-Study Variance</td>
<td><span class="math inline">\(\tau^2\)</span> (heterogeneity across
true effects)</td>
</tr>
<tr class="odd">
<td>Within-Study Variance</td>
<td><span class="math inline">\(\sigma_k^2\)</span> (sampling
error)</td>
</tr>
<tr class="even">
<td>Pooled Estimate</td>
<td>Weighted average using total variance <span
class="math inline">\(\sigma_k^2 + \tau^2\)</span></td>
</tr>
<tr class="odd">
<td>Weight Formula</td>
<td><span class="math inline">\(w_k^{RE} = \frac{1}{\sigma_k^2 +
\tau^2}\)</span></td>
</tr>
<tr class="even">
<td>Estimator of <span class="math inline">\(\tau^2\)</span></td>
<td>DerSimonian–Laird (DL)</td>
</tr>
<tr class="odd">
<td>Best for</td>
<td>Heterogeneous studies, moderate to large number of studies</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="meta-analysis-for-rare-events" class="section level1"
number="7">
<h1><span class="header-section-number">7</span> Meta-Analysis for Rare
Events</h1>
</div>
<div id="reference-apa-style" class="section level1" number="8">
<h1><span class="header-section-number">8</span> Reference (APA
Style)</h1>
<div id="book" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Book</h2>
<p>Harrer, M., Cuijpers, P., Furukawa, T.A., &amp; Ebert, D.D. (2021).
<em>Doing Meta-Analysis with R: A Hands-On Guide</em>. Boca Raton, FL
and London: Chapmann &amp; Hall/CRC Press. ISBN 978-0-367-61007-4. <a
href="https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/"
class="uri">https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/</a></p>
<p>Chen, D.-G. (Din), &amp; Peace, K. E. (2021). Applied Meta-Analysis
with R and Stata (2nd ed.). Chapman &amp; Hall/CRC Press. <a
href="https://doi.org/10.1201/9780429061240"
class="uri">https://doi.org/10.1201/9780429061240</a></p>
<p>Schwarzer, G., Carpenter, J. R., &amp; Rücker, G. (2015).
Meta-Analysis with R. Springer. <a
href="https://doi.org/10.1007/978-3-319-21416-0"
class="uri">https://doi.org/10.1007/978-3-319-21416-0</a></p>
</div>
<div id="meta-analysis" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Meta Analysis</h2>
<p>Berkey, C. S., D. C. Hoaglin, A. Antezak-Bouckoms, F. Mosteller, and
G. A. Colditz (1998). Meta-analysis of multiple outcomes by regression
with random effects. Statistics in Medicine 17, 2537–2550.</p>
<p>Berkey, C. S., D. C. Hoaglin, F. Mosteller, and G. A. Colditz (1995).
A random-effects regression model for meta-analysis. Statistics in
Medicine 14(4), 395–411.</p>
<p>Berman, N. G. and R. A. Parker (2002). Meta-analysis: Neither quick
nor easy. BMC Medical Research Methodology 2(10), 1–9.</p>
<p>Bhaumik, D. K., A. Amatya, S. T. Normand, J. Greenhouse, E. Kaizar,
B. Neelon, and R. D. Gibbons (2012). Meta-analysis of rare binary
adverse event data. Journal of the American Statistical Association
107(498), 555–567.</p>
<p>Bohning, D., E. Dietz, P. Schlattmann, C. Viwatwonkasem, A. Biggeri,
and J. Bock (2002). Some general points in estimating heterogeneity
variance with the dersimonianUlaird estimator. ˝ Biostatistics 3,
445–457.</p>
<p>Borenstein, M., L. V. Hedges, J. P. T. Higgins, and H. R. Rothstein
(2009). Introduction to Meta-Analysis. West Sussex, United Kingdom:
Wiley.</p>
<p>Cai, T., L. Parast, and L. Ryan (2010). Meta-analysis for rare
events. Statistics in Medicine 29(20), 2078–2089.</p>
<p>Chen, H., A. K. Manning, and J. Dupuis (2012). A method of moments
estimator for random effect multivariate meta-analysis. Biometrics,
(Epub May 2, 2012).</p>
<p>Cooper, H., L. V. Hedges, and J. C. Valentine (2009). The Handbook of
Research Synthesis and Meta-Analysis (2nd edition). New York: Russell
Sage Foundation.</p>
<p>DerSimonian, R. and N. Laird (1986). Meta-analysis in clinical
trials. Controlled Clinical Trials 7, 177–188.</p>
<p>Hartung, J., G. Knapp, and B. K. Sinha (2008). Statistical
Meta-Analysis with Applications. Hoboken, New Jersey: John Wiley &amp;
Sons, Inc.</p>
<p>Rucker2008-Simpson’s paradox visualized The example of the
Rosiglitazone meta-analysis</p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
