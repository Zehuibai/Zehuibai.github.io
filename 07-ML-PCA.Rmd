---
title: |
  ![](logo.png){width=3in}  
  Principal Component Analysis
output:
  html_document:
    df_print: paged
    number_sections: No
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
fontsize: 10pt
editor_options:
  chunk_output_type: console
colorlinks: yes
---
 
 

## Introduction

* 聚类分析，它可以将相似的观测 归成一类
* 主成分分析(PCA)，它可以对相关变量进行归类，从而降低数据维度，提高对数据的理解

### Component

成分就是特征的规范化线性组合(James，2012)。在一个数据集中，**第一主成分就 是能够最大程度解释数据中的方差的特征线性组合**。**第二主成分是另一种特征线性组合，它在方 向与第一主成分垂直这个限制条件下，最大程度解释数据中的方差**。其后的每一个主成分(可以 构造与变量数相等数目的主成分)都遵循同样的规则。

* 线性组合：如果你试图在一个变量之间基本不相关的数据集上使用PCA，很可能会得到一个毫无意义的分析结果
* 变量的均值和方差是充分统计量。也就是说，数据应该服从正态分布，这样协方差矩阵即可充分 描述数据集。换言之，数据要满足多元正态分布。PCA对于非正态分布的数据具有相当强的鲁棒 性，甚至可以和二值变量一起使用，所以结果具有很好的解释性。


### PCA算法

PCA顾名思义，就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。具体的，假如我们的数据集是n维的，共有m个数据 $(x^{(1)},x^{(2)},...,x^{(m)})$ 。我们希望将这m个数据的维度从n维降到n'维，希望这m个n'维的数据集尽可能的代表原始数据集。我们知道数据从n维降到n'维肯定会有损失，但是我们希望损失尽可能的小。如何让这n'维的数据尽可能表示原来的数据

求样本 $x^{(i)}$ 的n'维的主成分其实就是求样本集的协方差矩阵 $X X^{T}$ 的前n'个特征值对应特征向量矩阵W，然后对于每个样本 $x^{(i)}$,做如下变换 $z^{(i)}=W^{T} x^{(i)},$ 即达到降维的PCA目的。

具体的算法流程。

* 输入：n维样本集 $D=\left(x^{(1)}, x^{(2)}, \ldots, x^{(m)}\right),$ 要降维到的维数n'.
* 输出：降维后的样本集 $D^{\prime}$


1. 对所有的样本进行中心化: $x^{(i)}=x^{(i)}-\frac{1}{m} \sum_{j=1}^{m} x^{(j)}$
2. 计算样本的协方差矩阵 $X X^{T}$
3. 对矩阵 $X X^{T}$ 进行特征值分解
4. 取出最大的n'个特征值对应的特征向量 $\left(w_{1}, w_{2}, \ldots, w_{n^{\prime}}\right),$ 将所有的特征向量标准化后, 组成特征向量矩阵W。。
5. 对样本集中的每一个样本 $x^{(i)}$,转化为新的样本 $z^{(i)}=W^{T} x^{(i)}$
6.  得到输出样本集 $D^{\prime}=\left(z^{(1)}, z^{(2)}, \ldots, z^{(m)}\right)$

### 主成分旋转

**旋转方法有两种**

* **正交旋转**(rotate="varimax")\
  使得成分保持不相关（正交旋转）
* **斜交旋转**(rotate="promax")\
  使得成分相关\
  \
  最流行的正交旋转是 方差极大旋转，它试图对载荷的列进行去噪， 使得每个成分只由一组有限的变量来解释（即载荷阵每列只有少数几个很大的载荷，其他的都是很小的载荷）



### Kernelized PCA

PCA算法中，假设存在一个线性的超平面，可以让我们对数据进行投影。但是有些时候，数据不是线性的，不能直接进行PCA降维。

这里就需要用到和支持向量机一样的核函数的思想，先把数据集从n维映射到线性可分的高维N>n,然后再从N维降维到一个低维度n', 这里的维度之间满足$n'\lt n \lt N$。


使用了核函数的主成分分析一般称之为核主成分分析(Kernelized PCA, 以下简称KPCA。假设高维空间的数据是由n维空间的数据通过映射$\phi$产生。则对于n维空间的特征分解：

$$
\sum\limits_{i=1}^{m}x^{(i)}x^{(i)T}W=\lambda W
$$

映射为：

$$
\sum\limits_{i=1}^{m}\phi(x^{(i)})\phi(x^{(i)})^TW=\lambda W
$$

通过在高维空间进行协方差矩阵的特征值分解，然后用和PCA一样的方法进行降维。


## Application

### Data preparation

```{r,echo = F,message = FALSE, error = FALSE, warning = FALSE}
# <!-- ---------------------------------------------------------------------- -->
# <!--                    1. load the required packages                       -->
# <!-- ---------------------------------------------------------------------- --> 
packages<-c("tidyverse", "knitr")
ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}
ipak(packages)

 
## library(GPArotation) #support rotation
library(psych) #PCA package

## getwd()
test <- read.csv("./01_Datasets/NHLtest.csv", header=T)
train <- read.csv("./01_Datasets/NHLtrain.csv", header=T)
train.scale <- scale(train[, -1:-2])
```


### Modeling

对于模型构建过程，我们按照以下几个步骤进行:

* 抽取主成分并决定保留的数量;
    + 通过psych包抽取主成分要使用principal()函数，语法中要包括数据和是否要进行主成分旋转
    + pca <- principal(train.scale, rotate="none")
    + 碎石图可以帮助你评估能解释大部分数据方差的主成分, 需要在碎石图中找出使变化率降低的那个点， 也就是我们常说的统计图中的“肘点”或弯曲点。
    + 肘点表示在这个点上新增加一个主成分时，对方差的解释增加得并不太多。换句话说，这个点就是曲线由陡变平的转折点
    
* 对留下的主成分进行旋转;
    + 旋转背后的意义是使变量在某个主成分上的载荷最大化
    + 可以减少(或消灭)主成分之间的相关性，有助于对主成分的解释。
    + 进行正交旋转的方法称为“方差最大法”。还有其他非正交旋转方法，这种方法允许主成分(因子)之间存在相关性
    
* 对旋转后的解决方案进行解释; 生成各个因子的得分; 使用得分作为输入变量进行回归分析

* 使用测试数据评价模型效果。



```{r,echo = F,message = FALSE, error = FALSE, warning = FALSE}
## 通过psych包抽取主成分要使用principal()函数，语法中要包括数据和是否要进行主成分旋转:
pca <- principal(train.scale, rotate="none")

## 使用碎石图可以帮助你评估能解释大部分数据方差的主成分
## 它用X轴表示主成分的数量，用Y轴表示相应的特征值:
plot(pca$values, type="b", ylab="Eigenvalues", xlab="Component")


## 设定使用5个主成分，并需要进行正交旋转
pca.rotate <- principal(train.scale, nfactors = 5, rotate = "varimax")
pca.rotate

#### 根据主成分建立因子得分
pca.scores <- data.frame(pca.rotate$scores)
head(pca.scores)
### 得到每个球队在每个因子上的得分，这些得分的计算非常简单， 每个观测的变量值乘以载荷 然后相加即可。现在可以将响应变量(ppg)作为一列加入数据
pca.scores$ppg <- train$ppg


### 回归分析
nhl.lm <- lm(ppg ~ ., data = pca.scores)
summary(nhl.lm)

nhl.lm2 <- lm(ppg ~ RC1 + RC2, data = pca.scores)
summary(nhl.lm2)


### 评价模型误差
sqrt(mean(nhl.lm2$residuals^2))



## 模型在样本外数据上的效果
test.scores <- data.frame(predict(pca.rotate, test[, c(-1:-2)]))
test.scores$pred <- predict(nhl.lm2, test.scores)

test.scores$ppg <- test$ppg
test.scores$Team <- test$Team

## 评价模型误差
sqrt(mean(nhl.lm2$residuals^2))

## 模型在样本外数据上的效果
test.scores <- data.frame(predict(pca.rotate, test[, c(-1:-2)]))
test.scores$pred <- predict(nhl.lm2, test.scores)

test.scores$ppg <- test$ppg
test.scores$Team <- test$Team

p <- ggplot(test.scores, aes(x = pred,
                       y = ppg,
                       label = Team)) 
p + geom_point() + 
  geom_text(size=3.5, hjust=0.4, vjust = -0.9, angle = 35) + 
  xlim(0.75, 1.5) + ylim(0.5, 1.6) +
  stat_smooth(method="lm", se=FALSE)

resid <- test.scores$ppg - test.scores$pred
sqrt(mean(resid^2))
```





